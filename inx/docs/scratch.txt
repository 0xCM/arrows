Sse <- Sse2 <- Sse3 <- Ssse3 <- Sse41 <- Sse42 <- Avx <- Avx2

Sse
-------------------------------------------------------------------------------


docs: __m128 _mm_and_ps (__m128 a, __m128 b) ANDPS xmm, xmm/m128
clr-sig: Vector128<float> And(Vector128<float> left, Vector128<float> right);




docs: __m128 _mm_cmpeq_ps (__m128 a, __m128 b) CMPPS xmm, xmm/m128, imm8(0)
clr-sig: Vector128<float> CompareEqual(Vector128<float> left, Vector128<float> right);


docs: int _mm_comieq_ss (__m128 a, __m128 b) COMISS xmm, xmm/m32
clr-sig: bool CompareEqualOrderedScalar(Vector128<float> left, Vector128<float> right);


docs: __m128 _mm_cmpeq_ss (__m128 a, __m128 b) CMPSS xmm, xmm/m32, imm8(0)
clr-sig: Vector128<float> CompareEqualScalar(Vector128<float> left, Vector128<float> right);


docs: int _mm_ucomieq_ss (__m128 a, __m128 b) UCOMISS xmm, xmm/m32
clr-sig: bool CompareEqualUnorderedScalar(Vector128<float> left, Vector128<float> right);


docs: __m128 _mm_cmpgt_ps (__m128 a, __m128 b) CMPPS xmm, xmm/m128, imm8(6)
clr-sig: Vector128<float> CompareGreaterThan(Vector128<float> left, Vector128<float> right);


docs: int _mm_comigt_ss (__m128 a, __m128 b) COMISS xmm, xmm/m32
clr-sig: bool CompareGreaterThanOrderedScalar(Vector128<float> left, Vector128<float> right);


docs: __m128 _mm_cmpge_ps (__m128 a, __m128 b) CMPPS xmm, xmm/m128, imm8(5)
clr-sig: Vector128<float> CompareGreaterThanOrEqual(Vector128<float> left, Vector128<float> right);


docs: int _mm_comige_ss (__m128 a, __m128 b) COMISS xmm, xmm/m32
clr-sig: bool CompareGreaterThanOrEqualOrderedScalar(Vector128<float> left, Vector128<float> right);


docs: __m128 _mm_cmpge_ss (__m128 a, __m128 b) CMPPS xmm, xmm/m32, imm8(5)
clr-sig: Vector128<float> CompareGreaterThanOrEqualScalar(Vector128<float> left, Vector128<float> right);


docs: int _mm_ucomige_ss (__m128 a, __m128 b) UCOMISS xmm, xmm/m32
clr-sig: bool CompareGreaterThanOrEqualUnorderedScalar(Vector128<float> left, Vector128<float> right);


docs: __m128 _mm_cmpgt_ss (__m128 a, __m128 b) CMPSS xmm, xmm/m32, imm8(6)
clr-sig: Vector128<float> CompareGreaterThanScalar(Vector128<float> left, Vector128<float> right);


docs: int _mm_ucomigt_ss (__m128 a, __m128 b) UCOMISS xmm, xmm/m32
clr-sig: bool CompareGreaterThanUnorderedScalar(Vector128<float> left, Vector128<float> right);


docs: __m128 _mm_cmplt_ps (__m128 a, __m128 b) CMPPS xmm, xmm/m128, imm8(1)
clr-sig: Vector128<float> CompareLessThan(Vector128<float> left, Vector128<float> right);


docs: int _mm_comilt_ss (__m128 a, __m128 b) COMISS xmm, xmm/m32
clr-sig: bool CompareLessThanOrderedScalar(Vector128<float> left, Vector128<float> right);


docs: __m128 _mm_cmple_ps (__m128 a, __m128 b) CMPPS xmm, xmm/m128, imm8(2)
clr-sig: Vector128<float> CompareLessThanOrEqual(Vector128<float> left, Vector128<float> right);


docs: int _mm_comile_ss (__m128 a, __m128 b) COMISS xmm, xmm/m32
clr-sig: bool CompareLessThanOrEqualOrderedScalar(Vector128<float> left, Vector128<float> right);


docs: __m128 _mm_cmple_ss (__m128 a, __m128 b) CMPSS xmm, xmm/m32, imm8(2)
clr-sig: Vector128<float> CompareLessThanOrEqualScalar(Vector128<float> left, Vector128<float> right);


docs: int _mm_ucomile_ss (__m128 a, __m128 b) UCOMISS xmm, xmm/m32
clr-sig: bool CompareLessThanOrEqualUnorderedScalar(Vector128<float> left, Vector128<float> right);


docs: __m128 _mm_cmplt_ss (__m128 a, __m128 b) CMPSS xmm, xmm/m32, imm8(1)
clr-sig: Vector128<float> CompareLessThanScalar(Vector128<float> left, Vector128<float> right);


docs: int _mm_ucomilt_ss (__m128 a, __m128 b) UCOMISS xmm, xmm/m32
clr-sig: bool CompareLessThanUnorderedScalar(Vector128<float> left, Vector128<float> right);


docs: __m128 _mm_cmpneq_ps (__m128 a, __m128 b) CMPPS xmm, xmm/m128, imm8(4)
clr-sig: Vector128<float> CompareNotEqual(Vector128<float> left, Vector128<float> right);


docs: int _mm_comineq_ss (__m128 a, __m128 b) COMISS xmm, xmm/m32
clr-sig: bool CompareNotEqualOrderedScalar(Vector128<float> left, Vector128<float> right);


docs: __m128 _mm_cmpneq_ss (__m128 a, __m128 b) CMPSS xmm, xmm/m32, imm8(4)
clr-sig: Vector128<float> CompareNotEqualScalar(Vector128<float> left, Vector128<float> right);


docs: int _mm_ucomineq_ss (__m128 a, __m128 b) UCOMISS xmm, xmm/m32
clr-sig: bool CompareNotEqualUnorderedScalar(Vector128<float> left, Vector128<float> right);


docs: __m128 _mm_cmpngt_ps (__m128 a, __m128 b) CMPPS xmm, xmm/m128, imm8(2)
clr-sig: Vector128<float> CompareNotGreaterThan(Vector128<float> left, Vector128<float> right);


docs: __m128 _mm_cmpnge_ps (__m128 a, __m128 b) CMPPS xmm, xmm/m128, imm8(1)
clr-sig: Vector128<float> CompareNotGreaterThanOrEqual(Vector128<float> left, Vector128<float> right);


docs: __m128 _mm_cmpnge_ss (__m128 a, __m128 b) CMPSS xmm, xmm/m32, imm8(1)
clr-sig: Vector128<float> CompareNotGreaterThanOrEqualScalar(Vector128<float> left, Vector128<float> right);


docs: __m128 _mm_cmpngt_ss (__m128 a, __m128 b) CMPSS xmm, xmm/m32, imm8(2)
clr-sig: Vector128<float> CompareNotGreaterThanScalar(Vector128<float> left, Vector128<float> right);


docs: __m128 _mm_cmpnlt_ps (__m128 a, __m128 b) CMPPS xmm, xmm/m128, imm8(5)
clr-sig: Vector128<float> CompareNotLessThan(Vector128<float> left, Vector128<float> right);


docs: __m128 _mm_cmpnle_ps (__m128 a, __m128 b) CMPPS xmm, xmm/m128, imm8(6)
clr-sig: Vector128<float> CompareNotLessThanOrEqual(Vector128<float> left, Vector128<float> right);


docs: __m128 _mm_cmpnle_ss (__m128 a, __m128 b) CMPSS xmm, xmm/m32, imm8(6)
clr-sig: Vector128<float> CompareNotLessThanOrEqualScalar(Vector128<float> left, Vector128<float> right);


docs: __m128 _mm_cmpnlt_ss (__m128 a, __m128 b) CMPSS xmm, xmm/m32, imm8(5)
clr-sig: Vector128<float> CompareNotLessThanScalar(Vector128<float> left, Vector128<float> right);


docs: __m128 _mm_cmpord_ps (__m128 a, __m128 b) CMPPS xmm, xmm/m128, imm8(7)
clr-sig: Vector128<float> CompareOrdered(Vector128<float> left, Vector128<float> right);


docs: __m128 _mm_cmpord_ss (__m128 a, __m128 b) CMPSS xmm, xmm/m32, imm8(7)
clr-sig: Vector128<float> CompareOrderedScalar(Vector128<float> left, Vector128<float> right);


docs: __m128 _mm_cmpunord_ps (__m128 a, __m128 b) CMPPS xmm, xmm/m128, imm8(3)
clr-sig: Vector128<float> CompareUnordered(Vector128<float> left, Vector128<float> right);


docs: __m128 _mm_cmpunord_ss (__m128 a, __m128 b) CMPSS xmm, xmm/m32, imm8(3)
clr-sig: Vector128<float> CompareUnorderedScalar(Vector128<float> left, Vector128<float> right);


docs: __m128 _mm_cvtsi32_ss (__m128 a, int b) CVTSI2SS xmm, reg/m32
clr-sig: Vector128<float> ConvertScalarToVector128Single(Vector128<float> upper, int value);


docs: int _mm_cvtss_si32 (__m128 a) CVTSS2SI r32, xmm/m32
clr-sig: int ConvertToInt32(Vector128<float> value);


docs: int _mm_cvttss_si32 (__m128 a) CVTTSS2SI r32, xmm/m32
clr-sig: int ConvertToInt32WithTruncation(Vector128<float> value);


docs: __m128 _mm_div_ps (__m128 a, __m128 b) DIVPS xmm, xmm/m128
clr-sig: Vector128<float> Divide(Vector128<float> left, Vector128<float> right);


docs: __m128 _mm_div_ss (__m128 a, __m128 b) DIVSS xmm, xmm/m32
clr-sig: Vector128<float> DivideScalar(Vector128<float> left, Vector128<float> right);


docs: __m128 _mm_load_ps (float const* mem_address) MOVAPS xmm, m128
clr-sig: Vector128<float> LoadAlignedVector128(float* address);


docs: __m128 _mm_loadh_pi (__m128 a, __m64 const* mem_addr) MOVHPS xmm, m64
clr-sig: Vector128<float> LoadHigh(Vector128<float> lower, float* address);


docs: __m128 _mm_loadl_pi (__m128 a, __m64 const* mem_addr) MOVLPS xmm, m64
clr-sig: Vector128<float> LoadLow(Vector128<float> upper, float* address);


docs: __m128 _mm_load_ss (float const* mem_address) MOVSS xmm, m32
clr-sig: Vector128<float> LoadScalarVector128(float* address);


docs: __m128 _mm_loadu_ps (float const* mem_address) MOVUPS xmm, m128
clr-sig: Vector128<float> LoadVector128(float* address);


docs: __m128 _mm_max_ps (__m128 a, __m128 b) MAXPS xmm, xmm/m128
clr-sig: Vector128<float> Max(Vector128<float> left, Vector128<float> right);


docs: __m128 _mm_max_ss (__m128 a, __m128 b) MAXSS xmm, xmm/m32
clr-sig: Vector128<float> MaxScalar(Vector128<float> left, Vector128<float> right);


docs: __m128 _mm_min_ps (__m128 a, __m128 b) MINPS xmm, xmm/m128
clr-sig: Vector128<float> Min(Vector128<float> left, Vector128<float> right);


docs: __m128 _mm_min_ss (__m128 a, __m128 b) MINSS xmm, xmm/m32
clr-sig: Vector128<float> MinScalar(Vector128<float> left, Vector128<float> right);


docs: __m128 _mm_movehl_ps (__m128 a, __m128 b) MOVHLPS xmm, xmm
clr-sig: Vector128<float> MoveHighToLow(Vector128<float> left, Vector128<float> right);


docs: __m128 _mm_movelh_ps (__m128 a, __m128 b) MOVLHPS xmm, xmm
clr-sig: Vector128<float> MoveLowToHigh(Vector128<float> left, Vector128<float> right);


docs: int _mm_movemask_ps (__m128 a) MOVMSKPS reg, xmm
clr-sig: int MoveMask(Vector128<float> value);


docs: __m128 _mm_move_ss (__m128 a, __m128 b) MOVSS xmm, xmm
clr-sig: Vector128<float> MoveScalar(Vector128<float> upper, Vector128<float> value);


docs: __m128 _mm_mul_ps (__m128 a, __m128 b) MULPS xmm, xmm/m128
clr-sig: Vector128<float> Multiply(Vector128<float> left, Vector128<float> right);


docs: __m128 _mm_mul_ss (__m128 a, __m128 b) MULPS xmm, xmm/m32
clr-sig: Vector128<float> MultiplyScalar(Vector128<float> left, Vector128<float> right);


docs: __m128 _mm_or_ps (__m128 a, __m128 b) ORPS xmm, xmm/m128
clr-sig: Vector128<float> Or(Vector128<float> left, Vector128<float> right);


docs: void _mm_prefetch(char* p, int i) PREFETCHT0 m8
clr-sig: void Prefetch0(void* address);


docs: void _mm_prefetch(char* p, int i) PREFETCHT1 m8
clr-sig: void Prefetch1(void* address);


docs: void _mm_prefetch(char* p, int i) PREFETCHT2 m8
clr-sig: void Prefetch2(void* address);


docs: void _mm_prefetch(char* p, int i) PREFETCHNTA m8
clr-sig: void PrefetchNonTemporal(void* address);


docs: __m128 _mm_rcp_ps (__m128 a) RCPPS xmm, xmm/m128
clr-sig: Vector128<float> Reciprocal(Vector128<float> value);


docs: __m128 _mm_rcp_ss (__m128 a, __m128 b) RCPSS xmm, xmm/m32 The above native signature
docs: does not exist. We provide this additional overload for consistency with the
docs: other scalar APIs.
clr-sig: Vector128<float> ReciprocalScalar(Vector128<float> upper, Vector128<float> value);


docs: __m128 _mm_rcp_ss (__m128 a) RCPSS xmm, xmm/m32
clr-sig: Vector128<float> ReciprocalScalar(Vector128<float> value);


docs: __m128 _mm_rsqrt_ps (__m128 a) RSQRTPS xmm, xmm/m128
clr-sig: Vector128<float> ReciprocalSqrt(Vector128<float> value);


docs: __m128 _mm_rsqrt_ss (__m128 a) RSQRTSS xmm, xmm/m32
clr-sig: Vector128<float> ReciprocalSqrtScalar(Vector128<float> value);


docs: __m128 _mm_rsqrt_ss (__m128 a, __m128 b) RSQRTSS xmm, xmm/m32 The above native
docs: signature does not exist. We provide this additional overload for consistency
docs: with the other scalar APIs.
clr-sig: Vector128<float> ReciprocalSqrtScalar(Vector128<float> upper, Vector128<float> value);




docs: __m128 _mm_sqrt_ps (__m128 a) SQRTPS xmm, xmm/m128
clr-sig: Vector128<float> Sqrt(Vector128<float> value);


docs: __m128 _mm_sqrt_ss (__m128 a) SQRTSS xmm, xmm/m32
clr-sig: Vector128<float> SqrtScalar(Vector128<float> value);


docs: __m128 _mm_sqrt_ss (__m128 a, __m128 b) SQRTSS xmm, xmm/m32 The above native
docs: signature does not exist. We provide this additional overload for consistency
docs: with the other scalar APIs.
clr-sig: Vector128<float> SqrtScalar(Vector128<float> upper, Vector128<float> value);


docs: void _mm_storeu_ps (float* mem_addr, __m128 a) MOVUPS m128, xmm
clr-sig: void Store(float* address, Vector128<float> source);


docs: void _mm_store_ps (float* mem_addr, __m128 a) MOVAPS m128, xmm
clr-sig: void StoreAligned(float* address, Vector128<float> source);


docs: void _mm_stream_ps (float* mem_addr, __m128 a) MOVNTPS m128, xmm
clr-sig: void StoreAlignedNonTemporal(float* address, Vector128<float> source);


docs: void _mm_sfence(void) SFENCE
clr-sig: void StoreFence();


docs: void _mm_storeh_pi (__m64* mem_addr, __m128 a) MOVHPS m64, xmm
clr-sig: void StoreHigh(float* address, Vector128<float> source);


docs: void _mm_storel_pi (__m64* mem_addr, __m128 a) MOVLPS m64, xmm
clr-sig: void StoreLow(float* address, Vector128<float> source);


docs: void _mm_store_ss (float* mem_addr, __m128 a) MOVSS m32, xmm
clr-sig: void StoreScalar(float* address, Vector128<float> source);


docs: __m128d _mm_sub_ps (__m128d a, __m128d b) SUBPS xmm, xmm/m128
clr-sig: Vector128<float> Subtract(Vector128<float> left, Vector128<float> right);


docs: __m128 _mm_sub_ss (__m128 a, __m128 b) SUBSS xmm, xmm/m32
clr-sig: Vector128<float> SubtractScalar(Vector128<float> left, Vector128<float> right);





docs: __m128 _mm_xor_ps (__m128 a, __m128 b) XORPS xmm, xmm/m128
clr-sig: Vector128<float> Xor(Vector128<float> left, Vector128<float> right);

Sse.X64
-------------------------------------------------------------------------------
docs: __m128 _mm_cvtsi64_ss (__m128 a, __int64 b) CVTSI2SS xmm, reg/m64 This intrinisc
docs: is only available on 64-bit processes
clr-sig: Vector128<float> ConvertScalarToVector128Single(Vector128<float> upper, long value);


docs: __int64 _mm_cvtss_si64 (__m128 a) CVTSS2SI r64, xmm/m32 This intrinisc is only
docs: available on 64-bit processes
clr-sig: long ConvertToInt64(Vector128<float> value);


docs: __int64 _mm_cvttss_si64 (__m128 a) CVTTSS2SI r64, xmm/m32 This intrinisc is only
docs: available on 64-bit processes
clr-sig: long ConvertToInt64WithTruncation(Vector128<float> value);

Sse2 -> Sse
-------------------------------------------------------------------------------


docs: __m128i _mm_adds_epu8 (__m128i a, __m128i b) PADDUSB xmm, xmm/m128
clr-sig: Vector128<byte> AddSaturate(Vector128<byte> left, Vector128<byte> right);


docs: __m128i _mm_adds_epi16 (__m128i a, __m128i b) PADDSW xmm, xmm/m128
clr-sig: Vector128<short> AddSaturate(Vector128<short> left, Vector128<short> right);


docs: __m128i _mm_adds_epi8 (__m128i a, __m128i b) PADDSB xmm, xmm/m128
clr-sig: Vector128<sbyte> AddSaturate(Vector128<sbyte> left, Vector128<sbyte> right);


docs: __m128i _mm_adds_epu16 (__m128i a, __m128i b) PADDUSW xmm, xmm/m128
clr-sig: Vector128<ushort> AddSaturate(Vector128<ushort> left, Vector128<ushort> right);




docs: __m128i _mm_and_si128 (__m128i a, __m128i b) PAND xmm, xmm/m128
clr-sig: Vector128<uint> And(Vector128<uint> left, Vector128<uint> right);


docs: __m128i _mm_and_si128 (__m128i a, __m128i b) PAND xmm, xmm/m128
clr-sig: Vector128<ushort> And(Vector128<ushort> left, Vector128<ushort> right);


docs: __m128i _mm_and_si128 (__m128i a, __m128i b) PAND xmm, xmm/m128
clr-sig: Vector128<sbyte> And(Vector128<sbyte> left, Vector128<sbyte> right);


docs: __m128i _mm_and_si128 (__m128i a, __m128i b) PAND xmm, xmm/m128
clr-sig: Vector128<ulong> And(Vector128<ulong> left, Vector128<ulong> right);


docs: __m128i _mm_and_si128 (__m128i a, __m128i b) PAND xmm, xmm/m128
clr-sig: Vector128<int> And(Vector128<int> left, Vector128<int> right);


docs: __m128i _mm_and_si128 (__m128i a, __m128i b) PAND xmm, xmm/m128
clr-sig: Vector128<short> And(Vector128<short> left, Vector128<short> right);


docs: __m128d _mm_and_pd (__m128d a, __m128d b) ANDPD xmm, xmm/m128
clr-sig: Vector128<double> And(Vector128<double> left, Vector128<double> right);


docs: __m128i _mm_and_si128 (__m128i a, __m128i b) PAND xmm, xmm/m128
clr-sig: Vector128<byte> And(Vector128<byte> left, Vector128<byte> right);


docs: __m128i _mm_and_si128 (__m128i a, __m128i b) PAND xmm, xmm/m128
clr-sig: Vector128<long> And(Vector128<long> left, Vector128<long> right);




docs: __m128i _mm_avg_epu8 (__m128i a, __m128i b) PAVGB xmm, xmm/m128
clr-sig: Vector128<byte> Average(Vector128<byte> left, Vector128<byte> right);


docs: __m128i _mm_avg_epu16 (__m128i a, __m128i b) PAVGW xmm, xmm/m128
clr-sig: Vector128<ushort> Average(Vector128<ushort> left, Vector128<ushort> right);


docs: __m128i _mm_cmpeq_epi16 (__m128i a, __m128i b) PCMPEQW xmm, xmm/m128
clr-sig: Vector128<ushort> CompareEqual(Vector128<ushort> left, Vector128<ushort> right);


docs: __m128i _mm_cmpeq_epi8 (__m128i a, __m128i b) PCMPEQB xmm, xmm/m128
clr-sig: Vector128<sbyte> CompareEqual(Vector128<sbyte> left, Vector128<sbyte> right);


docs: __m128i _mm_cmpeq_epi32 (__m128i a, __m128i b) PCMPEQD xmm, xmm/m128
clr-sig: Vector128<uint> CompareEqual(Vector128<uint> left, Vector128<uint> right);


docs: __m128i _mm_cmpeq_epi16 (__m128i a, __m128i b) PCMPEQW xmm, xmm/m128
clr-sig: Vector128<short> CompareEqual(Vector128<short> left, Vector128<short> right);


docs: __m128d _mm_cmpeq_pd (__m128d a, __m128d b) CMPPD xmm, xmm/m128, imm8(0)
clr-sig: Vector128<double> CompareEqual(Vector128<double> left, Vector128<double> right);


docs: __m128i _mm_cmpeq_epi32 (__m128i a, __m128i b) PCMPEQD xmm, xmm/m128
clr-sig: Vector128<int> CompareEqual(Vector128<int> left, Vector128<int> right);


docs: __m128i _mm_cmpeq_epi8 (__m128i a, __m128i b) PCMPEQB xmm, xmm/m128
clr-sig: Vector128<byte> CompareEqual(Vector128<byte> left, Vector128<byte> right);


docs: int _mm_comieq_sd (__m128d a, __m128d b) COMISS xmm, xmm/m64
clr-sig: bool CompareEqualOrderedScalar(Vector128<double> left, Vector128<double> right);


docs: __m128d _mm_cmpeq_sd (__m128d a, __m128d b) CMPSD xmm, xmm/m64, imm8(0)
clr-sig: Vector128<double> CompareEqualScalar(Vector128<double> left, Vector128<double> right);


docs: int _mm_ucomieq_sd (__m128d a, __m128d b) UCOMISS xmm, xmm/m64
clr-sig: bool CompareEqualUnorderedScalar(Vector128<double> left, Vector128<double> right);


docs: __m128d _mm_cmpgt_pd (__m128d a, __m128d b) CMPPD xmm, xmm/m128, imm8(6)
clr-sig: Vector128<double> CompareGreaterThan(Vector128<double> left, Vector128<double> right);


docs: __m128i _mm_cmpgt_epi16 (__m128i a, __m128i b) PCMPGTW xmm, xmm/m128
clr-sig: Vector128<short> CompareGreaterThan(Vector128<short> left, Vector128<short> right);


docs: __m128i _mm_cmpgt_epi32 (__m128i a, __m128i b) PCMPGTD xmm, xmm/m128
clr-sig: Vector128<int> CompareGreaterThan(Vector128<int> left, Vector128<int> right);


docs: __m128i _mm_cmpgt_epi8 (__m128i a, __m128i b) PCMPGTB xmm, xmm/m128
clr-sig: Vector128<sbyte> CompareGreaterThan(Vector128<sbyte> left, Vector128<sbyte> right);


docs: int _mm_comigt_sd (__m128d a, __m128d b) COMISS xmm, xmm/m64
clr-sig: bool CompareGreaterThanOrderedScalar(Vector128<double> left, Vector128<double> right);


docs: __m128d _mm_cmpge_pd (__m128d a, __m128d b) CMPPD xmm, xmm/m128, imm8(5)
clr-sig: Vector128<double> CompareGreaterThanOrEqual(Vector128<double> left, Vector128<double> right);


docs: int _mm_comige_sd (__m128d a, __m128d b) COMISS xmm, xmm/m64
clr-sig: bool CompareGreaterThanOrEqualOrderedScalar(Vector128<double> left, Vector128<double> right);


docs: __m128d _mm_cmpge_sd (__m128d a, __m128d b) CMPSD xmm, xmm/m64, imm8(5)
clr-sig: Vector128<double> CompareGreaterThanOrEqualScalar(Vector128<double> left, Vector128<double> right);


docs: int _mm_ucomige_sd (__m128d a, __m128d b) UCOMISS xmm, xmm/m64
clr-sig: bool CompareGreaterThanOrEqualUnorderedScalar(Vector128<double> left, Vector128<double> right);


docs: __m128d _mm_cmpgt_sd (__m128d a, __m128d b) CMPSD xmm, xmm/m64, imm8(6)
clr-sig: Vector128<double> CompareGreaterThanScalar(Vector128<double> left, Vector128<double> right);


docs: int _mm_ucomigt_sd (__m128d a, __m128d b) UCOMISS xmm, xmm/m64
clr-sig: bool CompareGreaterThanUnorderedScalar(Vector128<double> left, Vector128<double> right);


docs: __m128i _mm_cmplt_epi32 (__m128i a, __m128i b) PCMPGTD xmm, xmm/m128
clr-sig: Vector128<int> CompareLessThan(Vector128<int> left, Vector128<int> right);


docs: __m128i _mm_cmplt_epi8 (__m128i a, __m128i b) PCMPGTB xmm, xmm/m128
clr-sig: Vector128<sbyte> CompareLessThan(Vector128<sbyte> left, Vector128<sbyte> right);


docs: __m128d _mm_cmplt_pd (__m128d a, __m128d b) CMPPD xmm, xmm/m128, imm8(1)
clr-sig: Vector128<double> CompareLessThan(Vector128<double> left, Vector128<double> right);


docs: __m128i _mm_cmplt_epi16 (__m128i a, __m128i b) PCMPGTW xmm, xmm/m128
clr-sig: Vector128<short> CompareLessThan(Vector128<short> left, Vector128<short> right);


docs: int _mm_comilt_sd (__m128d a, __m128d b) COMISS xmm, xmm/m64
clr-sig: bool CompareLessThanOrderedScalar(Vector128<double> left, Vector128<double> right);


docs: __m128d _mm_cmple_pd (__m128d a, __m128d b) CMPPD xmm, xmm/m128, imm8(2)
clr-sig: Vector128<double> CompareLessThanOrEqual(Vector128<double> left, Vector128<double> right);


docs: int _mm_comile_sd (__m128d a, __m128d b) COMISS xmm, xmm/m64
clr-sig: bool CompareLessThanOrEqualOrderedScalar(Vector128<double> left, Vector128<double> right);


docs: __m128d _mm_cmple_sd (__m128d a, __m128d b) CMPSD xmm, xmm/m64, imm8(2)
clr-sig: Vector128<double> CompareLessThanOrEqualScalar(Vector128<double> left, Vector128<double> right);


docs: int _mm_ucomile_sd (__m128d a, __m128d b) UCOMISS xmm, xmm/m64
clr-sig: bool CompareLessThanOrEqualUnorderedScalar(Vector128<double> left, Vector128<double> right);


docs: __m128d _mm_cmplt_sd (__m128d a, __m128d b) CMPSD xmm, xmm/m64, imm8(1)
clr-sig: Vector128<double> CompareLessThanScalar(Vector128<double> left, Vector128<double> right);


docs: int _mm_ucomilt_sd (__m128d a, __m128d b) UCOMISS xmm, xmm/m64
clr-sig: bool CompareLessThanUnorderedScalar(Vector128<double> left, Vector128<double> right);


docs: __m128d _mm_cmpneq_pd (__m128d a, __m128d b) CMPPD xmm, xmm/m128, imm8(4)
clr-sig: Vector128<double> CompareNotEqual(Vector128<double> left, Vector128<double> right);


docs: int _mm_comineq_sd (__m128d a, __m128d b) COMISS xmm, xmm/m64
clr-sig: bool CompareNotEqualOrderedScalar(Vector128<double> left, Vector128<double> right);


docs: __m128d _mm_cmpneq_sd (__m128d a, __m128d b) CMPSD xmm, xmm/m64, imm8(4)
clr-sig: Vector128<double> CompareNotEqualScalar(Vector128<double> left, Vector128<double> right);


docs: int _mm_ucomineq_sd (__m128d a, __m128d b) UCOMISS xmm, xmm/m64
clr-sig: bool CompareNotEqualUnorderedScalar(Vector128<double> left, Vector128<double> right);


docs: __m128d _mm_cmpngt_pd (__m128d a, __m128d b) CMPPD xmm, xmm/m128, imm8(2)
clr-sig: Vector128<double> CompareNotGreaterThan(Vector128<double> left, Vector128<double> right);


docs: __m128d _mm_cmpnge_pd (__m128d a, __m128d b) CMPPD xmm, xmm/m128, imm8(1)
clr-sig: Vector128<double> CompareNotGreaterThanOrEqual(Vector128<double> left, Vector128<double> right);


docs: __m128d _mm_cmpnge_sd (__m128d a, __m128d b) CMPSD xmm, xmm/m64, imm8(1)
clr-sig: Vector128<double> CompareNotGreaterThanOrEqualScalar(Vector128<double> left, Vector128<double> right);


docs: __m128d _mm_cmpngt_sd (__m128d a, __m128d b) CMPSD xmm, xmm/m64, imm8(2)
clr-sig: Vector128<double> CompareNotGreaterThanScalar(Vector128<double> left, Vector128<double> right);


docs: __m128d _mm_cmpnlt_pd (__m128d a, __m128d b) CMPPD xmm, xmm/m128, imm8(5)
clr-sig: Vector128<double> CompareNotLessThan(Vector128<double> left, Vector128<double> right);


docs: __m128d _mm_cmpnle_pd (__m128d a, __m128d b) CMPPD xmm, xmm/m128, imm8(6)
clr-sig: Vector128<double> CompareNotLessThanOrEqual(Vector128<double> left, Vector128<double> right);


docs: __m128d _mm_cmpnle_sd (__m128d a, __m128d b) CMPSD xmm, xmm/m64, imm8(6)
clr-sig: Vector128<double> CompareNotLessThanOrEqualScalar(Vector128<double> left, Vector128<double> right);


docs: __m128d _mm_cmpnlt_sd (__m128d a, __m128d b) CMPSD xmm, xmm/m64, imm8(5)
clr-sig: Vector128<double> CompareNotLessThanScalar(Vector128<double> left, Vector128<double> right);


docs: __m128d _mm_cmpord_pd (__m128d a, __m128d b) CMPPD xmm, xmm/m128, imm8(7)
clr-sig: Vector128<double> CompareOrdered(Vector128<double> left, Vector128<double> right);


docs: __m128d _mm_cmpord_sd (__m128d a, __m128d b) CMPSD xmm, xmm/m64, imm8(7)
clr-sig: Vector128<double> CompareOrderedScalar(Vector128<double> left, Vector128<double> right);


docs: __m128d _mm_cmpunord_pd (__m128d a, __m128d b) CMPPD xmm, xmm/m128, imm8(3)
clr-sig: Vector128<double> CompareUnordered(Vector128<double> left, Vector128<double> right);


docs: __m128d _mm_cmpunord_sd (__m128d a, __m128d b) CMPSD xmm, xmm/m64, imm8(3)
clr-sig: Vector128<double> CompareUnorderedScalar(Vector128<double> left, Vector128<double> right);


docs: __m128d _mm_cvtsi32_sd (__m128d a, int b) CVTSI2SD xmm, reg/m32
clr-sig: Vector128<double> ConvertScalarToVector128Double(Vector128<double> upper, int value);


docs: __m128d _mm_cvtss_sd (__m128d a, __m128 b) CVTSS2SD xmm, xmm/m32
clr-sig: Vector128<double> ConvertScalarToVector128Double(Vector128<double> upper, Vector128<float> value);


docs: int _mm256_cvtsi256_si32 (__m256i a) MOVD reg/m32, xmm
    int ConvertToInt32(Vector256<int> value);
    
docs: int _mm256_cvtsi256_si32 (__m256i a) MOVD reg/m32, xmm
    uint ConvertToUInt32(Vector256<uint> value);

docs: __m128i _mm_cvtsi32_si128 (int a) MOVD xmm, reg/m32
clr-sig: Vector128<int> ConvertScalarToVector128Int32(int value);
clr-sig: Vector128<uint> ConvertScalarToVector128UInt32(uint value);

docs: __m128 _mm_cvtsd_ss (__m128 a, __m128d b) CVTSD2SS xmm, xmm/m64
clr-sig: Vector128<float> ConvertScalarToVector128Single(Vector128<float> upper, Vector128<double> value);

docs: int _mm_cvtsd_si32 (__m128d a) CVTSD2SI r32, xmm/m64
clr-sig: int ConvertToInt32(Vector128<double> value);

docs: int _mm_cvtsi128_si32 (__m128i a) MOVD reg/m32, xmm
clr-sig: int ConvertToInt32(Vector128<int> value);


docs: int _mm_cvttsd_si32 (__m128d a) CVTTSD2SI reg, xmm/m64
clr-sig: int ConvertToInt32WithTruncation(Vector128<double> value);


docs: int _mm_cvtsi128_si32 (__m128i a) MOVD reg/m32, xmm
clr-sig: uint ConvertToUInt32(Vector128<uint> value);


docs: __m128d _mm_cvtepi32_pd (__m128i a) CVTDQ2PD xmm, xmm/m128
clr-sig: Vector128<double> ConvertToVector128Double(Vector128<int> value);


docs: __m128d _mm_cvtps_pd (__m128 a) CVTPS2PD xmm, xmm/m128
clr-sig: Vector128<double> ConvertToVector128Double(Vector128<float> value);


docs: __m128i _mm_cvtpd_epi32 (__m128d a) CVTPD2DQ xmm, xmm/m128
clr-sig: Vector128<int> ConvertToVector128Int32(Vector128<double> value);


docs: __m128i _mm_cvtps_epi32 (__m128 a) CVTPS2DQ xmm, xmm/m128
clr-sig: Vector128<int> ConvertToVector128Int32(Vector128<float> value);


docs: __m128i _mm_cvttpd_epi32 (__m128d a) CVTTPD2DQ xmm, xmm/m128
clr-sig: Vector128<int> ConvertToVector128Int32WithTruncation(Vector128<double> value);


docs: __m128i _mm_cvttps_epi32 (__m128 a) CVTTPS2DQ xmm, xmm/m128
clr-sig: Vector128<int> ConvertToVector128Int32WithTruncation(Vector128<float> value);


docs: __m128 _mm_cvtpd_ps (__m128d a) CVTPD2PS xmm, xmm/m128
clr-sig: Vector128<float> ConvertToVector128Single(Vector128<double> value);


docs: __m128 _mm_cvtepi32_ps (__m128i a) CVTDQ2PS xmm, xmm/m128
clr-sig: Vector128<float> ConvertToVector128Single(Vector128<int> value);


docs: __m128d _mm_div_pd (__m128d a, __m128d b) DIVPD xmm, xmm/m128
clr-sig: Vector128<double> Divide(Vector128<double> left, Vector128<double> right);


docs: __m128d _mm_div_sd (__m128d a, __m128d b) DIVSD xmm, xmm/m64
clr-sig: Vector128<double> DivideScalar(Vector128<double> left, Vector128<double> right);


docs: int _mm_extract_epi16 (__m128i a, int immediate) PEXTRW reg, xmm, imm8
clr-sig: ushort Extract(Vector128<ushort> value, byte index);




docs: __m128i _mm_load_si128 (__m128i const* mem_address) MOVDQA xmm, m128
clr-sig: Vector128<ushort> LoadAlignedVector128(ushort* address);


docs: __m128i _mm_load_si128 (__m128i const* mem_address) MOVDQA xmm, m128
clr-sig: Vector128<uint> LoadAlignedVector128(uint* address);


docs: __m128i _mm_load_si128 (__m128i const* mem_address) MOVDQA xmm, m128
clr-sig: Vector128<sbyte> LoadAlignedVector128(sbyte* address);


docs: __m128i _mm_load_si128 (__m128i const* mem_address) MOVDQA xmm, m128
clr-sig: Vector128<ulong> LoadAlignedVector128(ulong* address);


docs: __m128i _mm_load_si128 (__m128i const* mem_address) MOVDQA xmm, m128
clr-sig: Vector128<int> LoadAlignedVector128(int* address);


docs: __m128i _mm_load_si128 (__m128i const* mem_address) MOVDQA xmm, m128
clr-sig: Vector128<short> LoadAlignedVector128(short* address);


docs: __m128d _mm_load_pd (double const* mem_address) MOVAPD xmm, m128
clr-sig: Vector128<double> LoadAlignedVector128(double* address);


docs: __m128i _mm_load_si128 (__m128i const* mem_address) MOVDQA xmm, m128
clr-sig: Vector128<long> LoadAlignedVector128(long* address);


docs: __m128i _mm_load_si128 (__m128i const* mem_address) MOVDQA xmm, m128
clr-sig: Vector128<byte> LoadAlignedVector128(byte* address);


docs: void _mm_lfence(void) LFENCE
clr-sig: void LoadFence();


docs: __m128d _mm_loadh_pd (__m128d a, double const* mem_addr) MOVHPD xmm, m64
clr-sig: Vector128<double> LoadHigh(Vector128<double> lower, double* address);


docs: __m128d _mm_loadl_pd (__m128d a, double const* mem_addr) MOVLPD xmm, m64
clr-sig: Vector128<double> LoadLow(Vector128<double> upper, double* address);


docs: __m128d _mm_load_sd (double const* mem_address) MOVSD xmm, m64
clr-sig: Vector128<double> LoadScalarVector128(double* address);


docs: __m128i _mm_loadl_epi32 (__m128i const* mem_addr) MOVD xmm, reg/m32 The above
docs: native signature does not exist. We provide this additional overload for completeness.
clr-sig: Vector128<int> LoadScalarVector128(int* address);


docs: __m128i _mm_loadl_epi64 (__m128i const* mem_addr) MOVQ xmm, reg/m64
clr-sig: Vector128<long> LoadScalarVector128(long* address);


docs: __m128i _mm_loadl_epi32 (__m128i const* mem_addr) MOVD xmm, reg/m32 The above
docs: native signature does not exist. We provide this additional overload for completeness.
clr-sig: Vector128<uint> LoadScalarVector128(uint* address);


docs: __m128i _mm_loadl_epi64 (__m128i const* mem_addr) MOVQ xmm, reg/m64
clr-sig: Vector128<ulong> LoadScalarVector128(ulong* address);


docs: __m128i _mm_loadu_si128 (__m128i const* mem_address) MOVDQU xmm, m128
clr-sig: Vector128<uint> LoadVector128(uint* address);


docs: __m128i _mm_loadu_si128 (__m128i const* mem_address) MOVDQU xmm, m128
clr-sig: Vector128<ushort> LoadVector128(ushort* address);


docs: __m128i _mm_loadu_si128 (__m128i const* mem_address) MOVDQU xmm, m128
clr-sig: Vector128<sbyte> LoadVector128(sbyte* address);


docs: __m128i _mm_loadu_si128 (__m128i const* mem_address) MOVDQU xmm, m128
clr-sig: Vector128<ulong> LoadVector128(ulong* address);


docs: __m128i _mm_loadu_si128 (__m128i const* mem_address) MOVDQU xmm, m128
clr-sig: Vector128<int> LoadVector128(int* address);


docs: __m128i _mm_loadu_si128 (__m128i const* mem_address) MOVDQU xmm, m128
clr-sig: Vector128<short> LoadVector128(short* address);


docs: __m128d _mm_loadu_pd (double const* mem_address) MOVUPD xmm, m128
clr-sig: Vector128<double> LoadVector128(double* address);


docs: __m128i _mm_loadu_si128 (__m128i const* mem_address) MOVDQU xmm, m128
clr-sig: Vector128<long> LoadVector128(long* address);


docs: __m128i _mm_loadu_si128 (__m128i const* mem_address) MOVDQU xmm, m128
clr-sig: Vector128<byte> LoadVector128(byte* address);


docs: void _mm_maskmoveu_si128 (__m128i a, __m128i mask, char* mem_address) MASKMOVDQU
docs: xmm, xmm
clr-sig: void MaskMove(Vector128<byte> source, Vector128<byte> mask, byte* address);


docs: void _mm_maskmoveu_si128 (__m128i a, __m128i mask, char* mem_address) MASKMOVDQU
docs: xmm, xmm
clr-sig: void MaskMove(Vector128<sbyte> source, Vector128<sbyte> mask, sbyte* address);


docs: __m128i _mm_max_epi16 (__m128i a, __m128i b) PMAXSW xmm, xmm/m128
clr-sig: Vector128<short> Max(Vector128<short> left, Vector128<short> right);


docs: __m128i _mm_max_epu8 (__m128i a, __m128i b) PMAXUB xmm, xmm/m128
clr-sig: Vector128<byte> Max(Vector128<byte> left, Vector128<byte> right);


docs: __m128d _mm_max_pd (__m128d a, __m128d b) MAXPD xmm, xmm/m128
clr-sig: Vector128<double> Max(Vector128<double> left, Vector128<double> right);


docs: __m128d _mm_max_sd (__m128d a, __m128d b) MAXSD xmm, xmm/m64
clr-sig: Vector128<double> MaxScalar(Vector128<double> left, Vector128<double> right);


docs: void _mm_mfence(void) MFENCE
clr-sig: void MemoryFence();


docs: __m128d _mm_min_pd (__m128d a, __m128d b) MINPD xmm, xmm/m128
clr-sig: Vector128<double> Min(Vector128<double> left, Vector128<double> right);


docs: __m128i _mm_min_epi16 (__m128i a, __m128i b) PMINSW xmm, xmm/m128
clr-sig: Vector128<short> Min(Vector128<short> left, Vector128<short> right);


docs: __m128i _mm_min_epu8 (__m128i a, __m128i b) PMINUB xmm, xmm/m128
clr-sig: Vector128<byte> Min(Vector128<byte> left, Vector128<byte> right);


docs: __m128d _mm_min_sd (__m128d a, __m128d b) MINSD xmm, xmm/m64
clr-sig: Vector128<double> MinScalar(Vector128<double> left, Vector128<double> right);


docs: int _mm_movemask_epi8 (__m128i a) PMOVMSKB reg, xmm
clr-sig: int MoveMask(Vector128<byte> value);


docs: int _mm_movemask_pd (__m128d a) MOVMSKPD reg, xmm
clr-sig: int MoveMask(Vector128<double> value);


docs: int _mm_movemask_epi8 (__m128i a) PMOVMSKB reg, xmm
clr-sig: int MoveMask(Vector128<sbyte> value);


docs: __m128d _mm_move_sd (__m128d a, __m128d b) MOVSD xmm, xmm
clr-sig: Vector128<double> MoveScalar(Vector128<double> upper, Vector128<double> value);


docs: __m128i _mm_move_epi64 (__m128i a) MOVQ xmm, xmm
clr-sig: Vector128<long> MoveScalar(Vector128<long> value);


docs: __m128i _mm_move_epi64 (__m128i a) MOVQ xmm, xmm
clr-sig: Vector128<ulong> MoveScalar(Vector128<ulong> value);


docs: __m128i _mm_mul_epu32 (__m128i a, __m128i b) PMULUDQ xmm, xmm/m128
clr-sig: Vector128<ulong> Multiply(Vector128<uint> left, Vector128<uint> right);


docs: __m128d _mm_mul_pd (__m128d a, __m128d b) MULPD xmm, xmm/m128
clr-sig: Vector128<double> Multiply(Vector128<double> left, Vector128<double> right);


docs: __m128i _mm_madd_epi16 (__m128i a, __m128i b) PMADDWD xmm, xmm/m128
clr-sig: Vector128<int> MultiplyAddAdjacent(Vector128<short> left, Vector128<short> right);


docs: __m128i _mm_mulhi_epu16 (__m128i a, __m128i b) PMULHUW xmm, xmm/m128
clr-sig: Vector128<ushort> MultiplyHigh(Vector128<ushort> left, Vector128<ushort> right);


docs: __m128i _mm_mulhi_epi16 (__m128i a, __m128i b) PMULHW xmm, xmm/m128
clr-sig: Vector128<short> MultiplyHigh(Vector128<short> left, Vector128<short> right);


docs: __m128i _mm_mullo_epi16 (__m128i a, __m128i b) PMULLW xmm, xmm/m128
clr-sig: Vector128<ushort> MultiplyLow(Vector128<ushort> left, Vector128<ushort> right);


docs: __m128i _mm_mullo_epi16 (__m128i a, __m128i b) PMULLW xmm, xmm/m128
clr-sig: Vector128<short> MultiplyLow(Vector128<short> left, Vector128<short> right);


docs: __m128d _mm_mul_sd (__m128d a, __m128d b) MULSD xmm, xmm/m64
clr-sig: Vector128<double> MultiplyScalar(Vector128<double> left, Vector128<double> right);


docs: __m128i _mm_or_si128 (__m128i a, __m128i b) POR xmm, xmm/m128
clr-sig: Vector128<ulong> Or(Vector128<ulong> left, Vector128<ulong> right);


docs: __m128i _mm_or_si128 (__m128i a, __m128i b) POR xmm, xmm/m128
clr-sig: Vector128<ushort> Or(Vector128<ushort> left, Vector128<ushort> right);


docs: __m128i _mm_or_si128 (__m128i a, __m128i b) POR xmm, xmm/m128
clr-sig: Vector128<sbyte> Or(Vector128<sbyte> left, Vector128<sbyte> right);


docs: __m128i _mm_or_si128 (__m128i a, __m128i b) POR xmm, xmm/m128
clr-sig: Vector128<uint> Or(Vector128<uint> left, Vector128<uint> right);


docs: __m128i _mm_or_si128 (__m128i a, __m128i b) POR xmm, xmm/m128
clr-sig: Vector128<int> Or(Vector128<int> left, Vector128<int> right);


docs: __m128i _mm_or_si128 (__m128i a, __m128i b) POR xmm, xmm/m128
clr-sig: Vector128<short> Or(Vector128<short> left, Vector128<short> right);


docs: __m128d _mm_or_pd (__m128d a, __m128d b) ORPD xmm, xmm/m128
clr-sig: Vector128<double> Or(Vector128<double> left, Vector128<double> right);


docs: __m128i _mm_or_si128 (__m128i a, __m128i b) POR xmm, xmm/m128
clr-sig: Vector128<byte> Or(Vector128<byte> left, Vector128<byte> right);


docs: __m128i _mm_or_si128 (__m128i a, __m128i b) POR xmm, xmm/m128
clr-sig: Vector128<long> Or(Vector128<long> left, Vector128<long> right);


docs: __m128i _mm_packs_epi16 (__m128i a, __m128i b) PACKSSWB xmm, xmm/m128
clr-sig: Vector128<sbyte> PackSignedSaturate(Vector128<short> left, Vector128<short> right);


docs: __m128i _mm_packs_epi32 (__m128i a, __m128i b) PACKSSDW xmm, xmm/m128
clr-sig: Vector128<short> PackSignedSaturate(Vector128<int> left, Vector128<int> right);


docs: __m128i _mm_packus_epi16 (__m128i a, __m128i b) PACKUSWB xmm, xmm/m128
clr-sig: Vector128<byte> PackUnsignedSaturate(Vector128<short> left, Vector128<short> right);


docs: __m128i _mm_slli_epi32 (__m128i a, int immediate) PSLLD xmm, imm8
clr-sig: Vector128<uint> ShiftLeftLogical(Vector128<uint> value, byte count);


docs: __m128i _mm_sll_epi64 (__m128i a, __m128i count) PSLLQ xmm, xmm/m128
clr-sig: Vector128<ulong> ShiftLeftLogical(Vector128<ulong> value, Vector128<ulong> count);


docs: __m128i _mm_slli_epi64 (__m128i a, int immediate) PSLLQ xmm, imm8
clr-sig: Vector128<ulong> ShiftLeftLogical(Vector128<ulong> value, byte count);


docs: __m128i _mm_sll_epi32 (__m128i a, __m128i count) PSLLD xmm, xmm/m128
clr-sig: Vector128<uint> ShiftLeftLogical(Vector128<uint> value, Vector128<uint> count);


docs: __m128i _mm_sll_epi16 (__m128i a, __m128i count) PSLLW xmm, xmm/m128
clr-sig: Vector128<ushort> ShiftLeftLogical(Vector128<ushort> value, Vector128<ushort> count);


docs: __m128i _mm_sll_epi32 (__m128i a, __m128i count) PSLLD xmm, xmm/m128
clr-sig: Vector128<int> ShiftLeftLogical(Vector128<int> value, Vector128<int> count);


docs: __m128i _mm_sll_epi64 (__m128i a, __m128i count) PSLLQ xmm, xmm/m128
clr-sig: Vector128<long> ShiftLeftLogical(Vector128<long> value, Vector128<long> count);


docs: __m128i _mm_slli_epi64 (__m128i a, int immediate) PSLLQ xmm, imm8
clr-sig: Vector128<long> ShiftLeftLogical(Vector128<long> value, byte count);


docs: __m128i _mm_sll_epi16 (__m128i a, __m128i count) PSLLW xmm, xmm/m128
clr-sig: Vector128<short> ShiftLeftLogical(Vector128<short> value, Vector128<short> count);


docs: __m128i _mm_slli_epi16 (__m128i a, int immediate) PSLLW xmm, imm8
clr-sig: Vector128<short> ShiftLeftLogical(Vector128<short> value, byte count);


docs: __m128i _mm_slli_epi16 (__m128i a, int immediate) PSLLW xmm, imm8
clr-sig: Vector128<ushort> ShiftLeftLogical(Vector128<ushort> value, byte count);


docs: __m128i _mm_slli_epi32 (__m128i a, int immediate) PSLLD xmm, imm8
clr-sig: Vector128<int> ShiftLeftLogical(Vector128<int> value, byte count);


docs: __m128i _mm_bslli_si128 (__m128i a, int imm8) PSLLDQ xmm, imm8
clr-sig: Vector128<byte> ShiftLeftLogical128BitLane(Vector128<byte> value, byte numBytes);


docs: __m128i _mm_bslli_si128 (__m128i a, int imm8) PSLLDQ xmm, imm8
clr-sig: Vector128<short> ShiftLeftLogical128BitLane(Vector128<short> value, byte numBytes);


docs: __m128i _mm_bslli_si128 (__m128i a, int imm8) PSLLDQ xmm, imm8
clr-sig: Vector128<int> ShiftLeftLogical128BitLane(Vector128<int> value, byte numBytes);


docs: __m128i _mm_bslli_si128 (__m128i a, int imm8) PSLLDQ xmm, imm8
clr-sig: Vector128<long> ShiftLeftLogical128BitLane(Vector128<long> value, byte numBytes);


docs: __m128i _mm_bslli_si128 (__m128i a, int imm8) PSLLDQ xmm, imm8
clr-sig: Vector128<sbyte> ShiftLeftLogical128BitLane(Vector128<sbyte> value, byte numBytes);


docs: __m128i _mm_bslli_si128 (__m128i a, int imm8) PSLLDQ xmm, imm8
clr-sig: Vector128<ushort> ShiftLeftLogical128BitLane(Vector128<ushort> value, byte numBytes);


docs: __m128i _mm_bslli_si128 (__m128i a, int imm8) PSLLDQ xmm, imm8
clr-sig: Vector128<uint> ShiftLeftLogical128BitLane(Vector128<uint> value, byte numBytes);


docs: __m128i _mm_bslli_si128 (__m128i a, int imm8) PSLLDQ xmm, imm8
clr-sig: Vector128<ulong> ShiftLeftLogical128BitLane(Vector128<ulong> value, byte numBytes);


docs: __m128i _mm_sra_epi32 (__m128i a, __m128i count) PSRAD xmm, xmm/m128
clr-sig: Vector128<int> ShiftRightArithmetic(Vector128<int> value, Vector128<int> count);


docs: __m128i _mm_sra_epi16 (__m128i a, __m128i count) PSRAW xmm, xmm/m128
clr-sig: Vector128<short> ShiftRightArithmetic(Vector128<short> value, Vector128<short> count);


docs: __m128i _mm_srai_epi16 (__m128i a, int immediate) PSRAW xmm, imm8
clr-sig: Vector128<short> ShiftRightArithmetic(Vector128<short> value, byte count);


docs: __m128i _mm_srai_epi32 (__m128i a, int immediate) PSRAD xmm, imm8
clr-sig: Vector128<int> ShiftRightArithmetic(Vector128<int> value, byte count);


docs: __m128i _mm_srl_epi16 (__m128i a, __m128i count) PSRLW xmm, xmm/m128
clr-sig: Vector128<short> ShiftRightLogical(Vector128<short> value, Vector128<short> count);


docs: __m128i _mm_srli_epi32 (__m128i a, int immediate) PSRLD xmm, imm8
clr-sig: Vector128<int> ShiftRightLogical(Vector128<int> value, byte count);


docs: __m128i _mm_srl_epi32 (__m128i a, __m128i count) PSRLD xmm, xmm/m128
clr-sig: Vector128<int> ShiftRightLogical(Vector128<int> value, Vector128<int> count);


docs: __m128i _mm_srli_epi64 (__m128i a, int immediate) PSRLQ xmm, imm8
clr-sig: Vector128<long> ShiftRightLogical(Vector128<long> value, byte count);


docs: __m128i _mm_srl_epi64 (__m128i a, __m128i count) PSRLQ xmm, xmm/m128
clr-sig: Vector128<long> ShiftRightLogical(Vector128<long> value, Vector128<long> count);


docs: __m128i _mm_srli_epi16 (__m128i a, int immediate) PSRLW xmm, imm8
clr-sig: Vector128<ushort> ShiftRightLogical(Vector128<ushort> value, byte count);


docs: __m128i _mm_srli_epi32 (__m128i a, int immediate) PSRLD xmm, imm8
clr-sig: Vector128<uint> ShiftRightLogical(Vector128<uint> value, byte count);


docs: __m128i _mm_srl_epi32 (__m128i a, __m128i count) PSRLD xmm, xmm/m128
clr-sig: Vector128<uint> ShiftRightLogical(Vector128<uint> value, Vector128<uint> count);


docs: __m128i _mm_srli_epi64 (__m128i a, int immediate) PSRLQ xmm, imm8
clr-sig: Vector128<ulong> ShiftRightLogical(Vector128<ulong> value, byte count);


docs: __m128i _mm_srl_epi64 (__m128i a, __m128i count) PSRLQ xmm, xmm/m128
clr-sig: Vector128<ulong> ShiftRightLogical(Vector128<ulong> value, Vector128<ulong> count);


docs: __m128i _mm_srli_epi16 (__m128i a, int immediate) PSRLW xmm, imm8
clr-sig: Vector128<short> ShiftRightLogical(Vector128<short> value, byte count);


docs: __m128i _mm_srl_epi16 (__m128i a, __m128i count) PSRLW xmm, xmm/m128
clr-sig: Vector128<ushort> ShiftRightLogical(Vector128<ushort> value, Vector128<ushort> count);


docs: __m128i _mm_bsrli_si128 (__m128i a, int imm8) PSRLDQ xmm, imm8
clr-sig: Vector128<ushort> ShiftRightLogical128BitLane(Vector128<ushort> value, byte numBytes);


docs: __m128i _mm_bsrli_si128 (__m128i a, int imm8) PSRLDQ xmm, imm8
clr-sig: Vector128<ulong> ShiftRightLogical128BitLane(Vector128<ulong> value, byte numBytes);


docs: __m128i _mm_bsrli_si128 (__m128i a, int imm8) PSRLDQ xmm, imm8
clr-sig: Vector128<uint> ShiftRightLogical128BitLane(Vector128<uint> value, byte numBytes);


docs: __m128i _mm_bsrli_si128 (__m128i a, int imm8) PSRLDQ xmm, imm8
clr-sig: Vector128<sbyte> ShiftRightLogical128BitLane(Vector128<sbyte> value, byte numBytes);


docs: __m128i _mm_bsrli_si128 (__m128i a, int imm8) PSRLDQ xmm, imm8
clr-sig: Vector128<int> ShiftRightLogical128BitLane(Vector128<int> value, byte numBytes);


docs: __m128i _mm_bsrli_si128 (__m128i a, int imm8) PSRLDQ xmm, imm8
clr-sig: Vector128<short> ShiftRightLogical128BitLane(Vector128<short> value, byte numBytes);


docs: __m128i _mm_bsrli_si128 (__m128i a, int imm8) PSRLDQ xmm, imm8
clr-sig: Vector128<byte> ShiftRightLogical128BitLane(Vector128<byte> value, byte numBytes);


docs: __m128i _mm_bsrli_si128 (__m128i a, int imm8) PSRLDQ xmm, imm8
clr-sig: Vector128<long> ShiftRightLogical128BitLane(Vector128<long> value, byte numBytes);




docs: __m128d _mm_sqrt_pd (__m128d a) SQRTPD xmm, xmm/m128
clr-sig: Vector128<double> Sqrt(Vector128<double> value);


docs: __m128d _mm_sqrt_sd (__m128d a) SQRTSD xmm, xmm/64 The above native signature
docs: does not exist. We provide this additional overload for the recommended use case
docs: of this intrinsic.
clr-sig: Vector128<double> SqrtScalar(Vector128<double> value);


docs: __m128d _mm_sqrt_sd (__m128d a, __m128d b) SQRTSD xmm, xmm/64
clr-sig: Vector128<double> SqrtScalar(Vector128<double> upper, Vector128<double> value);


docs: void _mm_storeu_si128 (__m128i* mem_addr, __m128i a) MOVDQU m128, xmm
clr-sig: void Store(ulong* address, Vector128<ulong> source);


docs: void _mm_storeu_si128 (__m128i* mem_addr, __m128i a) MOVDQU m128, xmm
clr-sig: void Store(uint* address, Vector128<uint> source);


docs: void _mm_storeu_si128 (__m128i* mem_addr, __m128i a) MOVDQU m128, xmm
clr-sig: void Store(ushort* address, Vector128<ushort> source);


docs: void _mm_storeu_si128 (__m128i* mem_addr, __m128i a) MOVDQU m128, xmm
clr-sig: void Store(sbyte* address, Vector128<sbyte> source);


docs: void _mm_storeu_si128 (__m128i* mem_addr, __m128i a) MOVDQU m128, xmm
clr-sig: void Store(int* address, Vector128<int> source);


docs: void _mm_storeu_si128 (__m128i* mem_addr, __m128i a) MOVDQU m128, xmm
clr-sig: void Store(short* address, Vector128<short> source);


docs: void _mm_storeu_pd (double* mem_addr, __m128d a) MOVUPD m128, xmm
clr-sig: void Store(double* address, Vector128<double> source);


docs: void _mm_storeu_si128 (__m128i* mem_addr, __m128i a) MOVDQU m128, xmm
clr-sig: void Store(byte* address, Vector128<byte> source);


docs: void _mm_storeu_si128 (__m128i* mem_addr, __m128i a) MOVDQU m128, xmm
clr-sig: void Store(long* address, Vector128<long> source);


docs: void _mm_store_si128 (__m128i* mem_addr, __m128i a) MOVDQA m128, xmm
clr-sig: void StoreAligned(ulong* address, Vector128<ulong> source);


docs: void _mm_store_si128 (__m128i* mem_addr, __m128i a) MOVDQA m128, xmm
clr-sig: void StoreAligned(uint* address, Vector128<uint> source);


docs: void _mm_store_si128 (__m128i* mem_addr, __m128i a) MOVDQA m128, xmm
clr-sig: void StoreAligned(ushort* address, Vector128<ushort> source);


docs: void _mm_store_si128 (__m128i* mem_addr, __m128i a) MOVDQA m128, xmm
clr-sig: void StoreAligned(sbyte* address, Vector128<sbyte> source);


docs: void _mm_store_si128 (__m128i* mem_addr, __m128i a) MOVDQA m128, xmm
clr-sig: void StoreAligned(int* address, Vector128<int> source);


docs: void _mm_store_si128 (__m128i* mem_addr, __m128i a) MOVDQA m128, xmm
clr-sig: void StoreAligned(short* address, Vector128<short> source);


docs: void _mm_store_pd (double* mem_addr, __m128d a) MOVAPD m128, xmm
clr-sig: void StoreAligned(double* address, Vector128<double> source);


docs: void _mm_store_si128 (__m128i* mem_addr, __m128i a) MOVDQA m128, xmm
clr-sig: void StoreAligned(byte* address, Vector128<byte> source);


docs: void _mm_store_si128 (__m128i* mem_addr, __m128i a) MOVDQA m128, xmm
clr-sig: void StoreAligned(long* address, Vector128<long> source);


docs: void _mm_stream_si128 (__m128i* mem_addr, __m128i a) MOVNTDQ m128, xmm
clr-sig: void StoreAlignedNonTemporal(ushort* address, Vector128<ushort> source);


docs: void _mm_stream_si128 (__m128i* mem_addr, __m128i a) MOVNTDQ m128, xmm
clr-sig: void StoreAlignedNonTemporal(ulong* address, Vector128<ulong> source);


docs: void _mm_stream_si128 (__m128i* mem_addr, __m128i a) MOVNTDQ m128, xmm
clr-sig: void StoreAlignedNonTemporal(uint* address, Vector128<uint> source);


docs: void _mm_stream_si128 (__m128i* mem_addr, __m128i a) MOVNTDQ m128, xmm
clr-sig: void StoreAlignedNonTemporal(sbyte* address, Vector128<sbyte> source);


docs: void _mm_stream_si128 (__m128i* mem_addr, __m128i a) MOVNTDQ m128, xmm
clr-sig: void StoreAlignedNonTemporal(int* address, Vector128<int> source);


docs: void _mm_stream_si128 (__m128i* mem_addr, __m128i a) MOVNTDQ m128, xmm
clr-sig: void StoreAlignedNonTemporal(short* address, Vector128<short> source);


docs: void _mm_stream_pd (double* mem_addr, __m128d a) MOVNTPD m128, xmm
clr-sig: void StoreAlignedNonTemporal(double* address, Vector128<double> source);


docs: void _mm_stream_si128 (__m128i* mem_addr, __m128i a) MOVNTDQ m128, xmm
clr-sig: void StoreAlignedNonTemporal(byte* address, Vector128<byte> source);


docs: void _mm_stream_si128 (__m128i* mem_addr, __m128i a) MOVNTDQ m128, xmm
clr-sig: void StoreAlignedNonTemporal(long* address, Vector128<long> source);


docs: void _mm_storeh_pd (double* mem_addr, __m128d a) MOVHPD m64, xmm
clr-sig: void StoreHigh(double* address, Vector128<double> source);


docs: void _mm_storel_pd (double* mem_addr, __m128d a) MOVLPD m64, xmm
clr-sig: void StoreLow(double* address, Vector128<double> source);


docs: void _mm_storel_epi64 (__m128i* mem_addr, __m128i a) MOVQ m64, xmm
clr-sig: void StoreLow(long* address, Vector128<long> source);


docs: void _mm_storel_epi64 (__m128i* mem_addr, __m128i a) MOVQ m64, xmm
clr-sig: void StoreLow(ulong* address, Vector128<ulong> source);


docs: void _mm_stream_si32(int *p, int a) MOVNTI m32, r32
clr-sig: void StoreNonTemporal(int* address, int value);


docs: void _mm_stream_si32(int *p, int a) MOVNTI m32, r32
clr-sig: void StoreNonTemporal(uint* address, uint value);


docs: void _mm_store_sd (double* mem_addr, __m128d a) MOVSD m64, xmm
clr-sig: void StoreScalar(double* address, Vector128<double> source);


docs: __m128i _mm_sub_epi16 (__m128i a, __m128i b) PSUBW xmm, xmm/m128
clr-sig: Vector128<ushort> Subtract(Vector128<ushort> left, Vector128<ushort> right);


docs: __m128i _mm_sub_epi64 (__m128i a, __m128i b) PSUBQ xmm, xmm/m128
clr-sig: Vector128<ulong> Subtract(Vector128<ulong> left, Vector128<ulong> right);


docs: __m128i _mm_sub_epi8 (__m128i a, __m128i b) PSUBB xmm, xmm/m128
clr-sig: Vector128<sbyte> Subtract(Vector128<sbyte> left, Vector128<sbyte> right);


docs: __m128i _mm_sub_epi32 (__m128i a, __m128i b) PSUBD xmm, xmm/m128
clr-sig: Vector128<uint> Subtract(Vector128<uint> left, Vector128<uint> right);


docs: __m128i _mm_sub_epi32 (__m128i a, __m128i b) PSUBD xmm, xmm/m128
clr-sig: Vector128<int> Subtract(Vector128<int> left, Vector128<int> right);


docs: __m128i _mm_sub_epi16 (__m128i a, __m128i b) PSUBW xmm, xmm/m128
clr-sig: Vector128<short> Subtract(Vector128<short> left, Vector128<short> right);


docs: __m128d _mm_sub_pd (__m128d a, __m128d b) SUBPD xmm, xmm/m128
clr-sig: Vector128<double> Subtract(Vector128<double> left, Vector128<double> right);


docs: __m128i _mm_sub_epi8 (__m128i a, __m128i b) PSUBB xmm, xmm/m128
clr-sig: Vector128<byte> Subtract(Vector128<byte> left, Vector128<byte> right);


docs: __m128i _mm_sub_epi64 (__m128i a, __m128i b) PSUBQ xmm, xmm/m128
clr-sig: Vector128<long> Subtract(Vector128<long> left, Vector128<long> right);


docs: __m128i _mm_subs_epu8 (__m128i a, __m128i b) PSUBUSB xmm, xmm/m128
clr-sig: Vector128<byte> SubtractSaturate(Vector128<byte> left, Vector128<byte> right);


docs: __m128i _mm_subs_epi16 (__m128i a, __m128i b) PSUBSW xmm, xmm/m128
clr-sig: Vector128<short> SubtractSaturate(Vector128<short> left, Vector128<short> right);


docs: __m128i _mm_subs_epi8 (__m128i a, __m128i b) PSUBSB xmm, xmm/m128
clr-sig: Vector128<sbyte> SubtractSaturate(Vector128<sbyte> left, Vector128<sbyte> right);


docs: __m128i _mm_subs_epu16 (__m128i a, __m128i b) PSUBUSW xmm, xmm/m128
clr-sig: Vector128<ushort> SubtractSaturate(Vector128<ushort> left, Vector128<ushort> right);


docs: __m128d _mm_sub_sd (__m128d a, __m128d b) SUBSD xmm, xmm/m64
clr-sig: Vector128<double> SubtractScalar(Vector128<double> left, Vector128<double> right);


docs: __m128i _mm_sad_epu8 (__m128i a, __m128i b) PSADBW xmm, xmm/m128
clr-sig: Vector128<ushort> SumAbsoluteDifferences(Vector128<byte> left, Vector128<byte> right);




docs: __m128i _mm_xor_si128 (__m128i a, __m128i b) PXOR xmm, xmm/m128
clr-sig: Vector128<byte> Xor(Vector128<byte> left, Vector128<byte> right);


docs: __m128d _mm_xor_pd (__m128d a, __m128d b) XORPD xmm, xmm/m128
clr-sig: Vector128<double> Xor(Vector128<double> left, Vector128<double> right);


docs: __m128i _mm_xor_si128 (__m128i a, __m128i b) PXOR xmm, xmm/m128
clr-sig: Vector128<short> Xor(Vector128<short> left, Vector128<short> right);


docs: __m128i _mm_xor_si128 (__m128i a, __m128i b) PXOR xmm, xmm/m128
clr-sig: Vector128<int> Xor(Vector128<int> left, Vector128<int> right);


docs: __m128i _mm_xor_si128 (__m128i a, __m128i b) PXOR xmm, xmm/m128
clr-sig: Vector128<long> Xor(Vector128<long> left, Vector128<long> right);


docs: __m128i _mm_xor_si128 (__m128i a, __m128i b) PXOR xmm, xmm/m128
clr-sig: Vector128<sbyte> Xor(Vector128<sbyte> left, Vector128<sbyte> right);


docs: __m128i _mm_xor_si128 (__m128i a, __m128i b) PXOR xmm, xmm/m128
clr-sig: Vector128<ushort> Xor(Vector128<ushort> left, Vector128<ushort> right);


docs: __m128i _mm_xor_si128 (__m128i a, __m128i b) PXOR xmm, xmm/m128
clr-sig: Vector128<uint> Xor(Vector128<uint> left, Vector128<uint> right);


docs: __m128i _mm_xor_si128 (__m128i a, __m128i b) PXOR xmm, xmm/m128
clr-sig: Vector128<ulong> Xor(Vector128<ulong> left, Vector128<ulong> right);

public abstract class X64 : Sse.X64
{
clr-sig: bool IsSupported { get; }



docs: __m128d _mm_cvtsi64_sd (__m128d a, __int64 b) CVTSI2SD xmm, reg/m64 This intrinisc
docs: is only available on 64-bit processes
clr-sig: Vector128<double> ConvertScalarToVector128Double(Vector128<double> upper, long value);


docs: __m128i _mm_cvtsi64_si128 (__int64 a) MOVQ xmm, reg/m64 This intrinisc is only
docs: available on 64-bit processes
clr-sig: Vector128<long> ConvertScalarToVector128Int64(long value);


docs: __m128i _mm_cvtsi64_si128 (__int64 a) MOVQ xmm, reg/m64 This intrinisc is only
docs: available on 64-bit processes
clr-sig: Vector128<ulong> ConvertScalarToVector128UInt64(ulong value);


docs: __int64 _mm_cvtsd_si64 (__m128d a) CVTSD2SI r64, xmm/m64 This intrinisc is only
docs: available on 64-bit processes
clr-sig: long ConvertToInt64(Vector128<double> value);


docs: __int64 _mm_cvtsi128_si64 (__m128i a) MOVQ reg/m64, xmm This intrinisc is only
docs: available on 64-bit processes
clr-sig: long ConvertToInt64(Vector128<long> value);


docs: __int64 _mm_cvttsd_si64 (__m128d a) CVTTSD2SI reg, xmm/m64 This intrinisc is
docs: only available on 64-bit processes
clr-sig: long ConvertToInt64WithTruncation(Vector128<double> value);


docs: __int64 _mm_cvtsi128_si64 (__m128i a) MOVQ reg/m64, xmm This intrinisc is only
docs: available on 64-bit processes
clr-sig: ulong ConvertToUInt64(Vector128<ulong> value);


docs: void _mm_stream_si64(__int64 *p, __int64 a) MOVNTI m64, r64 This intrinisc is
docs: only available on 64-bit processes
clr-sig: void StoreNonTemporal(long* address, long value);


docs: void _mm_stream_si64(__int64 *p, __int64 a) MOVNTI m64, r64 This intrinisc is
docs: only available on 64-bit processes
clr-sig: void StoreNonTemporal(ulong* address, ulong value);


Sse3 -> Sse2 -> Sse
-------------------------------------------------------------------------------





docs: __m128d _mm_hadd_pd (__m128d a, __m128d b) HADDPD xmm, xmm/m128
clr-sig: Vector128<double> HorizontalAdd(Vector128<double> left, Vector128<double> right);


docs: __m128 _mm_hadd_ps (__m128 a, __m128 b) HADDPS xmm, xmm/m128
clr-sig: Vector128<float> HorizontalAdd(Vector128<float> left, Vector128<float> right);


docs: __m128d _mm_hsub_pd (__m128d a, __m128d b) HSUBPD xmm, xmm/m128
clr-sig: Vector128<double> HorizontalSubtract(Vector128<double> left, Vector128<double> right);


docs: __m128 _mm_hsub_ps (__m128 a, __m128 b) HSUBPS xmm, xmm/m128
clr-sig: Vector128<float> HorizontalSubtract(Vector128<float> left, Vector128<float> right);


docs: __m128d _mm_loaddup_pd (double const* mem_addr) MOVDDUP xmm, m64
clr-sig: Vector128<double> LoadAndDuplicateToVector128(double* address);

docs: __m128d _mm_movedup_pd (__m128d a) MOVDDUP xmm, xmm/m64
clr-sig: Vector128<double> MoveAndDuplicate(Vector128<double> source);

docs: __m128i _mm_lddqu_si128 (__m128i const* mem_addr) LDDQU xmm, m128
clr-sig: Vector128<byte> LoadDquVector128(byte* address);
clr-sig: Vector128<sbyte> LoadDquVector128(sbyte* address);
clr-sig: Vector128<short> LoadDquVector128(short* address);
clr-sig: Vector128<ushort> LoadDquVector128(ushort* address);
clr-sig: Vector128<int> LoadDquVector128(int* address);
clr-sig: Vector128<uint> LoadDquVector128(uint* address);
clr-sig: Vector128<long> LoadDquVector128(long* address);
clr-sig: Vector128<ulong> LoadDquVector128(ulong* address);


docs: __m256d _mm256_movedup_pd (__m256d a) VMOVDDUP ymm, ymm/m256
clr-sig: Vector256<double> DuplicateEvenIndexed(Vector256<double> value);

docs: __m128d _mm_broadcastsd_pd (__m128d a) VMOVDDUP xmm, xmm
clr-sig: Vector128<double> BroadcastScalarToVector128(Vector128<double> value);


docs: __m128 _mm_movehdup_ps (__m128 a) MOVSHDUP xmm, xmm/m128
clr-sig: Vector128<float> MoveHighAndDuplicate(Vector128<float> source);

docs: __m128 _mm_moveldup_ps (__m128 a) MOVSLDUP xmm, xmm/m128
clr-sig: Vector128<float> MoveLowAndDuplicate(Vector128<float> source);

Ssse3 -> Sse3 -> Sse2 -> Sse
-------------------------------------------------------------------------------



docs: __m128i _mm_alignr_epi8 (__m128i a, __m128i b, int count) PALIGNR xmm, xmm/m128,
docs: imm8 This intrinsic generates PALIGNR that operates over bytes rather than elements
docs: of the vectors.
clr-sig: Vector128<byte> AlignRight(Vector128<byte> left, Vector128<byte> right, byte mask);


docs: __m128i _mm_alignr_epi8 (__m128i a, __m128i b, int count) PALIGNR xmm, xmm/m128,
docs: imm8 This intrinsic generates PALIGNR that operates over bytes rather than elements
docs: of the vectors.
clr-sig: Vector128<short> AlignRight(Vector128<short> left, Vector128<short> right, byte mask);


docs: __m128i _mm_alignr_epi8 (__m128i a, __m128i b, int count) PALIGNR xmm, xmm/m128,
docs: imm8 This intrinsic generates PALIGNR that operates over bytes rather than elements
docs: of the vectors.
clr-sig: Vector128<int> AlignRight(Vector128<int> left, Vector128<int> right, byte mask);


docs: __m128i _mm_alignr_epi8 (__m128i a, __m128i b, int count) PALIGNR xmm, xmm/m128,
docs: imm8 This intrinsic generates PALIGNR that operates over bytes rather than elements
docs: of the vectors.
clr-sig: Vector128<long> AlignRight(Vector128<long> left, Vector128<long> right, byte mask);


docs: __m128i _mm_alignr_epi8 (__m128i a, __m128i b, int count) PALIGNR xmm, xmm/m128,
docs: imm8
clr-sig: Vector128<sbyte> AlignRight(Vector128<sbyte> left, Vector128<sbyte> right, byte mask);


docs: __m128i _mm_alignr_epi8 (__m128i a, __m128i b, int count) PALIGNR xmm, xmm/m128,
docs: imm8 This intrinsic generates PALIGNR that operates over bytes rather than elements
docs: of the vectors.
clr-sig: Vector128<ushort> AlignRight(Vector128<ushort> left, Vector128<ushort> right, byte mask);


docs: __m128i _mm_alignr_epi8 (__m128i a, __m128i b, int count) PALIGNR xmm, xmm/m128,
docs: imm8 This intrinsic generates PALIGNR that operates over bytes rather than elements
docs: of the vectors.
clr-sig: Vector128<uint> AlignRight(Vector128<uint> left, Vector128<uint> right, byte mask);


docs: __m128i _mm_alignr_epi8 (__m128i a, __m128i b, int count) PALIGNR xmm, xmm/m128,
docs: imm8 This intrinsic generates PALIGNR that operates over bytes rather than elements
docs: of the vectors.
clr-sig: Vector128<ulong> AlignRight(Vector128<ulong> left, Vector128<ulong> right, byte mask);


docs: __m128i _mm_hadd_epi32 (__m128i a, __m128i b) PHADDD xmm, xmm/m128
clr-sig: Vector128<int> HorizontalAdd(Vector128<int> left, Vector128<int> right);


docs: __m128i _mm_hadd_epi16 (__m128i a, __m128i b) PHADDW xmm, xmm/m128
clr-sig: Vector128<short> HorizontalAdd(Vector128<short> left, Vector128<short> right);


docs: __m128i _mm_hadds_epi16 (__m128i a, __m128i b) PHADDSW xmm, xmm/m128
clr-sig: Vector128<short> HorizontalAddSaturate(Vector128<short> left, Vector128<short> right);


docs: __m128i _mm_hsub_epi16 (__m128i a, __m128i b) PHSUBW xmm, xmm/m128
clr-sig: Vector128<short> HorizontalSubtract(Vector128<short> left, Vector128<short> right);


docs: __m128i _mm_hsub_epi32 (__m128i a, __m128i b) PHSUBD xmm, xmm/m128
clr-sig: Vector128<int> HorizontalSubtract(Vector128<int> left, Vector128<int> right);


docs: __m128i _mm_hsubs_epi16 (__m128i a, __m128i b) PHSUBSW xmm, xmm/m128
clr-sig: Vector128<short> HorizontalSubtractSaturate(Vector128<short> left, Vector128<short> right);




docs: __m128i _mm_mulhrs_epi16 (__m128i a, __m128i b) PMULHRSW xmm, xmm/m128
clr-sig: Vector128<short> MultiplyHighRoundScale(Vector128<short> left, Vector128<short> right);



docs: __m128i _mm_sign_epi16 (__m128i a, __m128i b) PSIGNW xmm, xmm/m128
clr-sig: Vector128<short> Sign(Vector128<short> left, Vector128<short> right);


docs: __m128i _mm_sign_epi32 (__m128i a, __m128i b) PSIGND xmm, xmm/m128
clr-sig: Vector128<int> Sign(Vector128<int> left, Vector128<int> right);


docs: __m128i _mm_sign_epi8 (__m128i a, __m128i b) PSIGNB xmm, xmm/m128
clr-sig: Vector128<sbyte> Sign(Vector128<sbyte> left, Vector128<sbyte> right);

Sse41 -> Ssse3 -> Sse3 -> Sse2 -> Sse
-------------------------------------------------------------------------------

docs: __m128i _mm_blend_epi16 (__m128i a, __m128i b, const int imm8) PBLENDW xmm, xmm/m128
docs: imm8
clr-sig: Vector128<ushort> Blend(Vector128<ushort> left, Vector128<ushort> right, byte control);


docs: __m128d _mm_blend_pd (__m128d a, __m128d b, const int imm8) BLENDPD xmm, xmm/m128,
docs: imm8
clr-sig: Vector128<double> Blend(Vector128<double> left, Vector128<double> right, byte control);


docs: __m128i _mm_blend_epi16 (__m128i a, __m128i b, const int imm8) PBLENDW xmm, xmm/m128
docs: imm8
clr-sig: Vector128<short> Blend(Vector128<short> left, Vector128<short> right, byte control);


docs: __m128 _mm_blend_ps (__m128 a, __m128 b, const int imm8) BLENDPS xmm, xmm/m128,
docs: imm8
clr-sig: Vector128<float> Blend(Vector128<float> left, Vector128<float> right, byte control);


docs: __m128d _mm_blendv_pd (__m128d a, __m128d b, __m128d mask) BLENDVPD xmm, xmm/m128,
docs: xmm0
clr-sig: Vector128<double> BlendVariable(Vector128<double> left, Vector128<double> right, Vector128<double> mask);


docs: __m128i _mm_blendv_epi8 (__m128i a, __m128i b, __m128i mask) PBLENDVB xmm, xmm/m128,
docs: xmm This intrinsic generates PBLENDVB that needs a BYTE mask-vector, so users
docs: should correctly set each mask byte for the selected elements.
clr-sig: Vector128<short> BlendVariable(Vector128<short> left, Vector128<short> right, Vector128<short> mask);


docs: __m128i _mm_blendv_epi8 (__m128i a, __m128i b, __m128i mask) PBLENDVB xmm, xmm/m128,
docs: xmm This intrinsic generates PBLENDVB that needs a BYTE mask-vector, so users
docs: should correctly set each mask byte for the selected elements.
clr-sig: Vector128<int> BlendVariable(Vector128<int> left, Vector128<int> right, Vector128<int> mask);


docs: __m128i _mm_blendv_epi8 (__m128i a, __m128i b, __m128i mask) PBLENDVB xmm, xmm/m128,
docs: xmm This intrinsic generates PBLENDVB that needs a BYTE mask-vector, so users
docs: should correctly set each mask byte for the selected elements.
clr-sig: Vector128<long> BlendVariable(Vector128<long> left, Vector128<long> right, Vector128<long> mask);


docs: __m128i _mm_blendv_epi8 (__m128i a, __m128i b, __m128i mask) PBLENDVB xmm, xmm/m128,
docs: xmm
clr-sig: Vector128<sbyte> BlendVariable(Vector128<sbyte> left, Vector128<sbyte> right, Vector128<sbyte> mask);


docs: __m128 _mm_blendv_ps (__m128 a, __m128 b, __m128 mask) BLENDVPS xmm, xmm/m128,
docs: xmm0
clr-sig: Vector128<float> BlendVariable(Vector128<float> left, Vector128<float> right, Vector128<float> mask);


docs: __m128i _mm_blendv_epi8 (__m128i a, __m128i b, __m128i mask) PBLENDVB xmm, xmm/m128,
docs: xmm This intrinsic generates PBLENDVB that needs a BYTE mask-vector, so users
docs: should correctly set each mask byte for the selected elements.
clr-sig: Vector128<ushort> BlendVariable(Vector128<ushort> left, Vector128<ushort> right, Vector128<ushort> mask);


docs: __m128i _mm_blendv_epi8 (__m128i a, __m128i b, __m128i mask) PBLENDVB xmm, xmm/m128,
docs: xmm This intrinsic generates PBLENDVB that needs a BYTE mask-vector, so users
docs: should correctly set each mask byte for the selected elements.
clr-sig: Vector128<uint> BlendVariable(Vector128<uint> left, Vector128<uint> right, Vector128<uint> mask);


docs: __m128i _mm_blendv_epi8 (__m128i a, __m128i b, __m128i mask) PBLENDVB xmm, xmm/m128,
docs: xmm This intrinsic generates PBLENDVB that needs a BYTE mask-vector, so users
docs: should correctly set each mask byte for the selected elements.
clr-sig: Vector128<ulong> BlendVariable(Vector128<ulong> left, Vector128<ulong> right, Vector128<ulong> mask);


docs: __m128i _mm_blendv_epi8 (__m128i a, __m128i b, __m128i mask) PBLENDVB xmm, xmm/m128,
docs: xmm
clr-sig: Vector128<byte> BlendVariable(Vector128<byte> left, Vector128<byte> right, Vector128<byte> mask);


docs: __m128d _mm_ceil_pd (__m128d a) ROUNDPD xmm, xmm/m128, imm8(10)
clr-sig: Vector128<double> Ceiling(Vector128<double> value);


docs: __m128 _mm_ceil_ps (__m128 a) ROUNDPS xmm, xmm/m128, imm8(10)
clr-sig: Vector128<float> Ceiling(Vector128<float> value);


docs: __m128 _mm_ceil_ss (__m128 a) ROUNDSD xmm, xmm/m128, imm8(10) The above native
docs: signature does not exist. We provide this additional overload for the recommended
docs: use case of this intrinsic.
clr-sig: Vector128<float> CeilingScalar(Vector128<float> value);


docs: __m128 _mm_ceil_ss (__m128 a, __m128 b) ROUNDSS xmm, xmm/m128, imm8(10)
clr-sig: Vector128<float> CeilingScalar(Vector128<float> upper, Vector128<float> value);


docs: __m128d _mm_ceil_sd (__m128d a) ROUNDSD xmm, xmm/m128, imm8(10) The above native
docs: signature does not exist. We provide this additional overload for the recommended
docs: use case of this intrinsic.
clr-sig: Vector128<double> CeilingScalar(Vector128<double> value);


docs: __m128d _mm_ceil_sd (__m128d a, __m128d b) ROUNDSD xmm, xmm/m128, imm8(10)
clr-sig: Vector128<double> CeilingScalar(Vector128<double> upper, Vector128<double> value);


docs: __m128i _mm_cmpeq_epi64 (__m128i a, __m128i b) PCMPEQQ xmm, xmm/m128
clr-sig: Vector128<long> CompareEqual(Vector128<long> left, Vector128<long> right);


docs: __m128i _mm_cmpeq_epi64 (__m128i a, __m128i b) PCMPEQQ xmm, xmm/m128
clr-sig: Vector128<ulong> CompareEqual(Vector128<ulong> left, Vector128<ulong> right);


docs: __m128i _mm_cvtepu8_epi16 (__m128i a) PMOVZXBW xmm, xmm/m64
clr-sig: Vector128<short> ConvertToVector128Int16(Vector128<byte> value);


docs: __m128i _mm_cvtepi8_epi16 (__m128i a) PMOVSXBW xmm, xmm/m64
clr-sig: Vector128<short> ConvertToVector128Int16(Vector128<sbyte> value);


docs: __m128i _mm_cvtepi8_epi32 (__m128i a) PMOVSXBD xmm, xmm/m32
clr-sig: Vector128<int> ConvertToVector128Int32(Vector128<sbyte> value);


docs: __m128i _mm_cvtepu16_epi32 (__m128i a) PMOVZXWD xmm, xmm/m64
clr-sig: Vector128<int> ConvertToVector128Int32(Vector128<ushort> value);


docs: __m128i _mm_cvtepu8_epi32 (__m128i a) PMOVZXBD xmm, xmm/m32
clr-sig: Vector128<int> ConvertToVector128Int32(Vector128<byte> value);


docs: __m128i _mm_cvtepi16_epi32 (__m128i a) PMOVSXWD xmm, xmm/m64
clr-sig: Vector128<int> ConvertToVector128Int32(Vector128<short> value);


docs: __m128i _mm_cvtepu8_epi64 (__m128i a) PMOVZXBQ xmm, xmm/m16
clr-sig: Vector128<long> ConvertToVector128Int64(Vector128<byte> value);


docs: __m128i _mm_cvtepi16_epi64 (__m128i a) PMOVSXWQ xmm, xmm/m32
clr-sig: Vector128<long> ConvertToVector128Int64(Vector128<short> value);


docs: __m128i _mm_cvtepi32_epi64 (__m128i a) PMOVSXDQ xmm, xmm/m64
clr-sig: Vector128<long> ConvertToVector128Int64(Vector128<int> value);


docs: __m128i _mm_cvtepi8_epi64 (__m128i a) PMOVSXBQ xmm, xmm/m16
clr-sig: Vector128<long> ConvertToVector128Int64(Vector128<sbyte> value);


docs: __m128i _mm_cvtepu16_epi64 (__m128i a) PMOVZXWQ xmm, xmm/m32
clr-sig: Vector128<long> ConvertToVector128Int64(Vector128<ushort> value);


docs: __m128i _mm_cvtepu32_epi64 (__m128i a) PMOVZXDQ xmm, xmm/m64
clr-sig: Vector128<long> ConvertToVector128Int64(Vector128<uint> value);


docs: __m128d _mm_dp_pd (__m128d a, __m128d b, const int imm8) DPPD xmm, xmm/m128,
docs: imm8
clr-sig: Vector128<double> DotProduct(Vector128<double> left, Vector128<double> right, byte control);


docs: __m128 _mm_dp_ps (__m128 a, __m128 b, const int imm8) DPPS xmm, xmm/m128, imm8
clr-sig: Vector128<float> DotProduct(Vector128<float> left, Vector128<float> right, byte control);


docs: int _mm_extract_epi32 (__m128i a, const int imm8) PEXTRD reg/m32, xmm, imm8
clr-sig: uint Extract(Vector128<uint> value, byte index);


docs: int _mm_extract_ps (__m128 a, const int imm8) EXTRACTPS xmm, xmm/m32, imm8
clr-sig: float Extract(Vector128<float> value, byte index);


docs: int _mm_extract_epi8 (__m128i a, const int imm8) PEXTRB reg/m8, xmm, imm8
clr-sig: byte Extract(Vector128<byte> value, byte index);


docs: int _mm_extract_epi32 (__m128i a, const int imm8) PEXTRD reg/m32, xmm, imm8
clr-sig: int Extract(Vector128<int> value, byte index);


docs: __m128d _mm_floor_pd (__m128d a) ROUNDPD xmm, xmm/m128, imm8(9)
clr-sig: Vector128<double> Floor(Vector128<double> value);


docs: __m128 _mm_floor_ps (__m128 a) ROUNDPS xmm, xmm/m128, imm8(9)
clr-sig: Vector128<float> Floor(Vector128<float> value);


docs: __m128d _mm_floor_sd (__m128d a) ROUNDSD xmm, xmm/m128, imm8(9) The above native
docs: signature does not exist. We provide this additional overload for the recommended
docs: use case of this intrinsic.
clr-sig: Vector128<double> FloorScalar(Vector128<double> value);


docs: __m128d _mm_floor_sd (__m128d a, __m128d b) ROUNDSD xmm, xmm/m128, imm8(9)
clr-sig: Vector128<double> FloorScalar(Vector128<double> upper, Vector128<double> value);


docs: __m128 _mm_floor_ss (__m128 a) ROUNDSS xmm, xmm/m128, imm8(9) The above native
docs: signature does not exist. We provide this additional overload for the recommended
docs: use case of this intrinsic.
clr-sig: Vector128<float> FloorScalar(Vector128<float> value);


docs: __m128 _mm_floor_ss (__m128 a, __m128 b) ROUNDSS xmm, xmm/m128, imm8(9)
clr-sig: Vector128<float> FloorScalar(Vector128<float> upper, Vector128<float> value);




docs: __m128i _mm_stream_load_si128 (const __m128i* mem_addr) MOVNTDQA xmm, m128
clr-sig: Vector128<ulong> LoadAlignedVector128NonTemporal(ulong* address);


docs: __m128i _mm_stream_load_si128 (const __m128i* mem_addr) MOVNTDQA xmm, m128
clr-sig: Vector128<ushort> LoadAlignedVector128NonTemporal(ushort* address);


docs: __m128i _mm_stream_load_si128 (const __m128i* mem_addr) MOVNTDQA xmm, m128
clr-sig: Vector128<sbyte> LoadAlignedVector128NonTemporal(sbyte* address);


docs: __m128i _mm_stream_load_si128 (const __m128i* mem_addr) MOVNTDQA xmm, m128
clr-sig: Vector128<uint> LoadAlignedVector128NonTemporal(uint* address);


docs: __m128i _mm_stream_load_si128 (const __m128i* mem_addr) MOVNTDQA xmm, m128
clr-sig: Vector128<int> LoadAlignedVector128NonTemporal(int* address);


docs: __m128i _mm_stream_load_si128 (const __m128i* mem_addr) MOVNTDQA xmm, m128
clr-sig: Vector128<short> LoadAlignedVector128NonTemporal(short* address);


docs: __m128i _mm_stream_load_si128 (const __m128i* mem_addr) MOVNTDQA xmm, m128
clr-sig: Vector128<byte> LoadAlignedVector128NonTemporal(byte* address);


docs: __m128i _mm_stream_load_si128 (const __m128i* mem_addr) MOVNTDQA xmm, m128
clr-sig: Vector128<long> LoadAlignedVector128NonTemporal(long* address);


docs: __m128i _mm_max_epi32 (__m128i a, __m128i b) PMAXSD xmm, xmm/m128
clr-sig: Vector128<int> Max(Vector128<int> left, Vector128<int> right);


docs: __m128i _mm_max_epi8 (__m128i a, __m128i b) PMAXSB xmm, xmm/m128
clr-sig: Vector128<sbyte> Max(Vector128<sbyte> left, Vector128<sbyte> right);


docs: __m128i _mm_max_epu16 (__m128i a, __m128i b) PMAXUW xmm, xmm/m128
clr-sig: Vector128<ushort> Max(Vector128<ushort> left, Vector128<ushort> right);


docs: __m128i _mm_max_epu32 (__m128i a, __m128i b) PMAXUD xmm, xmm/m128
clr-sig: Vector128<uint> Max(Vector128<uint> left, Vector128<uint> right);


docs: __m128i _mm_min_epu16 (__m128i a, __m128i b) PMINUW xmm, xmm/m128
clr-sig: Vector128<ushort> Min(Vector128<ushort> left, Vector128<ushort> right);


docs: __m128i _mm_min_epu32 (__m128i a, __m128i b) PMINUD xmm, xmm/m128
clr-sig: Vector128<uint> Min(Vector128<uint> left, Vector128<uint> right);


docs: __m128i _mm_min_epi32 (__m128i a, __m128i b) PMINSD xmm, xmm/m128
clr-sig: Vector128<int> Min(Vector128<int> left, Vector128<int> right);


docs: __m128i _mm_min_epi8 (__m128i a, __m128i b) PMINSB xmm, xmm/m128
clr-sig: Vector128<sbyte> Min(Vector128<sbyte> left, Vector128<sbyte> right);


docs: __m128i _mm_minpos_epu16 (__m128i a) PHMINPOSUW xmm, xmm/m128
clr-sig: Vector128<ushort> MinHorizontal(Vector128<ushort> value);


docs: __m128i _mm_mpsadbw_epu8 (__m128i a, __m128i b, const int imm8) MPSADBW xmm,
docs: xmm/m128, imm8
clr-sig: Vector128<ushort> MultipleSumAbsoluteDifferences(Vector128<byte> left, Vector128<byte> right, byte mask);


docs: __m128i _mm_mul_epi32 (__m128i a, __m128i b) PMULDQ xmm, xmm/m128
clr-sig: Vector128<long> Multiply(Vector128<int> left, Vector128<int> right);


docs: __m128i _mm_mullo_epi32 (__m128i a, __m128i b) PMULLD xmm, xmm/m128
clr-sig: Vector128<int> MultiplyLow(Vector128<int> left, Vector128<int> right);


docs: __m128i _mm_mullo_epi32 (__m128i a, __m128i b) PMULLD xmm, xmm/m128
clr-sig: Vector128<uint> MultiplyLow(Vector128<uint> left, Vector128<uint> right);


docs: __m128i _mm_packus_epi32 (__m128i a, __m128i b) PACKUSDW xmm, xmm/m128
clr-sig: Vector128<ushort> PackUnsignedSaturate(Vector128<int> left, Vector128<int> right);


docs: _MM_FROUND_CUR_DIRECTION; ROUNDPD xmm, xmm/m128, imm8(4)
clr-sig: Vector128<double> RoundCurrentDirection(Vector128<double> value);


docs: _MM_FROUND_CUR_DIRECTION; ROUNDPS xmm, xmm/m128, imm8(4)
clr-sig: Vector128<float> RoundCurrentDirection(Vector128<float> value);


docs: __m128 _mm_round_ss (__m128 a, __m128 b, _MM_FROUND_CUR_DIRECTION) ROUNDSS xmm,
docs: xmm/m128, imm8(4)
clr-sig: Vector128<float> RoundCurrentDirectionScalar(Vector128<float> upper, Vector128<float> value);


docs: __m128d _mm_round_sd (__m128d a, __m128d b, _MM_FROUND_CUR_DIRECTION) ROUNDSD
docs: xmm, xmm/m128, imm8(4)
clr-sig: Vector128<double> RoundCurrentDirectionScalar(Vector128<double> upper, Vector128<double> value);


docs: __m128d _mm_round_sd (__m128d a, _MM_FROUND_CUR_DIRECTION) ROUNDSD xmm, xmm/m128,
docs: imm8(4) The above native signature does not exist. We provide this additional
docs: overload for the recommended use case of this intrinsic.
clr-sig: Vector128<double> RoundCurrentDirectionScalar(Vector128<double> value);


docs: __m128 _mm_round_ss (__m128 a, _MM_FROUND_CUR_DIRECTION) ROUNDSS xmm, xmm/m128,
docs: imm8(4) The above native signature does not exist. We provide this additional
docs: overload for the recommended use case of this intrinsic.
clr-sig: Vector128<float> RoundCurrentDirectionScalar(Vector128<float> value);


docs: __m128 _mm_round_ps (__m128 a, int rounding) ROUNDPS xmm, xmm/m128, imm8(8) _MM_FROUND_TO_NEAREST_INT
docs: |_MM_FROUND_NO_EXC
clr-sig: Vector128<float> RoundToNearestInteger(Vector128<float> value);


docs: __m128d _mm_round_pd (__m128d a, int rounding) ROUNDPD xmm, xmm/m128, imm8(8)
docs: _MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC
clr-sig: Vector128<double> RoundToNearestInteger(Vector128<double> value);


docs: __m128d _mm_round_sd (__m128d a, _MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC)
docs: ROUNDSD xmm, xmm/m128, imm8(8) The above native signature does not exist. We
docs: provide this additional overload for the recommended use case of this intrinsic.
clr-sig: Vector128<double> RoundToNearestIntegerScalar(Vector128<double> value);


docs: __m128d _mm_round_sd (__m128d a, __m128d b, _MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC)
docs: ROUNDSD xmm, xmm/m128, imm8(8)
clr-sig: Vector128<double> RoundToNearestIntegerScalar(Vector128<double> upper, Vector128<double> value);


docs: __m128 _mm_round_ss (__m128 a, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC)
docs: ROUNDSS xmm, xmm/m128, imm8(8) The above native signature does not exist. We
docs: provide this additional overload for the recommended use case of this intrinsic.
clr-sig: Vector128<float> RoundToNearestIntegerScalar(Vector128<float> value);


docs: __m128 _mm_round_ss (__m128 a, __m128 b, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC)
docs: ROUNDSS xmm, xmm/m128, imm8(8)
clr-sig: Vector128<float> RoundToNearestIntegerScalar(Vector128<float> upper, Vector128<float> value);


docs: _MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC; ROUNDPD xmm, xmm/m128, imm8(9)
clr-sig: Vector128<double> RoundToNegativeInfinity(Vector128<double> value);


docs: _MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC; ROUNDPS xmm, xmm/m128, imm8(9)
clr-sig: Vector128<float> RoundToNegativeInfinity(Vector128<float> value);


docs: __m128d _mm_round_sd (__m128d a, _MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC) ROUNDSD
docs: xmm, xmm/m128, imm8(9) The above native signature does not exist. We provide
docs: this additional overload for the recommended use case of this intrinsic.
clr-sig: Vector128<double> RoundToNegativeInfinityScalar(Vector128<double> value);


docs: __m128d _mm_round_sd (__m128d a, __m128d b, _MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)
docs: ROUNDSD xmm, xmm/m128, imm8(9)
clr-sig: Vector128<double> RoundToNegativeInfinityScalar(Vector128<double> upper, Vector128<double> value);


docs: __m128 _mm_round_ss (__m128 a, _MM_FROUND_TO_NEG_INF | _MM_FROUND_NO_EXC) ROUNDSS
docs: xmm, xmm/m128, imm8(9) The above native signature does not exist. We provide
docs: this additional overload for the recommended use case of this intrinsic.
clr-sig: Vector128<float> RoundToNegativeInfinityScalar(Vector128<float> value);


docs: __m128 _mm_round_ss (__m128 a, __m128 b, _MM_FROUND_TO_NEG_INF | _MM_FROUND_NO_EXC)
docs: ROUNDSS xmm, xmm/m128, imm8(9)
clr-sig: Vector128<float> RoundToNegativeInfinityScalar(Vector128<float> upper, Vector128<float> value);


docs: _MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC; ROUNDPD xmm, xmm/m128, imm8(10)
clr-sig: Vector128<double> RoundToPositiveInfinity(Vector128<double> value);


docs: _MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC; ROUNDPS xmm, xmm/m128, imm8(10)
clr-sig: Vector128<float> RoundToPositiveInfinity(Vector128<float> value);


docs: __m128 _mm_round_ss (__m128 a, _MM_FROUND_TO_POS_INF | _MM_FROUND_NO_EXC) ROUNDSS
docs: xmm, xmm/m128, imm8(10) The above native signature does not exist. We provide
docs: this additional overload for the recommended use case of this intrinsic.
clr-sig: Vector128<float> RoundToPositiveInfinityScalar(Vector128<float> value);


docs: __m128 _mm_round_ss (__m128 a, __m128 b, _MM_FROUND_TO_POS_INF | _MM_FROUND_NO_EXC)
docs: ROUNDSS xmm, xmm/m128, imm8(10)
clr-sig: Vector128<float> RoundToPositiveInfinityScalar(Vector128<float> upper, Vector128<float> value);


docs: __m128d _mm_round_sd (__m128d a, _MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC) ROUNDSD
docs: xmm, xmm/m128, imm8(10) The above native signature does not exist. We provide
docs: this additional overload for the recommended use case of this intrinsic.
clr-sig: Vector128<double> RoundToPositiveInfinityScalar(Vector128<double> value);


docs: __m128d _mm_round_sd (__m128d a, __m128d b, _MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)
docs: ROUNDSD xmm, xmm/m128, imm8(10)
clr-sig: Vector128<double> RoundToPositiveInfinityScalar(Vector128<double> upper, Vector128<double> value);


docs: _MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC; ROUNDPD xmm, xmm/m128, imm8(11)
clr-sig: Vector128<double> RoundToZero(Vector128<double> value);


docs: _MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC; ROUNDPS xmm, xmm/m128, imm8(11)
clr-sig: Vector128<float> RoundToZero(Vector128<float> value);


docs: __m128d _mm_round_sd (__m128d a, _MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC) ROUNDSD
docs: xmm, xmm/m128, imm8(11) The above native signature does not exist. We provide
docs: this additional overload for the recommended use case of this intrinsic.
clr-sig: Vector128<double> RoundToZeroScalar(Vector128<double> value);


docs: __m128d _mm_round_sd (__m128d a, __m128d b, _MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)
docs: ROUNDSD xmm, xmm/m128, imm8(11)
clr-sig: Vector128<double> RoundToZeroScalar(Vector128<double> upper, Vector128<double> value);


docs: __m128 _mm_round_ss (__m128 a, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC) ROUNDSS
docs: xmm, xmm/m128, imm8(11) The above native signature does not exist. We provide
docs: this additional overload for the recommended use case of this intrinsic.
clr-sig: Vector128<float> RoundToZeroScalar(Vector128<float> value);


docs: __m128 _mm_round_ss (__m128 a, __m128 b, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC)
docs: ROUNDSS xmm, xmm/m128, imm8(11)
clr-sig: Vector128<float> RoundToZeroScalar(Vector128<float> upper, Vector128<float> value);
clr-sig: bool TestAllOnes(Vector128<ulong> value);
clr-sig: bool TestAllOnes(Vector128<uint> value);
clr-sig: bool TestAllOnes(Vector128<ushort> value);
clr-sig: bool TestAllOnes(Vector128<int> value);
clr-sig: bool TestAllOnes(Vector128<long> value);
clr-sig: bool TestAllOnes(Vector128<short> value);
clr-sig: bool TestAllOnes(Vector128<byte> value);


docs: int _mm_test_all_ones (__m128i a) PCMPEQD xmm, xmm/m128 PTEST xmm, xmm/m128
clr-sig: bool TestAllOnes(Vector128<sbyte> value);
clr-sig: bool TestAllZeros(Vector128<ulong> left, Vector128<ulong> right);
clr-sig: bool TestAllZeros(Vector128<uint> left, Vector128<uint> right);
clr-sig: bool TestAllZeros(Vector128<ushort> left, Vector128<ushort> right);


docs: int _mm_test_all_zeros (__m128i a, __m128i mask) PTEST xmm, xmm/m128
clr-sig: bool TestAllZeros(Vector128<sbyte> left, Vector128<sbyte> right);
clr-sig: bool TestAllZeros(Vector128<int> left, Vector128<int> right);
clr-sig: bool TestAllZeros(Vector128<short> left, Vector128<short> right);
clr-sig: bool TestAllZeros(Vector128<byte> left, Vector128<byte> right);
clr-sig: bool TestAllZeros(Vector128<long> left, Vector128<long> right);
clr-sig: bool TestC(Vector128<byte> left, Vector128<byte> right);
clr-sig: bool TestC(Vector128<short> left, Vector128<short> right);
clr-sig: bool TestC(Vector128<int> left, Vector128<int> right);
clr-sig: bool TestC(Vector128<long> left, Vector128<long> right);


docs: int _mm_testc_si128 (__m128i a, __m128i b) PTEST xmm, xmm/m128
clr-sig: bool TestC(Vector128<sbyte> left, Vector128<sbyte> right);
clr-sig: bool TestC(Vector128<ushort> left, Vector128<ushort> right);
clr-sig: bool TestC(Vector128<uint> left, Vector128<uint> right);
clr-sig: bool TestC(Vector128<ulong> left, Vector128<ulong> right);
clr-sig: bool TestMixOnesZeros(Vector128<uint> left, Vector128<uint> right);
clr-sig: bool TestMixOnesZeros(Vector128<ushort> left, Vector128<ushort> right);


docs: int _mm_test_mix_ones_zeros (__m128i a, __m128i mask) PTEST xmm, xmm/m128
clr-sig: bool TestMixOnesZeros(Vector128<sbyte> left, Vector128<sbyte> right);
clr-sig: bool TestMixOnesZeros(Vector128<ulong> left, Vector128<ulong> right);
clr-sig: bool TestMixOnesZeros(Vector128<int> left, Vector128<int> right);
clr-sig: bool TestMixOnesZeros(Vector128<short> left, Vector128<short> right);
clr-sig: bool TestMixOnesZeros(Vector128<byte> left, Vector128<byte> right);
clr-sig: bool TestMixOnesZeros(Vector128<long> left, Vector128<long> right);
clr-sig: bool TestNotZAndNotC(Vector128<uint> left, Vector128<uint> right);
clr-sig: bool TestNotZAndNotC(Vector128<ushort> left, Vector128<ushort> right);


docs: int _mm_testnzc_si128 (__m128i a, __m128i b) PTEST xmm, xmm/m128
clr-sig: bool TestNotZAndNotC(Vector128<sbyte> left, Vector128<sbyte> right);
clr-sig: bool TestNotZAndNotC(Vector128<ulong> left, Vector128<ulong> right);
clr-sig: bool TestNotZAndNotC(Vector128<int> left, Vector128<int> right);
clr-sig: bool TestNotZAndNotC(Vector128<short> left, Vector128<short> right);
clr-sig: bool TestNotZAndNotC(Vector128<byte> left, Vector128<byte> right);
clr-sig: bool TestNotZAndNotC(Vector128<long> left, Vector128<long> right);
clr-sig: bool TestZ(Vector128<byte> left, Vector128<byte> right);
clr-sig: bool TestZ(Vector128<short> left, Vector128<short> right);
clr-sig: bool TestZ(Vector128<int> left, Vector128<int> right);
clr-sig: bool TestZ(Vector128<long> left, Vector128<long> right);


docs: int _mm_testz_si128 (__m128i a, __m128i b) PTEST xmm, xmm/m128
clr-sig: bool TestZ(Vector128<sbyte> left, Vector128<sbyte> right);
clr-sig: bool TestZ(Vector128<ushort> left, Vector128<ushort> right);
clr-sig: bool TestZ(Vector128<uint> left, Vector128<uint> right);
clr-sig: bool TestZ(Vector128<ulong> left, Vector128<ulong> right);

Sse2.X64 <- X64
-------------------------------------------------------------------------------    
docs:
docs:     __int64 _mm_extract_epi64 (__m128i a, const int imm8) PEXTRQ reg/m64, xmm, imm8
docs:     This intrinisc is only available on 64-bit processes
    clr-sig: long Extract(Vector128<long> value, byte index);
    
docs:
docs:     __int64 _mm_extract_epi64 (__m128i a, const int imm8) PEXTRQ reg/m64, xmm, imm8
docs:     This intrinisc is only available on 64-bit processes
    clr-sig: ulong Extract(Vector128<ulong> value, byte index);
    

Sse42 -> Sse41 -> Ssse3 -> Sse3 -> Sse2 -> Sse
-------------------------------------------------------------------------------
docs: __m128i _mm_cmpgt_epi64 (__m128i a, __m128i b) PCMPGTQ xmm, xmm/m128
clr sig: Vector128<long> CompareGreaterThan(Vector128<long> left, Vector128<long> right);

docs: unsigned int _mm_crc32_u8 (unsigned int crc, unsigned char v) CRC32 reg, reg/m8
clr sig: uint Crc32(uint crc, byte data);

docs: unsigned int _mm_crc32_u16 (unsigned int crc, unsigned short v) CRC32 reg, reg/m16
clr sig: uint Crc32(uint crc, ushort data);


docs: unsigned int _mm_crc32_u32 (unsigned int crc, unsigned int v) CRC32 reg, reg/m32
clr sig: uint Crc32(uint crc, uint data);

Sse41.X64 <- Sse42.X64    
-------------------------------------------------------------------------------
docs: unsigned __int64 _mm_crc32_u64 (unsigned __int64 crc, unsigned __int64 v) CRC32
docs: reg, reg/m64 This intrinisc is only available on 64-bit processes
clr sig: ulong Crc32(ulong crc, ulong data);

Avx -> Sse42 -> Sse41 -> Ssse3 -> Sse3 -> Sse2 -> Sse
-------------------------------------------------------------------------------
docs: __m256d _mm256_and_pd (__m256d a, __m256d b) VANDPD ymm, ymm, ymm/m256
clr-sig: Vector256<double> And(Vector256<double> left, Vector256<double> right);


docs: __m256 _mm256_and_ps (__m256 a, __m256 b) VANDPS ymm, ymm, ymm/m256
clr-sig: Vector256<float> And(Vector256<float> left, Vector256<float> right);




docs: __m256d _mm256_blend_pd (__m256d a, __m256d b, const int imm8) VBLENDPD ymm,
docs: ymm, ymm/m256, imm8
clr-sig: Vector256<double> Blend(Vector256<double> left, Vector256<double> right, byte control);


docs: __m256 _mm256_blend_ps (__m256 a, __m256 b, const int imm8) VBLENDPS ymm, ymm,
docs: ymm/m256, imm8
clr-sig: Vector256<float> Blend(Vector256<float> left, Vector256<float> right, byte control);


docs: __m256d _mm256_blendv_pd (__m256d a, __m256d b, __m256d mask) VBLENDVPD ymm,
docs: ymm, ymm/m256, ymm
clr-sig: Vector256<double> BlendVariable(Vector256<double> left, Vector256<double> right, Vector256<double> mask);


docs: __m256 _mm256_blendv_ps (__m256 a, __m256 b, __m256 mask) VBLENDVPS ymm, ymm,
docs: ymm/m256, ymm
clr-sig: Vector256<float> BlendVariable(Vector256<float> left, Vector256<float> right, Vector256<float> mask);


docs: __m128 _mm_broadcast_ss (float const * mem_addr) VBROADCASTSS xmm, m32
clr-sig: Vector128<float> BroadcastScalarToVector128(float* source);


docs: __m256d _mm256_broadcast_sd (double const * mem_addr) VBROADCASTSD ymm, m64
clr-sig: Vector256<double> BroadcastScalarToVector256(double* source);


docs: __m256 _mm256_broadcast_ss (float const * mem_addr) VBROADCASTSS ymm, m32
clr-sig: Vector256<float> BroadcastScalarToVector256(float* source);


docs: __m256d _mm256_broadcast_pd (__m128d const * mem_addr) VBROADCASTF128, ymm, m128
clr-sig: Vector256<double> BroadcastVector128ToVector256(double* address);


docs: __m256 _mm256_broadcast_ps (__m128 const * mem_addr) VBROADCASTF128, ymm, m128
clr-sig: Vector256<float> BroadcastVector128ToVector256(float* address);


docs: __m256 _mm256_ceil_ps (__m256 a) VROUNDPS ymm, ymm/m256, imm8(10)
clr-sig: Vector256<float> Ceiling(Vector256<float> value);


docs: __m256d _mm256_ceil_pd (__m256d a) VROUNDPD ymm, ymm/m256, imm8(10)
clr-sig: Vector256<double> Ceiling(Vector256<double> value);


docs: __m128d _mm_cmp_pd (__m128d a, __m128d b, const int imm8) VCMPPD xmm, xmm, xmm/m128,
docs: imm8
clr-sig: Vector128<double> Compare(Vector128<double> left, Vector128<double> right, FloatComparisonMode mode);


docs: __m128 _mm_cmp_ps (__m128 a, __m128 b, const int imm8) VCMPPS xmm, xmm, xmm/m128,
docs: imm8
clr-sig: Vector128<float> Compare(Vector128<float> left, Vector128<float> right, FloatComparisonMode mode);


docs: __m256d _mm256_cmp_pd (__m256d a, __m256d b, const int imm8) VCMPPD ymm, ymm,
docs: ymm/m256, imm8
clr-sig: Vector256<double> Compare(Vector256<double> left, Vector256<double> right, FloatComparisonMode mode);


docs: __m256 _mm256_cmp_ps (__m256 a, __m256 b, const int imm8) VCMPPS ymm, ymm, ymm/m256,
docs: imm8
clr-sig: Vector256<float> Compare(Vector256<float> left, Vector256<float> right, FloatComparisonMode mode);


docs: __m128d _mm_cmp_sd (__m128d a, __m128d b, const int imm8) VCMPSS xmm, xmm, xmm/m32,
docs: imm8
clr-sig: Vector128<double> CompareScalar(Vector128<double> left, Vector128<double> right, FloatComparisonMode mode);


docs: __m128 _mm_cmp_ss (__m128 a, __m128 b, const int imm8) VCMPSD xmm, xmm, xmm/m64,
docs: imm8
clr-sig: Vector128<float> CompareScalar(Vector128<float> left, Vector128<float> right, FloatComparisonMode mode);


docs: __m128i _mm256_cvtpd_epi32 (__m256d a) VCVTPD2DQ xmm, ymm/m256
clr-sig: Vector128<int> ConvertToVector128Int32(Vector256<double> value);


docs: __m128i _mm256_cvttpd_epi32 (__m256d a) VCVTTPD2DQ xmm, ymm/m256
clr-sig: Vector128<int> ConvertToVector128Int32WithTruncation(Vector256<double> value);


docs: __m128 _mm256_cvtpd_ps (__m256d a) VCVTPD2PS xmm, ymm/m256
clr-sig: Vector128<float> ConvertToVector128Single(Vector256<double> value);


docs: __m256d _mm256_cvtepi32_pd (__m128i a) VCVTDQ2PD ymm, xmm/m128
clr-sig: Vector256<double> ConvertToVector256Double(Vector128<int> value);


docs: __m256d _mm256_cvtps_pd (__m128 a) VCVTPS2PD ymm, xmm/m128
clr-sig: Vector256<double> ConvertToVector256Double(Vector128<float> value);


docs: __m256i _mm256_cvtps_epi32 (__m256 a) VCVTPS2DQ ymm, ymm/m256
clr-sig: Vector256<int> ConvertToVector256Int32(Vector256<float> value);


docs: __m256i _mm256_cvttps_epi32 (__m256 a) VCVTTPS2DQ ymm, ymm/m256
clr-sig: Vector256<int> ConvertToVector256Int32WithTruncation(Vector256<float> value);


docs: __m256 _mm256_cvtepi32_ps (__m256i a) VCVTDQ2PS ymm, ymm/m256
clr-sig: Vector256<float> ConvertToVector256Single(Vector256<int> value);


docs: __m256d _mm256_div_pd (__m256d a, __m256d b) VDIVPD ymm, ymm, ymm/m256
clr-sig: Vector256<double> Divide(Vector256<double> left, Vector256<double> right);


docs: __m256 _mm256_div_ps (__m256 a, __m256 b) VDIVPS ymm, ymm, ymm/m256
clr-sig: Vector256<float> Divide(Vector256<float> left, Vector256<float> right);


docs: __m256 _mm256_dp_ps (__m256 a, __m256 b, const int imm8) VDPPS ymm, ymm, ymm/m256,
docs: imm8
clr-sig: Vector256<float> DotProduct(Vector256<float> left, Vector256<float> right, byte control);



docs: __m256 _mm256_moveldup_ps (__m256 a) VMOVSLDUP ymm, ymm/m256
clr-sig: Vector256<float> DuplicateEvenIndexed(Vector256<float> value);


docs: __m256 _mm256_movehdup_ps (__m256 a) VMOVSHDUP ymm, ymm/m256
clr-sig: Vector256<float> DuplicateOddIndexed(Vector256<float> value);


docs: __m128i _mm256_extractf128_si256 (__m256i a, const int imm8) VEXTRACTF128 xmm/m128,
docs: ymm, imm8
clr-sig: Vector128<ulong> ExtractVector128(Vector256<ulong> value, byte index);


docs: __m128i _mm256_extractf128_si256 (__m256i a, const int imm8) VEXTRACTF128 xmm/m128,
docs: ymm, imm8
clr-sig: Vector128<uint> ExtractVector128(Vector256<uint> value, byte index);


docs: __m128 _mm256_extractf128_ps (__m256 a, const int imm8) VEXTRACTF128 xmm/m128,
docs: ymm, imm8
clr-sig: Vector128<float> ExtractVector128(Vector256<float> value, byte index);


docs: __m128i _mm256_extractf128_si256 (__m256i a, const int imm8) VEXTRACTF128 xmm/m128,
docs: ymm, imm8
clr-sig: Vector128<sbyte> ExtractVector128(Vector256<sbyte> value, byte index);


docs: __m128i _mm256_extractf128_si256 (__m256i a, const int imm8) VEXTRACTF128 xmm/m128,
docs: ymm, imm8
clr-sig: Vector128<ushort> ExtractVector128(Vector256<ushort> value, byte index);


docs: __m128i _mm256_extractf128_si256 (__m256i a, const int imm8) VEXTRACTF128 xmm/m128,
docs: ymm, imm8
clr-sig: Vector128<int> ExtractVector128(Vector256<int> value, byte index);


docs: __m128i _mm256_extractf128_si256 (__m256i a, const int imm8) VEXTRACTF128 xmm/m128,
docs: ymm, imm8
clr-sig: Vector128<short> ExtractVector128(Vector256<short> value, byte index);


docs: __m128d _mm256_extractf128_pd (__m256d a, const int imm8) VEXTRACTF128 xmm/m128,
docs: ymm, imm8
clr-sig: Vector128<double> ExtractVector128(Vector256<double> value, byte index);


docs: __m128i _mm256_extractf128_si256 (__m256i a, const int imm8) VEXTRACTF128 xmm/m128,
docs: ymm, imm8
clr-sig: Vector128<byte> ExtractVector128(Vector256<byte> value, byte index);


docs: __m128i _mm256_extractf128_si256 (__m256i a, const int imm8) VEXTRACTF128 xmm/m128,
docs: ymm, imm8
clr-sig: Vector128<long> ExtractVector128(Vector256<long> value, byte index);


docs: __m256d _mm256_floor_pd (__m256d a) VROUNDPS ymm, ymm/m256, imm8(9)
clr-sig: Vector256<double> Floor(Vector256<double> value);


docs: __m256 _mm256_floor_ps (__m256 a) VROUNDPS ymm, ymm/m256, imm8(9)
clr-sig: Vector256<float> Floor(Vector256<float> value);


docs: __m256d _mm256_hadd_pd (__m256d a, __m256d b) VHADDPD ymm, ymm, ymm/m256
clr-sig: Vector256<double> HorizontalAdd(Vector256<double> left, Vector256<double> right);


docs: __m256 _mm256_hadd_ps (__m256 a, __m256 b) VHADDPS ymm, ymm, ymm/m256
clr-sig: Vector256<float> HorizontalAdd(Vector256<float> left, Vector256<float> right);


docs: __m256d _mm256_hsub_pd (__m256d a, __m256d b) VHSUBPD ymm, ymm, ymm/m256
clr-sig: Vector256<double> HorizontalSubtract(Vector256<double> left, Vector256<double> right);


docs: __m256 _mm256_hsub_ps (__m256 a, __m256 b) VHSUBPS ymm, ymm, ymm/m256
clr-sig: Vector256<float> HorizontalSubtract(Vector256<float> left, Vector256<float> right);



docs: __m256i _mm256_load_si256 (__m256i const * mem_addr) VMOVDQA ymm, m256
clr-sig: Vector256<ulong> LoadAlignedVector256(ulong* address);


docs: __m256i _mm256_load_si256 (__m256i const * mem_addr) VMOVDQA ymm, m256
clr-sig: Vector256<ushort> LoadAlignedVector256(ushort* address);


docs: __m256 _mm256_load_ps (float const * mem_addr) VMOVAPS ymm, ymm/m256
clr-sig: Vector256<float> LoadAlignedVector256(float* address);


docs: __m256i _mm256_load_si256 (__m256i const * mem_addr) VMOVDQA ymm, m256
clr-sig: Vector256<sbyte> LoadAlignedVector256(sbyte* address);


docs: __m256i _mm256_load_si256 (__m256i const * mem_addr) VMOVDQA ymm, m256
clr-sig: Vector256<uint> LoadAlignedVector256(uint* address);


docs: __m256i _mm256_load_si256 (__m256i const * mem_addr) VMOVDQA ymm, m256
clr-sig: Vector256<int> LoadAlignedVector256(int* address);


docs: __m256i _mm256_load_si256 (__m256i const * mem_addr) VMOVDQA ymm, m256
clr-sig: Vector256<short> LoadAlignedVector256(short* address);


docs: __m256d _mm256_load_pd (double const * mem_addr) VMOVAPD ymm, ymm/m256
clr-sig: Vector256<double> LoadAlignedVector256(double* address);


docs: __m256i _mm256_load_si256 (__m256i const * mem_addr) VMOVDQA ymm, m256
clr-sig: Vector256<byte> LoadAlignedVector256(byte* address);


docs: __m256i _mm256_load_si256 (__m256i const * mem_addr) VMOVDQA ymm, m256
clr-sig: Vector256<long> LoadAlignedVector256(long* address);


docs: __m256i _mm256_lddqu_si256 (__m256i const * mem_addr) VLDDQU ymm, m256
clr-sig: Vector256<ulong> LoadDquVector256(ulong* address);


docs: __m256i _mm256_lddqu_si256 (__m256i const * mem_addr) VLDDQU ymm, m256
clr-sig: Vector256<byte> LoadDquVector256(byte* address);


docs: __m256i _mm256_lddqu_si256 (__m256i const * mem_addr) VLDDQU ymm, m256
clr-sig: Vector256<int> LoadDquVector256(int* address);


docs: __m256i _mm256_lddqu_si256 (__m256i const * mem_addr) VLDDQU ymm, m256
clr-sig: Vector256<long> LoadDquVector256(long* address);


docs: __m256i _mm256_lddqu_si256 (__m256i const * mem_addr) VLDDQU ymm, m256
clr-sig: Vector256<sbyte> LoadDquVector256(sbyte* address);


docs: __m256i _mm256_lddqu_si256 (__m256i const * mem_addr) VLDDQU ymm, m256
clr-sig: Vector256<ushort> LoadDquVector256(ushort* address);


docs: __m256i _mm256_lddqu_si256 (__m256i const * mem_addr) VLDDQU ymm, m256
clr-sig: Vector256<uint> LoadDquVector256(uint* address);


docs: __m256i _mm256_lddqu_si256 (__m256i const * mem_addr) VLDDQU ymm, m256
clr-sig: Vector256<short> LoadDquVector256(short* address);


docs: __m256i _mm256_loadu_si256 (__m256i const * mem_addr) VMOVDQU ymm, m256
clr-sig: Vector256<int> LoadVector256(int* address);


docs: __m256i _mm256_loadu_si256 (__m256i const * mem_addr) VMOVDQU ymm, m256
clr-sig: Vector256<ulong> LoadVector256(ulong* address);


docs: __m256i _mm256_loadu_si256 (__m256i const * mem_addr) VMOVDQU ymm, m256
clr-sig: Vector256<ushort> LoadVector256(ushort* address);


docs: __m256i _mm256_loadu_si256 (__m256i const * mem_addr) VMOVDQU ymm, m256
clr-sig: Vector256<uint> LoadVector256(uint* address);


docs: __m256 _mm256_loadu_ps (float const * mem_addr) VMOVUPS ymm, ymm/m256
clr-sig: Vector256<float> LoadVector256(float* address);


docs: __m256i _mm256_loadu_si256 (__m256i const * mem_addr) VMOVDQU ymm, m256
clr-sig: Vector256<short> LoadVector256(short* address);


docs: __m256i _mm256_loadu_si256 (__m256i const * mem_addr) VMOVDQU ymm, m256
clr-sig: Vector256<long> LoadVector256(long* address);


docs: __m256d _mm256_loadu_pd (double const * mem_addr) VMOVUPD ymm, ymm/m256
clr-sig: Vector256<double> LoadVector256(double* address);


docs: __m256i _mm256_loadu_si256 (__m256i const * mem_addr) VMOVDQU ymm, m256
clr-sig: Vector256<byte> LoadVector256(byte* address);


docs: __m256i _mm256_loadu_si256 (__m256i const * mem_addr) VMOVDQU ymm, m256
clr-sig: Vector256<sbyte> LoadVector256(sbyte* address);


docs: __m128d _mm_maskload_pd (double const * mem_addr, __m128i mask) VMASKMOVPD xmm,
docs: xmm, m128
clr-sig: Vector128<double> MaskLoad(double* address, Vector128<double> mask);


docs: __m256d _mm256_maskload_pd (double const * mem_addr, __m256i mask) VMASKMOVPD
docs: ymm, ymm, m256
clr-sig: Vector256<double> MaskLoad(double* address, Vector256<double> mask);


docs: __m128 _mm_maskload_ps (float const * mem_addr, __m128i mask) VMASKMOVPS xmm,
docs: xmm, m128
clr-sig: Vector128<float> MaskLoad(float* address, Vector128<float> mask);


docs: __m256 _mm256_maskload_ps (float const * mem_addr, __m256i mask) VMASKMOVPS ymm,
docs: ymm, m256
clr-sig: Vector256<float> MaskLoad(float* address, Vector256<float> mask);


docs: void _mm256_maskstore_ps (float * mem_addr, __m256i mask, __m256 a) VMASKMOVPS
docs: m256, ymm, ymm
clr-sig: void MaskStore(float* address, Vector256<float> mask, Vector256<float> source);


docs: void _mm_maskstore_ps (float * mem_addr, __m128i mask, __m128 a) VMASKMOVPS m128,
docs: xmm, xmm
clr-sig: void MaskStore(float* address, Vector128<float> mask, Vector128<float> source);


docs: void _mm_maskstore_pd (double * mem_addr, __m128i mask, __m128d a) VMASKMOVPD
docs: m128, xmm, xmm
clr-sig: void MaskStore(double* address, Vector128<double> mask, Vector128<double> source);


docs: void _mm256_maskstore_pd (double * mem_addr, __m256i mask, __m256d a) VMASKMOVPD
docs: m256, ymm, ymm
clr-sig: void MaskStore(double* address, Vector256<double> mask, Vector256<double> source);


docs: __m256d _mm256_max_pd (__m256d a, __m256d b) VMAXPD ymm, ymm, ymm/m256
clr-sig: Vector256<double> Max(Vector256<double> left, Vector256<double> right);


docs: __m256 _mm256_max_ps (__m256 a, __m256 b) VMAXPS ymm, ymm, ymm/m256
clr-sig: Vector256<float> Max(Vector256<float> left, Vector256<float> right);


docs: __m256d _mm256_min_pd (__m256d a, __m256d b) VMINPD ymm, ymm, ymm/m256
clr-sig: Vector256<double> Min(Vector256<double> left, Vector256<double> right);


docs: __m256 _mm256_min_ps (__m256 a, __m256 b) VMINPS ymm, ymm, ymm/m256
clr-sig: Vector256<float> Min(Vector256<float> left, Vector256<float> right);


docs: int _mm256_movemask_pd (__m256d a) VMOVMSKPD reg, ymm
clr-sig: int MoveMask(Vector256<double> value);


docs: int _mm256_movemask_ps (__m256 a) VMOVMSKPS reg, ymm
clr-sig: int MoveMask(Vector256<float> value);


docs: __m256d _mm256_mul_pd (__m256d a, __m256d b) VMULPD ymm, ymm, ymm/m256
clr-sig: Vector256<double> Multiply(Vector256<double> left, Vector256<double> right);


docs: __m256 _mm256_mul_ps (__m256 a, __m256 b) VMULPS ymm, ymm, ymm/m256
clr-sig: Vector256<float> Multiply(Vector256<float> left, Vector256<float> right);


docs: __m256 _mm256_or_ps (__m256 a, __m256 b) VORPS ymm, ymm, ymm/m256
clr-sig: Vector256<float> Or(Vector256<float> left, Vector256<float> right);


docs: __m256d _mm256_or_pd (__m256d a, __m256d b) VORPD ymm, ymm, ymm/m256
clr-sig: Vector256<double> Or(Vector256<double> left, Vector256<double> right);


docs: __m256 _mm256_permute_ps (__m256 a, int imm8) VPERMILPS ymm, ymm, imm8
clr-sig: Vector256<float> Permute(Vector256<float> value, byte control);


docs: __m128d _mm_permute_pd (__m128d a, int imm8) VPERMILPD xmm, xmm, imm8
clr-sig: Vector128<double> Permute(Vector128<double> value, byte control);


docs: __m128 _mm_permute_ps (__m128 a, int imm8) VPERMILPS xmm, xmm, imm8
clr-sig: Vector128<float> Permute(Vector128<float> value, byte control);


docs: __m256d _mm256_permute_pd (__m256d a, int imm8) VPERMILPD ymm, ymm, imm8
clr-sig: Vector256<double> Permute(Vector256<double> value, byte control);


docs: __m256d _mm256_permute2f128_pd (__m256d a, __m256d b, int imm8) VPERM2F128 ymm,
docs: ymm, ymm/m256, imm8
clr-sig: Vector256<double> Permute2x128(Vector256<double> left, Vector256<double> right, byte control);


docs: __m256i _mm256_permute2f128_si256 (__m256i a, __m256i b, int imm8) VPERM2F128
docs: ymm, ymm, ymm/m256, imm8
clr-sig: Vector256<short> Permute2x128(Vector256<short> left, Vector256<short> right, byte control);


docs: __m256i _mm256_permute2f128_si256 (__m256i a, __m256i b, int imm8) VPERM2F128
docs: ymm, ymm, ymm/m256, imm8
clr-sig: Vector256<int> Permute2x128(Vector256<int> left, Vector256<int> right, byte control);


docs: __m256i _mm256_permute2f128_si256 (__m256i a, __m256i b, int imm8) VPERM2F128
docs: ymm, ymm, ymm/m256, imm8
clr-sig: Vector256<long> Permute2x128(Vector256<long> left, Vector256<long> right, byte control);


docs: __m256i _mm256_permute2f128_si256 (__m256i a, __m256i b, int imm8) VPERM2F128
docs: ymm, ymm, ymm/m256, imm8
clr-sig: Vector256<sbyte> Permute2x128(Vector256<sbyte> left, Vector256<sbyte> right, byte control);


docs: __m256 _mm256_permute2f128_ps (__m256 a, __m256 b, int imm8) VPERM2F128 ymm,
docs: ymm, ymm/m256, imm8
clr-sig: Vector256<float> Permute2x128(Vector256<float> left, Vector256<float> right, byte control);


docs: __m256i _mm256_permute2f128_si256 (__m256i a, __m256i b, int imm8) VPERM2F128
docs: ymm, ymm, ymm/m256, imm8
clr-sig: Vector256<ushort> Permute2x128(Vector256<ushort> left, Vector256<ushort> right, byte control);


docs: __m256i _mm256_permute2f128_si256 (__m256i a, __m256i b, int imm8) VPERM2F128
docs: ymm, ymm, ymm/m256, imm8
clr-sig: Vector256<uint> Permute2x128(Vector256<uint> left, Vector256<uint> right, byte control);


docs: __m256i _mm256_permute2f128_si256 (__m256i a, __m256i b, int imm8) VPERM2F128
docs: ymm, ymm, ymm/m256, imm8
clr-sig: Vector256<ulong> Permute2x128(Vector256<ulong> left, Vector256<ulong> right, byte control);


docs: __m256i _mm256_permute2f128_si256 (__m256i a, __m256i b, int imm8) VPERM2F128
docs: ymm, ymm, ymm/m256, imm8
clr-sig: Vector256<byte> Permute2x128(Vector256<byte> left, Vector256<byte> right, byte control);


docs: __m256 _mm256_permutevar_ps (__m256 a, __m256i b) VPERMILPS ymm, ymm, ymm/m256
clr-sig: Vector256<float> PermuteVar(Vector256<float> left, Vector256<int> control);


docs: __m128d _mm_permutevar_pd (__m128d a, __m128i b) VPERMILPD xmm, xmm, xmm/m128
clr-sig: Vector128<double> PermuteVar(Vector128<double> left, Vector128<long> control);


docs: __m128 _mm_permutevar_ps (__m128 a, __m128i b) VPERMILPS xmm, xmm, xmm/m128
clr-sig: Vector128<float> PermuteVar(Vector128<float> left, Vector128<int> control);


docs: __m256d _mm256_permutevar_pd (__m256d a, __m256i b) VPERMILPD ymm, ymm, ymm/m256
clr-sig: Vector256<double> PermuteVar(Vector256<double> left, Vector256<long> control);


docs: __m256 _mm256_rcp_ps (__m256 a) VRCPPS ymm, ymm/m256
clr-sig: Vector256<float> Reciprocal(Vector256<float> value);


docs: __m256 _mm256_rsqrt_ps (__m256 a) VRSQRTPS ymm, ymm/m256
clr-sig: Vector256<float> ReciprocalSqrt(Vector256<float> value);


docs: __m256d _mm256_round_pd (__m256d a, _MM_FROUND_CUR_DIRECTION) VROUNDPD ymm, ymm/m256,
docs: imm8(4)
clr-sig: Vector256<double> RoundCurrentDirection(Vector256<double> value);


docs: __m256 _mm256_round_ps (__m256 a, _MM_FROUND_CUR_DIRECTION) VROUNDPS ymm, ymm/m256,
docs: imm8(4)
clr-sig: Vector256<float> RoundCurrentDirection(Vector256<float> value);


docs: __m256d _mm256_round_pd (__m256d a, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC)
docs: VROUNDPD ymm, ymm/m256, imm8(8)
clr-sig: Vector256<double> RoundToNearestInteger(Vector256<double> value);


docs: __m256 _mm256_round_ps (__m256 a, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC)
docs: VROUNDPS ymm, ymm/m256, imm8(8)
clr-sig: Vector256<float> RoundToNearestInteger(Vector256<float> value);


docs: __m256d _mm256_round_pd (__m256d a, _MM_FROUND_TO_NEG_INF | _MM_FROUND_NO_EXC)
docs: VROUNDPD ymm, ymm/m256, imm8(9)
clr-sig: Vector256<double> RoundToNegativeInfinity(Vector256<double> value);


docs: __m256 _mm256_round_ps (__m256 a, _MM_FROUND_TO_NEG_INF | _MM_FROUND_NO_EXC)
docs: VROUNDPS ymm, ymm/m256, imm8(9)
clr-sig: Vector256<float> RoundToNegativeInfinity(Vector256<float> value);


docs: __m256d _mm256_round_pd (__m256d a, _MM_FROUND_TO_POS_INF | _MM_FROUND_NO_EXC)
docs: VROUNDPD ymm, ymm/m256, imm8(10)
clr-sig: Vector256<double> RoundToPositiveInfinity(Vector256<double> value);


docs: __m256 _mm256_round_ps (__m256 a, _MM_FROUND_TO_POS_INF | _MM_FROUND_NO_EXC)
docs: VROUNDPS ymm, ymm/m256, imm8(10)
clr-sig: Vector256<float> RoundToPositiveInfinity(Vector256<float> value);


docs: __m256 _mm256_round_ps (__m256 a, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC) VROUNDPS
docs: ymm, ymm/m256, imm8(11)
clr-sig: Vector256<float> RoundToZero(Vector256<float> value);


docs: __m256d _mm256_round_pd (__m256d a, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC) VROUNDPD
docs: ymm, ymm/m256, imm8(11)
clr-sig: Vector256<double> RoundToZero(Vector256<double> value);




docs: __m256d _mm256_sqrt_pd (__m256d a) VSQRTPD ymm, ymm/m256
clr-sig: Vector256<double> Sqrt(Vector256<double> value);


docs: __m256 _mm256_sqrt_ps (__m256 a) VSQRTPS ymm, ymm/m256
clr-sig: Vector256<float> Sqrt(Vector256<float> value);


docs: void _mm256_storeu_si256 (__m256i * mem_addr, __m256i a) MOVDQU m256, ymm
clr-sig: void Store(ulong* address, Vector256<ulong> source);


docs: void _mm256_storeu_si256 (__m256i * mem_addr, __m256i a) MOVDQU m256, ymm
clr-sig: void Store(uint* address, Vector256<uint> source);


docs: void _mm256_storeu_si256 (__m256i * mem_addr, __m256i a) MOVDQU m256, ymm
clr-sig: void Store(ushort* address, Vector256<ushort> source);


docs: void _mm256_storeu_si256 (__m256i * mem_addr, __m256i a) MOVDQU m256, ymm
clr-sig: void Store(sbyte* address, Vector256<sbyte> source);


docs: void _mm256_storeu_ps (float * mem_addr, __m256 a) MOVUPS m256, ymm
clr-sig: void Store(float* address, Vector256<float> source);


docs: void _mm256_storeu_si256 (__m256i * mem_addr, __m256i a) MOVDQU m256, ymm
clr-sig: void Store(int* address, Vector256<int> source);


docs: void _mm256_storeu_si256 (__m256i * mem_addr, __m256i a) MOVDQU m256, ymm
clr-sig: void Store(short* address, Vector256<short> source);


docs: void _mm256_storeu_pd (double * mem_addr, __m256d a) MOVUPD m256, ymm
clr-sig: void Store(double* address, Vector256<double> source);


docs: void _mm256_storeu_si256 (__m256i * mem_addr, __m256i a) MOVDQU m256, ymm
clr-sig: void Store(byte* address, Vector256<byte> source);


docs: void _mm256_storeu_si256 (__m256i * mem_addr, __m256i a) MOVDQU m256, ymm
clr-sig: void Store(long* address, Vector256<long> source);


docs: void _mm256_store_si256 (__m256i * mem_addr, __m256i a) MOVDQA m256, ymm
clr-sig: void StoreAligned(ushort* address, Vector256<ushort> source);


docs: void _mm256_store_si256 (__m256i * mem_addr, __m256i a) MOVDQA m256, ymm
clr-sig: void StoreAligned(ulong* address, Vector256<ulong> source);


docs: void _mm256_store_si256 (__m256i * mem_addr, __m256i a) MOVDQA m256, ymm
clr-sig: void StoreAligned(uint* address, Vector256<uint> source);


docs: void _mm256_store_ps (float * mem_addr, __m256 a) VMOVAPS m256, ymm
clr-sig: void StoreAligned(float* address, Vector256<float> source);


docs: void _mm256_store_pd (double * mem_addr, __m256d a) VMOVAPD m256, ymm
clr-sig: void StoreAligned(double* address, Vector256<double> source);


docs: void _mm256_store_si256 (__m256i * mem_addr, __m256i a) MOVDQA m256, ymm
clr-sig: void StoreAligned(long* address, Vector256<long> source);


docs: void _mm256_store_si256 (__m256i * mem_addr, __m256i a) MOVDQA m256, ymm
clr-sig: void StoreAligned(int* address, Vector256<int> source);


docs: void _mm256_store_si256 (__m256i * mem_addr, __m256i a) MOVDQA m256, ymm
clr-sig: void StoreAligned(short* address, Vector256<short> source);


docs: void _mm256_store_si256 (__m256i * mem_addr, __m256i a) MOVDQA m256, ymm
clr-sig: void StoreAligned(byte* address, Vector256<byte> source);


docs: void _mm256_store_si256 (__m256i * mem_addr, __m256i a) MOVDQA m256, ymm
clr-sig: void StoreAligned(sbyte* address, Vector256<sbyte> source);


docs: void _mm256_stream_si256 (__m256i * mem_addr, __m256i a) VMOVNTDQ m256, ymm
clr-sig: void StoreAlignedNonTemporal(ulong* address, Vector256<ulong> source);


docs: void _mm256_stream_si256 (__m256i * mem_addr, __m256i a) VMOVNTDQ m256, ymm
clr-sig: void StoreAlignedNonTemporal(ushort* address, Vector256<ushort> source);


docs: void _mm256_stream_si256 (__m256i * mem_addr, __m256i a) VMOVNTDQ m256, ymm
clr-sig: void StoreAlignedNonTemporal(uint* address, Vector256<uint> source);


docs: void _mm256_stream_ps (float * mem_addr, __m256 a) MOVNTPS m256, ymm
clr-sig: void StoreAlignedNonTemporal(float* address, Vector256<float> source);


docs: void _mm256_stream_pd (double * mem_addr, __m256d a) MOVNTPD m256, ymm
clr-sig: void StoreAlignedNonTemporal(double* address, Vector256<double> source);


docs: void _mm256_stream_si256 (__m256i * mem_addr, __m256i a) VMOVNTDQ m256, ymm
clr-sig: void StoreAlignedNonTemporal(long* address, Vector256<long> source);


docs: void _mm256_stream_si256 (__m256i * mem_addr, __m256i a) VMOVNTDQ m256, ymm
clr-sig: void StoreAlignedNonTemporal(sbyte* address, Vector256<sbyte> source);


docs: void _mm256_stream_si256 (__m256i * mem_addr, __m256i a) VMOVNTDQ m256, ymm
clr-sig: void StoreAlignedNonTemporal(byte* address, Vector256<byte> source);


docs: void _mm256_stream_si256 (__m256i * mem_addr, __m256i a) VMOVNTDQ m256, ymm
clr-sig: void StoreAlignedNonTemporal(int* address, Vector256<int> source);


docs: void _mm256_stream_si256 (__m256i * mem_addr, __m256i a) VMOVNTDQ m256, ymm
clr-sig: void StoreAlignedNonTemporal(short* address, Vector256<short> source);


docs: __m256d _mm256_sub_pd (__m256d a, __m256d b) VSUBPD ymm, ymm, ymm/m256
clr-sig: Vector256<double> Subtract(Vector256<double> left, Vector256<double> right);


docs: __m256 _mm256_sub_ps (__m256 a, __m256 b) VSUBPS ymm, ymm, ymm/m256
clr-sig: Vector256<float> Subtract(Vector256<float> left, Vector256<float> right);


docs: int _mm256_testc_si256 (__m256i a, __m256i b) VPTEST ymm, ymm/m256
clr-sig: bool TestC(Vector256<ulong> left, Vector256<ulong> right);


docs: int _mm256_testc_si256 (__m256i a, __m256i b) VPTEST ymm, ymm/m256
clr-sig: bool TestC(Vector256<uint> left, Vector256<uint> right);


docs: int _mm256_testc_si256 (__m256i a, __m256i b) VPTEST ymm, ymm/m256
clr-sig: bool TestC(Vector256<ushort> left, Vector256<ushort> right);


docs: int _mm256_testc_ps (__m256 a, __m256 b) VTESTPS ymm, ymm/m256
clr-sig: bool TestC(Vector256<float> left, Vector256<float> right);


docs: int _mm256_testc_si256 (__m256i a, __m256i b) VPTEST ymm, ymm/m256
clr-sig: bool TestC(Vector256<long> left, Vector256<long> right);


docs: int _mm256_testc_si256 (__m256i a, __m256i b) VPTEST ymm, ymm/m256
clr-sig: bool TestC(Vector256<sbyte> left, Vector256<sbyte> right);


docs: int _mm256_testc_si256 (__m256i a, __m256i b) VPTEST ymm, ymm/m256
clr-sig: bool TestC(Vector256<short> left, Vector256<short> right);


docs: int _mm256_testc_pd (__m256d a, __m256d b) VTESTPS ymm, ymm/m256
clr-sig: bool TestC(Vector256<double> left, Vector256<double> right);


docs: int _mm256_testc_si256 (__m256i a, __m256i b) VPTEST ymm, ymm/m256
clr-sig: bool TestC(Vector256<byte> left, Vector256<byte> right);


docs: int _mm_testc_ps (__m128 a, __m128 b) VTESTPS xmm, xmm/m128
clr-sig: bool TestC(Vector128<float> left, Vector128<float> right);


docs: int _mm_testc_pd (__m128d a, __m128d b) VTESTPD xmm, xmm/m128
clr-sig: bool TestC(Vector128<double> left, Vector128<double> right);


docs: int _mm256_testc_si256 (__m256i a, __m256i b) VPTEST ymm, ymm/m256
clr-sig: bool TestC(Vector256<int> left, Vector256<int> right);


docs: int _mm256_testnzc_si256 (__m256i a, __m256i b) VPTEST ymm, ymm/m256
clr-sig: bool TestNotZAndNotC(Vector256<ulong> left, Vector256<ulong> right);


docs: int _mm256_testnzc_si256 (__m256i a, __m256i b) VPTEST ymm, ymm/m256
clr-sig: bool TestNotZAndNotC(Vector256<uint> left, Vector256<uint> right);


docs: int _mm256_testnzc_si256 (__m256i a, __m256i b) VPTEST ymm, ymm/m256
clr-sig: bool TestNotZAndNotC(Vector256<ushort> left, Vector256<ushort> right);


docs: int _mm256_testnzc_ps (__m256 a, __m256 b) VTESTPS ymm, ymm/m256
clr-sig: bool TestNotZAndNotC(Vector256<float> left, Vector256<float> right);


docs: int _mm256_testnzc_si256 (__m256i a, __m256i b) VPTEST ymm, ymm/m256
clr-sig: bool TestNotZAndNotC(Vector256<sbyte> left, Vector256<sbyte> right);


docs: int _mm256_testnzc_si256 (__m256i a, __m256i b) VPTEST ymm, ymm/m256
clr-sig: bool TestNotZAndNotC(Vector256<long> left, Vector256<long> right);


docs: int _mm256_testnzc_si256 (__m256i a, __m256i b) VPTEST ymm, ymm/m256
clr-sig: bool TestNotZAndNotC(Vector256<short> left, Vector256<short> right);


docs: int _mm256_testnzc_pd (__m256d a, __m256d b) VTESTPD ymm, ymm/m256
clr-sig: bool TestNotZAndNotC(Vector256<double> left, Vector256<double> right);


docs: int _mm256_testnzc_si256 (__m256i a, __m256i b) VPTEST ymm, ymm/m256
clr-sig: bool TestNotZAndNotC(Vector256<byte> left, Vector256<byte> right);


docs: int _mm_testnzc_ps (__m128 a, __m128 b) VTESTPS xmm, xmm/m128
clr-sig: bool TestNotZAndNotC(Vector128<float> left, Vector128<float> right);


docs: int _mm_testnzc_pd (__m128d a, __m128d b) VTESTPD xmm, xmm/m128
clr-sig: bool TestNotZAndNotC(Vector128<double> left, Vector128<double> right);


docs: int _mm256_testnzc_si256 (__m256i a, __m256i b) VPTEST ymm, ymm/m256
clr-sig: bool TestNotZAndNotC(Vector256<int> left, Vector256<int> right);


docs: int _mm256_testz_si256 (__m256i a, __m256i b) VPTEST ymm, ymm/m256
clr-sig: bool TestZ(Vector256<ulong> left, Vector256<ulong> right);


docs: int _mm256_testz_si256 (__m256i a, __m256i b) VPTEST ymm, ymm/m256
clr-sig: bool TestZ(Vector256<uint> left, Vector256<uint> right);


docs: int _mm256_testz_si256 (__m256i a, __m256i b) VPTEST ymm, ymm/m256
clr-sig: bool TestZ(Vector256<ushort> left, Vector256<ushort> right);


docs: int _mm256_testz_ps (__m256 a, __m256 b) VTESTPS ymm, ymm/m256
clr-sig: bool TestZ(Vector256<float> left, Vector256<float> right);


docs: int _mm256_testz_si256 (__m256i a, __m256i b) VPTEST ymm, ymm/m256
clr-sig: bool TestZ(Vector256<sbyte> left, Vector256<sbyte> right);


docs: int _mm256_testz_si256 (__m256i a, __m256i b) VPTEST ymm, ymm/m256
clr-sig: bool TestZ(Vector256<long> left, Vector256<long> right);


docs: int _mm256_testz_si256 (__m256i a, __m256i b) VPTEST ymm, ymm/m256
clr-sig: bool TestZ(Vector256<short> left, Vector256<short> right);


docs: int _mm256_testz_pd (__m256d a, __m256d b) VTESTPD ymm, ymm/m256
clr-sig: bool TestZ(Vector256<double> left, Vector256<double> right);


docs: int _mm256_testz_si256 (__m256i a, __m256i b) VPTEST ymm, ymm/m256
clr-sig: bool TestZ(Vector256<byte> left, Vector256<byte> right);


docs: int _mm_testz_ps (__m128 a, __m128 b) VTESTPS xmm, xmm/m128
clr-sig: bool TestZ(Vector128<float> left, Vector128<float> right);


docs: int _mm_testz_pd (__m128d a, __m128d b) VTESTPD xmm, xmm/m128
clr-sig: bool TestZ(Vector128<double> left, Vector128<double> right);


docs: int _mm256_testz_si256 (__m256i a, __m256i b) VPTEST ymm, ymm/m256
clr-sig: bool TestZ(Vector256<int> left, Vector256<int> right);






docs: __m256d _mm256_xor_pd (__m256d a, __m256d b) VXORPS ymm, ymm, ymm/m256
clr-sig: Vector256<double> Xor(Vector256<double> left, Vector256<double> right);


docs: __m256 _mm256_xor_ps (__m256 a, __m256 b) VXORPS ymm, ymm, ymm/m256
clr-sig: Vector256<float> Xor(Vector256<float> left, Vector256<float> right);

Fma -> Avx -> Sse42 -> Sse41 -> Ssse3 -> Sse3 -> Sse2 -> Sse
-------------------------------------------------------------------------------

docs: __m256 _mm256_fmadd_ps (__m256 a, __m256 b, __m256 c) VFMADDPS ymm, ymm, ymm/m256
clr-sig: Vector256<float> MultiplyAdd(Vector256<float> a, Vector256<float> b, Vector256<float> c);


docs: __m128d _mm_fmadd_pd (__m128d a, __m128d b, __m128d c) VFMADDPD xmm, xmm, xmm/m128
clr-sig: Vector128<double> MultiplyAdd(Vector128<double> a, Vector128<double> b, Vector128<double> c);


docs: __m128 _mm_fmadd_ps (__m128 a, __m128 b, __m128 c) VFMADDPS xmm, xmm, xmm/m128
clr-sig: Vector128<float> MultiplyAdd(Vector128<float> a, Vector128<float> b, Vector128<float> c);


docs: __m256d _mm256_fmadd_pd (__m256d a, __m256d b, __m256d c) VFMADDPS ymm, ymm,
docs: ymm/m256
clr-sig: Vector256<double> MultiplyAdd(Vector256<double> a, Vector256<double> b, Vector256<double> c);


docs: __m128d _mm_fnmadd_pd (__m128d a, __m128d b, __m128d c) VFNMADDPD xmm, xmm, xmm/m128
clr-sig: Vector128<double> MultiplyAddNegated(Vector128<double> a, Vector128<double> b, Vector128<double> c);


docs: __m128 _mm_fnmadd_ps (__m128 a, __m128 b, __m128 c) VFNMADDPS xmm, xmm, xmm/m128
clr-sig: Vector128<float> MultiplyAddNegated(Vector128<float> a, Vector128<float> b, Vector128<float> c);


docs: __m256d _mm256_fnmadd_pd (__m256d a, __m256d b, __m256d c) VFNMADDPD ymm, ymm,
docs: ymm/m256
clr-sig: Vector256<double> MultiplyAddNegated(Vector256<double> a, Vector256<double> b, Vector256<double> c);


docs: __m256 _mm256_fnmadd_ps (__m256 a, __m256 b, __m256 c) VFNMADDPS ymm, ymm, ymm/m256
clr-sig: Vector256<float> MultiplyAddNegated(Vector256<float> a, Vector256<float> b, Vector256<float> c);


docs: __m128d _mm_fnmadd_sd (__m128d a, __m128d b, __m128d c) VFNMADDSD xmm, xmm, xmm/m64
clr-sig: Vector128<double> MultiplyAddNegatedScalar(Vector128<double> a, Vector128<double> b, Vector128<double> c);


docs: __m128 _mm_fnmadd_ss (__m128 a, __m128 b, __m128 c) VFNMADDSS xmm, xmm, xmm/m32
clr-sig: Vector128<float> MultiplyAddNegatedScalar(Vector128<float> a, Vector128<float> b, Vector128<float> c);


docs: __m128d _mm_fmadd_sd (__m128d a, __m128d b, __m128d c) VFMADDSS xmm, xmm, xmm/m64
clr-sig: Vector128<double> MultiplyAddScalar(Vector128<double> a, Vector128<double> b, Vector128<double> c);


docs: __m128 _mm_fmadd_ss (__m128 a, __m128 b, __m128 c) VFMADDSS xmm, xmm, xmm/m32
clr-sig: Vector128<float> MultiplyAddScalar(Vector128<float> a, Vector128<float> b, Vector128<float> c);


docs: __m128d _mm_fmaddsub_pd (__m128d a, __m128d b, __m128d c) VFMADDSUBPD xmm, xmm,
docs: xmm/m128
clr-sig: Vector128<double> MultiplyAddSubtract(Vector128<double> a, Vector128<double> b, Vector128<double> c);


docs: __m128 _mm_fmaddsub_ps (__m128 a, __m128 b, __m128 c) VFMADDSUBPS xmm, xmm, xmm/m128
clr-sig: Vector128<float> MultiplyAddSubtract(Vector128<float> a, Vector128<float> b, Vector128<float> c);


docs: __m256d _mm256_fmaddsub_pd (__m256d a, __m256d b, __m256d c) VFMADDSUBPD ymm,
docs: ymm, ymm/m256
clr-sig: Vector256<double> MultiplyAddSubtract(Vector256<double> a, Vector256<double> b, Vector256<double> c);


docs: __m256 _mm256_fmaddsub_ps (__m256 a, __m256 b, __m256 c) VFMADDSUBPS ymm, ymm,
docs: ymm/m256
clr-sig: Vector256<float> MultiplyAddSubtract(Vector256<float> a, Vector256<float> b, Vector256<float> c);


docs: __m256 _mm256_fmsub_ps (__m256 a, __m256 b, __m256 c) VFMSUBPS ymm, ymm, ymm/m256
clr-sig: Vector256<float> MultiplySubtract(Vector256<float> a, Vector256<float> b, Vector256<float> c);


docs: __m128 _mm_fmsub_ps (__m128 a, __m128 b, __m128 c) VFMSUBPS xmm, xmm, xmm/m128
clr-sig: Vector128<float> MultiplySubtract(Vector128<float> a, Vector128<float> b, Vector128<float> c);


docs: __m256d _mm256_fmsub_pd (__m256d a, __m256d b, __m256d c) VFMSUBPD ymm, ymm,
docs: ymm/m256
clr-sig: Vector256<double> MultiplySubtract(Vector256<double> a, Vector256<double> b, Vector256<double> c);


docs: __m128d _mm_fmsub_pd (__m128d a, __m128d b, __m128d c) VFMSUBPS xmm, xmm, xmm/m128
clr-sig: Vector128<double> MultiplySubtract(Vector128<double> a, Vector128<double> b, Vector128<double> c);


docs: __m128d _mm_fmsubadd_pd (__m128d a, __m128d b, __m128d c) VFMSUBADDPD xmm, xmm,
docs: xmm/m128
clr-sig: Vector128<double> MultiplySubtractAdd(Vector128<double> a, Vector128<double> b, Vector128<double> c);


docs: __m128 _mm_fmsubadd_ps (__m128 a, __m128 b, __m128 c) VFMSUBADDPS xmm, xmm, xmm/m128
clr-sig: Vector128<float> MultiplySubtractAdd(Vector128<float> a, Vector128<float> b, Vector128<float> c);


docs: __m256d _mm256_fmsubadd_pd (__m256d a, __m256d b, __m256d c) VFMSUBADDPD ymm,
docs: ymm, ymm/m256
clr-sig: Vector256<double> MultiplySubtractAdd(Vector256<double> a, Vector256<double> b, Vector256<double> c);


docs: __m256 _mm256_fmsubadd_ps (__m256 a, __m256 b, __m256 c) VFMSUBADDPS ymm, ymm,
docs: ymm/m256
clr-sig: Vector256<float> MultiplySubtractAdd(Vector256<float> a, Vector256<float> b, Vector256<float> c);


docs: __m128d _mm_fnmsub_pd (__m128d a, __m128d b, __m128d c) VFNMSUBPD xmm, xmm, xmm/m128
clr-sig: Vector128<double> MultiplySubtractNegated(Vector128<double> a, Vector128<double> b, Vector128<double> c);


docs: __m128 _mm_fnmsub_ps (__m128 a, __m128 b, __m128 c) VFNMSUBPS xmm, xmm, xmm/m128
clr-sig: Vector128<float> MultiplySubtractNegated(Vector128<float> a, Vector128<float> b, Vector128<float> c);


docs: __m256d _mm256_fnmsub_pd (__m256d a, __m256d b, __m256d c) VFNMSUBPD ymm, ymm,
docs: ymm/m256
clr-sig: Vector256<double> MultiplySubtractNegated(Vector256<double> a, Vector256<double> b, Vector256<double> c);


docs: __m256 _mm256_fnmsub_ps (__m256 a, __m256 b, __m256 c) VFNMSUBPS ymm, ymm, ymm/m256
clr-sig: Vector256<float> MultiplySubtractNegated(Vector256<float> a, Vector256<float> b, Vector256<float> c);


docs: __m128d _mm_fnmsub_sd (__m128d a, __m128d b, __m128d c) VFNMSUBSD xmm, xmm, xmm/m64
clr-sig: Vector128<double> MultiplySubtractNegatedScalar(Vector128<double> a, Vector128<double> b, Vector128<double> c);


docs: __m128 _mm_fnmsub_ss (__m128 a, __m128 b, __m128 c) VFNMSUBSS xmm, xmm, xmm/m32
clr-sig: Vector128<float> MultiplySubtractNegatedScalar(Vector128<float> a, Vector128<float> b, Vector128<float> c);


docs: __m128 _mm_fmsub_ss (__m128 a, __m128 b, __m128 c) VFMSUBSS xmm, xmm, xmm/m32
clr-sig: Vector128<float> MultiplySubtractScalar(Vector128<float> a, Vector128<float> b, Vector128<float> c);


docs: __m128d _mm_fmsub_sd (__m128d a, __m128d b, __m128d c) VFMSUBSD xmm, xmm, xmm/m64
clr-sig: Vector128<double> MultiplySubtractScalar(Vector128<double> a, Vector128<double> b, Vector128<double> c);


Avx2 -> Avx -> Sse42 -> Sse41 -> Ssse3 -> Sse3 -> Sse2 -> Sse
-------------------------------------------------------------------------------


    

    
docs: __m256i _mm256_adds_epi8 (__m256i a, __m256i b) VPADDSB ymm, ymm, ymm/m256
clr-sig: Vector256<sbyte> AddSaturate(Vector256<sbyte> left, Vector256<sbyte> right);

    
docs: __m256i _mm256_adds_epu16 (__m256i a, __m256i b) VPADDUSW ymm, ymm, ymm/m256
clr-sig: Vector256<ushort> AddSaturate(Vector256<ushort> left, Vector256<ushort> right);

    
docs: __m256i _mm256_adds_epu8 (__m256i a, __m256i b) VPADDUSB ymm, ymm, ymm/m256
clr-sig: Vector256<byte> AddSaturate(Vector256<byte> left, Vector256<byte> right);

    
docs: __m256i _mm256_adds_epi16 (__m256i a, __m256i b) VPADDSW ymm, ymm, ymm/m256
clr-sig: Vector256<short> AddSaturate(Vector256<short> left, Vector256<short> right);

    
docs: __m256i _mm256_alignr_epi8 (__m256i a, __m256i b, const int count) VPALIGNR ymm,
docs: ymm, ymm/m256, imm8
clr-sig: Vector256<byte> AlignRight(Vector256<byte> left, Vector256<byte> right, byte mask);

    
docs: __m256i _mm256_alignr_epi8 (__m256i a, __m256i b, const int count) VPALIGNR ymm,
docs: ymm, ymm/m256, imm8 This intrinsic generates VPALIGNR that operates over bytes
docs: rather than elements of the vectors.
clr-sig: Vector256<short> AlignRight(Vector256<short> left, Vector256<short> right, byte mask);

    
docs: __m256i _mm256_alignr_epi8 (__m256i a, __m256i b, const int count) VPALIGNR ymm,
docs: ymm, ymm/m256, imm8 This intrinsic generates VPALIGNR that operates over bytes
docs: rather than elements of the vectors.
clr-sig: Vector256<int> AlignRight(Vector256<int> left, Vector256<int> right, byte mask);

    
docs: __m256i _mm256_alignr_epi8 (__m256i a, __m256i b, const int count) VPALIGNR ymm,
docs: ymm, ymm/m256, imm8 This intrinsic generates VPALIGNR that operates over bytes
docs: rather than elements of the vectors.
clr-sig: Vector256<long> AlignRight(Vector256<long> left, Vector256<long> right, byte mask);

    
docs: __m256i _mm256_alignr_epi8 (__m256i a, __m256i b, const int count) VPALIGNR ymm,
docs: ymm, ymm/m256, imm8
clr-sig: Vector256<sbyte> AlignRight(Vector256<sbyte> left, Vector256<sbyte> right, byte mask);

    
docs: __m256i _mm256_alignr_epi8 (__m256i a, __m256i b, const int count) VPALIGNR ymm,
docs: ymm, ymm/m256, imm8 This intrinsic generates VPALIGNR that operates over bytes
docs: rather than elements of the vectors.
clr-sig: Vector256<ushort> AlignRight(Vector256<ushort> left, Vector256<ushort> right, byte mask);

    
docs: __m256i _mm256_alignr_epi8 (__m256i a, __m256i b, const int count) VPALIGNR ymm,
docs: ymm, ymm/m256, imm8 This intrinsic generates VPALIGNR that operates over bytes
docs: rather than elements of the vectors.
clr-sig: Vector256<uint> AlignRight(Vector256<uint> left, Vector256<uint> right, byte mask);

    
docs: __m256i _mm256_alignr_epi8 (__m256i a, __m256i b, const int count) VPALIGNR ymm,
docs: ymm, ymm/m256, imm8 This intrinsic generates VPALIGNR that operates over bytes
docs: rather than elements of the vectors.
clr-sig: Vector256<ulong> AlignRight(Vector256<ulong> left, Vector256<ulong> right, byte mask);

    
docs: __m256i _mm256_and_si256 (__m256i a, __m256i b) VPAND ymm, ymm, ymm/m256
clr-sig: Vector256<uint> And(Vector256<uint> left, Vector256<uint> right);

    
docs: __m256i _mm256_and_si256 (__m256i a, __m256i b) VPAND ymm, ymm, ymm/m256
clr-sig: Vector256<ulong> And(Vector256<ulong> left, Vector256<ulong> right);

    
docs: __m256i _mm256_and_si256 (__m256i a, __m256i b) VPAND ymm, ymm, ymm/m256
clr-sig: Vector256<ushort> And(Vector256<ushort> left, Vector256<ushort> right);

    
docs: __m256i _mm256_and_si256 (__m256i a, __m256i b) VPAND ymm, ymm, ymm/m256
clr-sig: Vector256<long> And(Vector256<long> left, Vector256<long> right);

    
docs: __m256i _mm256_and_si256 (__m256i a, __m256i b) VPAND ymm, ymm, ymm/m256
clr-sig: Vector256<byte> And(Vector256<byte> left, Vector256<byte> right);

    
docs: __m256i _mm256_and_si256 (__m256i a, __m256i b) VPAND ymm, ymm, ymm/m256
clr-sig: Vector256<int> And(Vector256<int> left, Vector256<int> right);

    
docs: __m256i _mm256_and_si256 (__m256i a, __m256i b) VPAND ymm, ymm, ymm/m256
clr-sig: Vector256<short> And(Vector256<short> left, Vector256<short> right);

    
docs: __m256i _mm256_and_si256 (__m256i a, __m256i b) VPAND ymm, ymm, ymm/m256
clr-sig: Vector256<sbyte> And(Vector256<sbyte> left, Vector256<sbyte> right);

    

    
docs: __m256i _mm256_avg_epu16 (__m256i a, __m256i b) VPAVGW ymm, ymm, ymm/m256
clr-sig: Vector256<ushort> Average(Vector256<ushort> left, Vector256<ushort> right);

    
docs: __m256i _mm256_avg_epu8 (__m256i a, __m256i b) VPAVGB ymm, ymm, ymm/m256
clr-sig: Vector256<byte> Average(Vector256<byte> left, Vector256<byte> right);

    
docs: __m256i _mm256_blend_epi32 (__m256i a, __m256i b, const int imm8) VPBLENDD ymm,
docs: ymm, ymm/m256, imm8
clr-sig: Vector256<uint> Blend(Vector256<uint> left, Vector256<uint> right, byte control);

    
docs: __m128i _mm_blend_epi32 (__m128i a, __m128i b, const int imm8) VPBLENDD xmm,
docs: xmm, xmm/m128, imm8
clr-sig: Vector128<int> Blend(Vector128<int> left, Vector128<int> right, byte control);

    
docs: __m128i _mm_blend_epi32 (__m128i a, __m128i b, const int imm8) VPBLENDD xmm,
docs: xmm, xmm/m128, imm8
clr-sig: Vector128<uint> Blend(Vector128<uint> left, Vector128<uint> right, byte control);

    
docs: __m256i _mm256_blend_epi16 (__m256i a, __m256i b, const int imm8) VPBLENDW ymm,
docs: ymm, ymm/m256, imm8
clr-sig: Vector256<short> Blend(Vector256<short> left, Vector256<short> right, byte control);

    
docs: __m256i _mm256_blend_epi32 (__m256i a, __m256i b, const int imm8) VPBLENDD ymm,
docs: ymm, ymm/m256, imm8
clr-sig: Vector256<int> Blend(Vector256<int> left, Vector256<int> right, byte control);

    
docs: __m256i _mm256_blend_epi16 (__m256i a, __m256i b, const int imm8) VPBLENDW ymm,
docs: ymm, ymm/m256, imm8
clr-sig: Vector256<ushort> Blend(Vector256<ushort> left, Vector256<ushort> right, byte control);

    
docs: __m256i _mm256_blendv_epi8 (__m256i a, __m256i b, __m256i mask) VPBLENDVB ymm,
docs: ymm, ymm/m256, ymm This intrinsic generates VPBLENDVB that needs a BYTE mask-vector,
docs: so users should correctly set each mask byte for the selected elements.
clr-sig: Vector256<uint> BlendVariable(Vector256<uint> left, Vector256<uint> right, Vector256<uint> mask);

    
docs: __m256i _mm256_blendv_epi8 (__m256i a, __m256i b, __m256i mask) VPBLENDVB ymm,
docs: ymm, ymm/m256, ymm This intrinsic generates VPBLENDVB that needs a BYTE mask-vector,
docs: so users should correctly set each mask byte for the selected elements.
clr-sig: Vector256<ulong> BlendVariable(Vector256<ulong> left, Vector256<ulong> right, Vector256<ulong> mask);

    
docs: __m256i _mm256_blendv_epi8 (__m256i a, __m256i b, __m256i mask) VPBLENDVB ymm,
docs: ymm, ymm/m256, ymm This intrinsic generates VPBLENDVB that needs a BYTE mask-vector,
docs: so users should correctly set each mask byte for the selected elements.
clr-sig: Vector256<ushort> BlendVariable(Vector256<ushort> left, Vector256<ushort> right, Vector256<ushort> mask);

    
docs: __m256i _mm256_blendv_epi8 (__m256i a, __m256i b, __m256i mask) VPBLENDVB ymm,
docs: ymm, ymm/m256, ymm This intrinsic generates VPBLENDVB that needs a BYTE mask-vector,
docs: so users should correctly set each mask byte for the selected elements.
clr-sig: Vector256<short> BlendVariable(Vector256<short> left, Vector256<short> right, Vector256<short> mask);

    
docs: __m256i _mm256_blendv_epi8 (__m256i a, __m256i b, __m256i mask) VPBLENDVB ymm,
docs: ymm, ymm/m256, ymm This intrinsic generates VPBLENDVB that needs a BYTE mask-vector,
docs: so users should correctly set each mask byte for the selected elements.
clr-sig: Vector256<long> BlendVariable(Vector256<long> left, Vector256<long> right, Vector256<long> mask);

    
docs: __m256i _mm256_blendv_epi8 (__m256i a, __m256i b, __m256i mask) VPBLENDVB ymm,
docs: ymm, ymm/m256, ymm This intrinsic generates VPBLENDVB that needs a BYTE mask-vector,
docs: so users should correctly set each mask byte for the selected elements.
clr-sig: Vector256<int> BlendVariable(Vector256<int> left, Vector256<int> right, Vector256<int> mask);

    
docs: __m256i _mm256_blendv_epi8 (__m256i a, __m256i b, __m256i mask) VPBLENDVB ymm,
docs: ymm, ymm/m256, ymm
clr-sig: Vector256<byte> BlendVariable(Vector256<byte> left, Vector256<byte> right, Vector256<byte> mask);

    
docs: __m256i _mm256_blendv_epi8 (__m256i a, __m256i b, __m256i mask) VPBLENDVB ymm,
docs: ymm, ymm/m256, ymm
clr-sig: Vector256<sbyte> BlendVariable(Vector256<sbyte> left, Vector256<sbyte> right, Vector256<sbyte> mask);

    
docs: __m128i _mm_broadcastq_epi64 (__m128i a) VPBROADCASTQ xmm, m64 The above native
docs: signature does not directly correspond to the managed signature.
clr-sig: Vector128<ulong> BroadcastScalarToVector128(ulong* source);

    
docs: __m128i _mm_broadcastd_epi32 (__m128i a) VPBROADCASTD xmm, m32 The above native
docs: signature does not directly correspond to the managed signature.
clr-sig: Vector128<uint> BroadcastScalarToVector128(uint* source);

    
docs: __m128i _mm_broadcastw_epi16 (__m128i a) VPBROADCASTW xmm, m16 The above native
docs: signature does not directly correspond to the managed signature.
clr-sig: Vector128<ushort> BroadcastScalarToVector128(ushort* source);

    
docs: __m128i _mm_broadcastb_epi8 (__m128i a) VPBROADCASTB xmm, m8 The above native
docs: signature does not directly correspond to the managed signature.
clr-sig: Vector128<sbyte> BroadcastScalarToVector128(sbyte* source);

    
docs: __m128i _mm_broadcastq_epi64 (__m128i a) VPBROADCASTQ xmm, xmm
clr-sig: Vector128<ulong> BroadcastScalarToVector128(Vector128<ulong> value);

    
docs: __m128i _mm_broadcastd_epi32 (__m128i a) VPBROADCASTD xmm, xmm
clr-sig: Vector128<uint> BroadcastScalarToVector128(Vector128<uint> value);

    
docs: __m128 _mm_broadcastss_ps (__m128 a) VBROADCASTSS xmm, xmm
clr-sig: Vector128<float> BroadcastScalarToVector128(Vector128<float> value);

    
docs: __m128i _mm_broadcastb_epi8 (__m128i a) VPBROADCASTB xmm, xmm
clr-sig: Vector128<sbyte> BroadcastScalarToVector128(Vector128<sbyte> value);

    
docs: __m128i _mm_broadcastw_epi16 (__m128i a) VPBROADCASTW xmm, xmm
clr-sig: Vector128<ushort> BroadcastScalarToVector128(Vector128<ushort> value);

    
docs: __m128i _mm_broadcastd_epi32 (__m128i a) VPBROADCASTD xmm, xmm
clr-sig: Vector128<int> BroadcastScalarToVector128(Vector128<int> value);

    
docs: __m128i _mm_broadcastq_epi64 (__m128i a) VPBROADCASTQ xmm, xmm
clr-sig: Vector128<long> BroadcastScalarToVector128(Vector128<long> value);

    
docs: __m128i _mm_broadcastw_epi16 (__m128i a) VPBROADCASTW xmm, m16 The above native
docs: signature does not directly correspond to the managed signature.
clr-sig: Vector128<short> BroadcastScalarToVector128(short* source);

    
docs: __m128i _mm_broadcastd_epi32 (__m128i a) VPBROADCASTD xmm, m32 The above native
docs: signature does not directly correspond to the managed signature.
clr-sig: Vector128<int> BroadcastScalarToVector128(int* source);

    
docs: __m128i _mm_broadcastq_epi64 (__m128i a) VPBROADCASTQ xmm, m64 The above native
docs: signature does not directly correspond to the managed signature.
clr-sig: Vector128<long> BroadcastScalarToVector128(long* source);

    
docs: __m128i _mm_broadcastb_epi8 (__m128i a) VPBROADCASTB xmm, m8 The above native
docs: signature does not directly correspond to the managed signature.
clr-sig: Vector128<byte> BroadcastScalarToVector128(byte* source);

    

    
docs: __m128i _mm_broadcastw_epi16 (__m128i a) VPBROADCASTW xmm, xmm
clr-sig: Vector128<short> BroadcastScalarToVector128(Vector128<short> value);

    
docs: __m128i _mm_broadcastb_epi8 (__m128i a) VPBROADCASTB xmm, xmm
clr-sig: Vector128<byte> BroadcastScalarToVector128(Vector128<byte> value);

    
docs: __m256i _mm256_broadcastw_epi16 (__m128i a) VPBROADCASTW ymm, xmm
clr-sig: Vector256<ushort> BroadcastScalarToVector256(Vector128<ushort> value);

    
docs: __m256i _mm256_broadcastq_epi64 (__m128i a) VPBROADCASTQ ymm, m64 The above native
docs: signature does not directly correspond to the managed signature.
clr-sig: Vector256<ulong> BroadcastScalarToVector256(ulong* source);

    
docs: __m256i _mm256_broadcastd_epi32 (__m128i a) VPBROADCASTD ymm, m32 The above native
docs: signature does not directly correspond to the managed signature.
clr-sig: Vector256<uint> BroadcastScalarToVector256(uint* source);

    
docs: __m256i _mm256_broadcastw_epi16 (__m128i a) VPBROADCASTW ymm, m16 The above native
docs: signature does not directly correspond to the managed signature.
clr-sig: Vector256<ushort> BroadcastScalarToVector256(ushort* source);

    
docs: __m256i _mm256_broadcastb_epi8 (__m128i a) VPBROADCASTB ymm, m8 The above native
docs: signature does not directly correspond to the managed signature.
clr-sig: Vector256<sbyte> BroadcastScalarToVector256(sbyte* source);

    
docs: __m256i _mm256_broadcastq_epi64 (__m128i a) VPBROADCASTQ ymm, xmm
clr-sig: Vector256<ulong> BroadcastScalarToVector256(Vector128<ulong> value);

    
docs: __m256i _mm256_broadcastd_epi32 (__m128i a) VPBROADCASTD ymm, xmm
clr-sig: Vector256<uint> BroadcastScalarToVector256(Vector128<uint> value);

    
docs: __m256 _mm256_broadcastss_ps (__m128 a) VBROADCASTSS ymm, xmm
clr-sig: Vector256<float> BroadcastScalarToVector256(Vector128<float> value);

    
docs: __m256i _mm256_broadcastb_epi8 (__m128i a) VPBROADCASTB ymm, m8 The above native
docs: signature does not directly correspond to the managed signature.
clr-sig: Vector256<byte> BroadcastScalarToVector256(byte* source);

    
docs: __m256i _mm256_broadcastq_epi64 (__m128i a) VPBROADCASTQ ymm, xmm
clr-sig: Vector256<long> BroadcastScalarToVector256(Vector128<long> value);

    
docs: __m256i _mm256_broadcastd_epi32 (__m128i a) VPBROADCASTD ymm, xmm
clr-sig: Vector256<int> BroadcastScalarToVector256(Vector128<int> value);

    
docs: __m256i _mm256_broadcastw_epi16 (__m128i a) VPBROADCASTW ymm, xmm
clr-sig: Vector256<short> BroadcastScalarToVector256(Vector128<short> value);

    
docs: __m256d _mm256_broadcastsd_pd (__m128d a) VBROADCASTSD ymm, xmm
clr-sig: Vector256<double> BroadcastScalarToVector256(Vector128<double> value);

    
docs: __m256i _mm256_broadcastb_epi8 (__m128i a) VPBROADCASTB ymm, xmm
clr-sig: Vector256<byte> BroadcastScalarToVector256(Vector128<byte> value);

    
docs: __m256i _mm256_broadcastq_epi64 (__m128i a) VPBROADCASTQ ymm, m64 The above native
docs: signature does not directly correspond to the managed signature.
clr-sig: Vector256<long> BroadcastScalarToVector256(long* source);

    
docs: __m256i _mm256_broadcastd_epi32 (__m128i a) VPBROADCASTD ymm, m32 The above native
docs: signature does not directly correspond to the managed signature.
clr-sig: Vector256<int> BroadcastScalarToVector256(int* source);

    
docs: __m256i _mm256_broadcastw_epi16 (__m128i a) VPBROADCASTW ymm, m16 The above native
docs: signature does not directly correspond to the managed signature.
clr-sig: Vector256<short> BroadcastScalarToVector256(short* source);

    
docs: __m256i _mm256_broadcastb_epi8 (__m128i a) VPBROADCASTB ymm, xmm
clr-sig: Vector256<sbyte> BroadcastScalarToVector256(Vector128<sbyte> value);

    
docs: __m256i _mm256_broadcastsi128_si256 (__m128i a) VBROADCASTI128 ymm, m128 The
docs: above native signature does not directly correspond to the managed signature.
clr-sig: Vector256<ulong> BroadcastVector128ToVector256(ulong* address);

    
docs: __m256i _mm256_broadcastsi128_si256 (__m128i a) VBROADCASTI128 ymm, m128 The
docs: above native signature does not directly correspond to the managed signature.
clr-sig: Vector256<uint> BroadcastVector128ToVector256(uint* address);

    
docs: __m256i _mm256_broadcastsi128_si256 (__m128i a) VBROADCASTI128 ymm, m128 The
docs: above native signature does not directly correspond to the managed signature.
clr-sig: Vector256<sbyte> BroadcastVector128ToVector256(sbyte* address);

    
docs: __m256i _mm256_broadcastsi128_si256 (__m128i a) VBROADCASTI128 ymm, m128 The
docs: above native signature does not directly correspond to the managed signature.
clr-sig: Vector256<ushort> BroadcastVector128ToVector256(ushort* address);

    
docs: __m256i _mm256_broadcastsi128_si256 (__m128i a) VBROADCASTI128 ymm, m128 The
docs: above native signature does not directly correspond to the managed signature.
clr-sig: Vector256<int> BroadcastVector128ToVector256(int* address);

    
docs: __m256i _mm256_broadcastsi128_si256 (__m128i a) VBROADCASTI128 ymm, m128 The
docs: above native signature does not directly correspond to the managed signature.
clr-sig: Vector256<short> BroadcastVector128ToVector256(short* address);

    
docs: __m256i _mm256_broadcastsi128_si256 (__m128i a) VBROADCASTI128 ymm, m128 The
docs: above native signature does not directly correspond to the managed signature.
clr-sig: Vector256<byte> BroadcastVector128ToVector256(byte* address);

    
docs: __m256i _mm256_broadcastsi128_si256 (__m128i a) VBROADCASTI128 ymm, m128 The
docs: above native signature does not directly correspond to the managed signature.
clr-sig: Vector256<long> BroadcastVector128ToVector256(long* address);

    
docs: __m256i _mm256_cmpeq_epi32 (__m256i a, __m256i b) VPCMPEQD ymm, ymm, ymm/m256
clr-sig: Vector256<uint> CompareEqual(Vector256<uint> left, Vector256<uint> right);

    
docs: __m256i _mm256_cmpeq_epi16 (__m256i a, __m256i b) VPCMPEQW ymm, ymm, ymm/m256
clr-sig: Vector256<ushort> CompareEqual(Vector256<ushort> left, Vector256<ushort> right);

    
docs: __m256i _mm256_cmpeq_epi8 (__m256i a, __m256i b) VPCMPEQB ymm, ymm, ymm/m256
clr-sig: Vector256<sbyte> CompareEqual(Vector256<sbyte> left, Vector256<sbyte> right);

    
docs: __m256i _mm256_cmpeq_epi64 (__m256i a, __m256i b) VPCMPEQQ ymm, ymm, ymm/m256
clr-sig: Vector256<ulong> CompareEqual(Vector256<ulong> left, Vector256<ulong> right);

    
docs: __m256i _mm256_cmpeq_epi32 (__m256i a, __m256i b) VPCMPEQD ymm, ymm, ymm/m256
clr-sig: Vector256<int> CompareEqual(Vector256<int> left, Vector256<int> right);

    
docs: __m256i _mm256_cmpeq_epi16 (__m256i a, __m256i b) VPCMPEQW ymm, ymm, ymm/m256
clr-sig: Vector256<short> CompareEqual(Vector256<short> left, Vector256<short> right);

    
docs: __m256i _mm256_cmpeq_epi8 (__m256i a, __m256i b) VPCMPEQB ymm, ymm, ymm/m256
clr-sig: Vector256<byte> CompareEqual(Vector256<byte> left, Vector256<byte> right);

    
docs: __m256i _mm256_cmpeq_epi64 (__m256i a, __m256i b) VPCMPEQQ ymm, ymm, ymm/m256
clr-sig: Vector256<long> CompareEqual(Vector256<long> left, Vector256<long> right);

    
docs: __m256i _mm256_cmpgt_epi16 (__m256i a, __m256i b) VPCMPGTW ymm, ymm, ymm/m256
clr-sig: Vector256<short> CompareGreaterThan(Vector256<short> left, Vector256<short> right);

    
docs: __m256i _mm256_cmpgt_epi32 (__m256i a, __m256i b) VPCMPGTD ymm, ymm, ymm/m256
clr-sig: Vector256<int> CompareGreaterThan(Vector256<int> left, Vector256<int> right);

    
docs: __m256i _mm256_cmpgt_epi64 (__m256i a, __m256i b) VPCMPGTQ ymm, ymm, ymm/m256
clr-sig: Vector256<long> CompareGreaterThan(Vector256<long> left, Vector256<long> right);

    
docs: __m256i _mm256_cmpgt_epi8 (__m256i a, __m256i b) VPCMPGTB ymm, ymm, ymm/m256
clr-sig: Vector256<sbyte> CompareGreaterThan(Vector256<sbyte> left, Vector256<sbyte> right);

    

    
docs: __m256i _mm256_cvtepi8_epi16 (__m128i a) VPMOVSXBW ymm, xmm/m128
clr-sig: Vector256<short> ConvertToVector256Int16(Vector128<sbyte> value);

    
docs: __m256i _mm256_cvtepi16_epi32 (__m128i a) VPMOVSXWD ymm, xmm/m128
clr-sig: Vector256<int> ConvertToVector256Int32(Vector128<short> value);

    
docs: __m256i _mm256_cvtepi8_epi32 (__m128i a) VPMOVSXBD ymm, xmm/m128
clr-sig: Vector256<int> ConvertToVector256Int32(Vector128<sbyte> value);

    
docs: __m256i _mm256_cvtepi8_epi64 (__m128i a) VPMOVSXBQ ymm, xmm/m128
clr-sig: Vector256<long> ConvertToVector256Int64(Vector128<sbyte> value);

    
docs: __m256i _mm256_cvtepi32_epi64 (__m128i a) VPMOVSXDQ ymm, xmm/m128
clr-sig: Vector256<long> ConvertToVector256Int64(Vector128<int> value);

    
docs: __m256i _mm256_cvtepi16_epi64 (__m128i a) VPMOVSXWQ ymm, xmm/m128
clr-sig: Vector256<long> ConvertToVector256Int64(Vector128<short> value);

    
docs: __m256i _mm256_cvtepu8_epi16 (__m128i a) VPMOVZXBW ymm, xmm/m128
clr-sig: Vector256<ushort> ConvertToVector256UInt16(Vector128<byte> value);

    
docs: __m256i _mm256_cvtepu8_epi32 (__m128i a) VPMOVZXBD ymm, xmm/m128
clr-sig: Vector256<uint> ConvertToVector256UInt32(Vector128<byte> value);

    
docs: __m256i _mm256_cvtepu16_epi32 (__m128i a) VPMOVZXWD ymm, xmm/m128
clr-sig: Vector256<uint> ConvertToVector256UInt32(Vector128<ushort> value);

    
docs: __m256i _mm256_cvtepu8_epi64 (__m128i a) VPMOVZXBQ ymm, xmm/m128
clr-sig: Vector256<ulong> ConvertToVector256UInt64(Vector128<byte> value);

    
docs: __m256i _mm256_cvtepu16_epi64 (__m128i a) VPMOVZXWQ ymm, xmm/m128
clr-sig: Vector256<ulong> ConvertToVector256UInt64(Vector128<ushort> value);

    
docs: __m256i _mm256_cvtepu32_epi64 (__m128i a) VPMOVZXDQ ymm, xmm/m128
clr-sig: Vector256<ulong> ConvertToVector256UInt64(Vector128<uint> value);

    
docs: __m128i _mm256_extracti128_si256 (__m256i a, const int imm8) VEXTRACTI128 xmm,
docs: ymm, imm8
clr-sig: Vector128<ulong> ExtractVector128(Vector256<ulong> value, byte index);

    
docs: __m128i _mm256_extracti128_si256 (__m256i a, const int imm8) VEXTRACTI128 xmm,
docs: ymm, imm8
clr-sig: Vector128<uint> ExtractVector128(Vector256<uint> value, byte index);

    
docs: __m128i _mm256_extracti128_si256 (__m256i a, const int imm8) VEXTRACTI128 xmm,
docs: ymm, imm8
clr-sig: Vector128<ushort> ExtractVector128(Vector256<ushort> value, byte index);

    
docs: __m128i _mm256_extracti128_si256 (__m256i a, const int imm8) VEXTRACTI128 xmm,
docs: ymm, imm8
clr-sig: Vector128<sbyte> ExtractVector128(Vector256<sbyte> value, byte index);

    
docs: __m128i _mm256_extracti128_si256 (__m256i a, const int imm8) VEXTRACTI128 xmm,
docs: ymm, imm8
clr-sig: Vector128<long> ExtractVector128(Vector256<long> value, byte index);

    
docs: __m128i _mm256_extracti128_si256 (__m256i a, const int imm8) VEXTRACTI128 xmm,
docs: ymm, imm8
clr-sig: Vector128<int> ExtractVector128(Vector256<int> value, byte index);

    
docs: __m128i _mm256_extracti128_si256 (__m256i a, const int imm8) VEXTRACTI128 xmm,
docs: ymm, imm8
clr-sig: Vector128<short> ExtractVector128(Vector256<short> value, byte index);

    
docs: __m128i _mm256_extracti128_si256 (__m256i a, const int imm8) VEXTRACTI128 xmm,
docs: ymm, imm8
clr-sig: Vector128<byte> ExtractVector128(Vector256<byte> value, byte index);

    
docs: __m128i _mm_mask_i64gather_epi64 (__m128i src, __int64 const* base_addr, __m128i
docs: vindex, __m128i mask, const int scale) VPGATHERQQ xmm, vm64x, xmm The scale parameter
docs: should be 1, 2, 4 or 8, otherwise, ArgumentOutOfRangeException will be thrown.
clr-sig: Vector128<ulong> GatherMaskVector128(Vector128<ulong> source, ulong* baseAddress, Vector128<long> index, Vector128<ulong> mask, byte scale);

    
docs: __m128i _mm_mask_i32gather_epi64 (__m128i src, __int64 const* base_addr, __m128i
docs: vindex, __m128i mask, const int scale) VPGATHERDQ xmm, vm32x, xmm The scale parameter
docs: should be 1, 2, 4 or 8, otherwise, ArgumentOutOfRangeException will be thrown.
clr-sig: Vector128<ulong> GatherMaskVector128(Vector128<ulong> source, ulong* baseAddress, Vector128<int> index, Vector128<ulong> mask, byte scale);

    
docs: __m128i _mm256_mask_i64gather_epi32 (__m128i src, int const* base_addr, __m256i
docs: vindex, __m128i mask, const int scale) VPGATHERQD xmm, vm32y, xmm The scale parameter
docs: should be 1, 2, 4 or 8, otherwise, ArgumentOutOfRangeException will be thrown.
clr-sig: Vector128<uint> GatherMaskVector128(Vector128<uint> source, uint* baseAddress, Vector256<long> index, Vector128<uint> mask, byte scale);

    
docs: __m128i _mm_mask_i64gather_epi32 (__m128i src, int const* base_addr, __m128i
docs: vindex, __m128i mask, const int scale) VPGATHERQD xmm, vm64x, xmm The scale parameter
docs: should be 1, 2, 4 or 8, otherwise, ArgumentOutOfRangeException will be thrown.
clr-sig: Vector128<uint> GatherMaskVector128(Vector128<uint> source, uint* baseAddress, Vector128<long> index, Vector128<uint> mask, byte scale);

    
docs: __m128 _mm256_mask_i64gather_ps (__m128 src, float const* base_addr, __m256i
docs: vindex, __m128 mask, const int scale) VGATHERQPS xmm, vm32y, xmm The scale parameter
docs: should be 1, 2, 4 or 8, otherwise, ArgumentOutOfRangeException will be thrown.
clr-sig: Vector128<float> GatherMaskVector128(Vector128<float> source, float* baseAddress, Vector256<long> index, Vector128<float> mask, byte scale);

    
docs: __m128 _mm_mask_i64gather_ps (__m128 src, float const* base_addr, __m128i vindex,
docs: __m128 mask, const int scale) VGATHERQPS xmm, vm64x, xmm The scale parameter
docs: should be 1, 2, 4 or 8, otherwise, ArgumentOutOfRangeException will be thrown.
clr-sig: Vector128<float> GatherMaskVector128(Vector128<float> source, float* baseAddress, Vector128<long> index, Vector128<float> mask, byte scale);

    
docs: __m128i _mm_mask_i32gather_epi32 (__m128i src, int const* base_addr, __m128i
docs: vindex, __m128i mask, const int scale) VPGATHERDD xmm, vm32x, xmm The scale parameter
docs: should be 1, 2, 4 or 8, otherwise, ArgumentOutOfRangeException will be thrown.
clr-sig: Vector128<uint> GatherMaskVector128(Vector128<uint> source, uint* baseAddress, Vector128<int> index, Vector128<uint> mask, byte scale);

    
docs: __m128i _mm_mask_i64gather_epi64 (__m128i src, __int64 const* base_addr, __m128i
docs: vindex, __m128i mask, const int scale) VPGATHERQQ xmm, vm64x, xmm The scale parameter
docs: should be 1, 2, 4 or 8, otherwise, ArgumentOutOfRangeException will be thrown.
clr-sig: Vector128<long> GatherMaskVector128(Vector128<long> source, long* baseAddress, Vector128<long> index, Vector128<long> mask, byte scale);

    
docs: __m128d _mm_mask_i32gather_pd (__m128d src, double const* base_addr, __m128i
docs: vindex, __m128d mask, const int scale) VGATHERDPD xmm, vm32x, xmm The scale parameter
docs: should be 1, 2, 4 or 8, otherwise, ArgumentOutOfRangeException will be thrown.
clr-sig: Vector128<double> GatherMaskVector128(Vector128<double> source, double* baseAddress, Vector128<int> index, Vector128<double> mask, byte scale);

    
docs: __m128d _mm_mask_i64gather_pd (__m128d src, double const* base_addr, __m128i
docs: vindex, __m128d mask, const int scale) VGATHERQPD xmm, vm64x, xmm The scale parameter
docs: should be 1, 2, 4 or 8, otherwise, ArgumentOutOfRangeException will be thrown.
clr-sig: Vector128<double> GatherMaskVector128(Vector128<double> source, double* baseAddress, Vector128<long> index, Vector128<double> mask, byte scale);

    
docs: __m128i _mm_mask_i32gather_epi32 (__m128i src, int const* base_addr, __m128i
docs: vindex, __m128i mask, const int scale) VPGATHERDD xmm, vm32x, xmm The scale parameter
docs: should be 1, 2, 4 or 8, otherwise, ArgumentOutOfRangeException will be thrown.
clr-sig: Vector128<int> GatherMaskVector128(Vector128<int> source, int* baseAddress, Vector128<int> index, Vector128<int> mask, byte scale);

    
docs: __m128 _mm_mask_i32gather_ps (__m128 src, float const* base_addr, __m128i vindex,
docs: __m128 mask, const int scale) VGATHERDPS xmm, vm32x, xmm The scale parameter
docs: should be 1, 2, 4 or 8, otherwise, ArgumentOutOfRangeException will be thrown.
clr-sig: Vector128<float> GatherMaskVector128(Vector128<float> source, float* baseAddress, Vector128<int> index, Vector128<float> mask, byte scale);

    
docs: __m128i _mm256_mask_i64gather_epi32 (__m128i src, int const* base_addr, __m256i
docs: vindex, __m128i mask, const int scale) VPGATHERQD xmm, vm32y, xmm The scale parameter
docs: should be 1, 2, 4 or 8, otherwise, ArgumentOutOfRangeException will be thrown.
clr-sig: Vector128<int> GatherMaskVector128(Vector128<int> source, int* baseAddress, Vector256<long> index, Vector128<int> mask, byte scale);

    
docs: __m128i _mm_mask_i32gather_epi64 (__m128i src, __int64 const* base_addr, __m128i
docs: vindex, __m128i mask, const int scale) VPGATHERDQ xmm, vm32x, xmm The scale parameter
docs: should be 1, 2, 4 or 8, otherwise, ArgumentOutOfRangeException will be thrown.
clr-sig: Vector128<long> GatherMaskVector128(Vector128<long> source, long* baseAddress, Vector128<int> index, Vector128<long> mask, byte scale);

    
docs: __m128i _mm_mask_i64gather_epi32 (__m128i src, int const* base_addr, __m128i
docs: vindex, __m128i mask, const int scale) VPGATHERQD xmm, vm64x, xmm The scale parameter
docs: should be 1, 2, 4 or 8, otherwise, ArgumentOutOfRangeException will be thrown.
clr-sig: Vector128<int> GatherMaskVector128(Vector128<int> source, int* baseAddress, Vector128<long> index, Vector128<int> mask, byte scale);

    
docs: __m256i _mm256_mask_i64gather_epi64 (__m256i src, __int64 const* base_addr, __m256i
docs: vindex, __m256i mask, const int scale) VPGATHERQQ ymm, vm32y, ymm The scale parameter
docs: should be 1, 2, 4 or 8, otherwise, ArgumentOutOfRangeException will be thrown.
clr-sig: Vector256<ulong> GatherMaskVector256(Vector256<ulong> source, ulong* baseAddress, Vector256<long> index, Vector256<ulong> mask, byte scale);

    
docs: __m256i _mm256_mask_i32gather_epi64 (__m256i src, __int64 const* base_addr, __m128i
docs: vindex, __m256i mask, const int scale) VPGATHERDQ ymm, vm32y, ymm The scale parameter
docs: should be 1, 2, 4 or 8, otherwise, ArgumentOutOfRangeException will be thrown.
clr-sig: Vector256<ulong> GatherMaskVector256(Vector256<ulong> source, ulong* baseAddress, Vector128<int> index, Vector256<ulong> mask, byte scale);

    
docs: __m256 _mm256_mask_i32gather_ps (__m256 src, float const* base_addr, __m256i
docs: vindex, __m256 mask, const int scale) VPGATHERDPS ymm, vm32y, ymm The scale parameter
docs: should be 1, 2, 4 or 8, otherwise, ArgumentOutOfRangeException will be thrown.
clr-sig: Vector256<float> GatherMaskVector256(Vector256<float> source, float* baseAddress, Vector256<int> index, Vector256<float> mask, byte scale);

    
docs: __m256i _mm256_mask_i32gather_epi32 (__m256i src, int const* base_addr, __m256i
docs: vindex, __m256i mask, const int scale) VPGATHERDD ymm, vm32y, ymm The scale parameter
docs: should be 1, 2, 4 or 8, otherwise, ArgumentOutOfRangeException will be thrown.
clr-sig: Vector256<uint> GatherMaskVector256(Vector256<uint> source, uint* baseAddress, Vector256<int> index, Vector256<uint> mask, byte scale);

    
docs: __m256i _mm256_mask_i32gather_epi64 (__m256i src, __int64 const* base_addr, __m128i
docs: vindex, __m256i mask, const int scale) VPGATHERDQ ymm, vm32y, ymm The scale parameter
docs: should be 1, 2, 4 or 8, otherwise, ArgumentOutOfRangeException will be thrown.
clr-sig: Vector256<long> GatherMaskVector256(Vector256<long> source, long* baseAddress, Vector128<int> index, Vector256<long> mask, byte scale);

    
docs: __m256i _mm256_mask_i32gather_epi32 (__m256i src, int const* base_addr, __m256i
docs: vindex, __m256i mask, const int scale) VPGATHERDD ymm, vm32y, ymm The scale parameter
docs: should be 1, 2, 4 or 8, otherwise, ArgumentOutOfRangeException will be thrown.
clr-sig: Vector256<int> GatherMaskVector256(Vector256<int> source, int* baseAddress, Vector256<int> index, Vector256<int> mask, byte scale);

    
docs: __m256d _mm256_mask_i64gather_pd (__m256d src, double const* base_addr, __m256i
docs: vindex, __m256d mask, const int scale) VGATHERQPD ymm, vm32y, ymm The scale parameter
docs: should be 1, 2, 4 or 8, otherwise, ArgumentOutOfRangeException will be thrown.
clr-sig: Vector256<double> GatherMaskVector256(Vector256<double> source, double* baseAddress, Vector256<long> index, Vector256<double> mask, byte scale);

    
docs: __m256d _mm256_mask_i32gather_pd (__m256d src, double const* base_addr, __m128i
docs: vindex, __m256d mask, const int scale) VPGATHERDPD ymm, vm32y, ymm The scale
docs: parameter should be 1, 2, 4 or 8, otherwise, ArgumentOutOfRangeException will
docs: be thrown.
clr-sig: Vector256<double> GatherMaskVector256(Vector256<double> source, double* baseAddress, Vector128<int> index, Vector256<double> mask, byte scale);

    
docs: __m256i _mm256_mask_i64gather_epi64 (__m256i src, __int64 const* base_addr, __m256i
docs: vindex, __m256i mask, const int scale) VPGATHERQQ ymm, vm32y, ymm The scale parameter
docs: should be 1, 2, 4 or 8, otherwise, ArgumentOutOfRangeException will be thrown.
clr-sig: Vector256<long> GatherMaskVector256(Vector256<long> source, long* baseAddress, Vector256<long> index, Vector256<long> mask, byte scale);

    
docs: __m128i _mm_i64gather_epi64 (__int64 const* base_addr, __m128i vindex, const
docs: int scale) VPGATHERQQ xmm, vm64x, xmm The scale parameter should be 1, 2, 4 or
docs: 8, otherwise, ArgumentOutOfRangeException will be thrown.
clr-sig: Vector128<ulong> GatherVector128(ulong* baseAddress, Vector128<long> index, byte scale);

    
docs: __m128i _mm_i32gather_epi64 (__int64 const* base_addr, __m128i vindex, const
docs: int scale) VPGATHERDQ xmm, vm32x, xmm The scale parameter should be 1, 2, 4 or
docs: 8, otherwise, ArgumentOutOfRangeException will be thrown.
clr-sig: Vector128<ulong> GatherVector128(ulong* baseAddress, Vector128<int> index, byte scale);

    
docs: __m128i _mm256_i64gather_epi32 (int const* base_addr, __m256i vindex, const int
docs: scale) VPGATHERQD xmm, vm64y, xmm The scale parameter should be 1, 2, 4 or 8,
docs: otherwise, ArgumentOutOfRangeException will be thrown.
clr-sig: Vector128<uint> GatherVector128(uint* baseAddress, Vector256<long> index, byte scale);

    
docs: __m128i _mm_i64gather_epi32 (int const* base_addr, __m128i vindex, const int
docs: scale) VPGATHERQD xmm, vm64x, xmm The scale parameter should be 1, 2, 4 or 8,
docs: otherwise, ArgumentOutOfRangeException will be thrown.
clr-sig: Vector128<uint> GatherVector128(uint* baseAddress, Vector128<long> index, byte scale);

    
docs: __m128i _mm_i32gather_epi32 (int const* base_addr, __m128i vindex, const int
docs: scale) VPGATHERDD xmm, vm32x, xmm The scale parameter should be 1, 2, 4 or 8,
docs: otherwise, ArgumentOutOfRangeException will be thrown.
clr-sig: Vector128<uint> GatherVector128(uint* baseAddress, Vector128<int> index, byte scale);

    
docs: __m128 _mm256_i64gather_ps (float const* base_addr, __m256i vindex, const int
docs: scale) VGATHERQPS xmm, vm64y, xmm The scale parameter should be 1, 2, 4 or 8,
docs: otherwise, ArgumentOutOfRangeException will be thrown.
clr-sig: Vector128<float> GatherVector128(float* baseAddress, Vector256<long> index, byte scale);

    
docs: __m128 _mm_i64gather_ps (float const* base_addr, __m128i vindex, const int scale)
docs: VGATHERQPS xmm, vm64x, xmm The scale parameter should be 1, 2, 4 or 8, otherwise,
docs: ArgumentOutOfRangeException will be thrown.
clr-sig: Vector128<float> GatherVector128(float* baseAddress, Vector128<long> index, byte scale);

    
docs: __m128i _mm_i64gather_epi64 (__int64 const* base_addr, __m128i vindex, const
docs: int scale) VPGATHERQQ xmm, vm64x, xmm The scale parameter should be 1, 2, 4 or
docs: 8, otherwise, ArgumentOutOfRangeException will be thrown.
clr-sig: Vector128<long> GatherVector128(long* baseAddress, Vector128<long> index, byte scale);

    
docs: __m128i _mm_i32gather_epi64 (__int64 const* base_addr, __m128i vindex, const
docs: int scale) VPGATHERDQ xmm, vm32x, xmm The scale parameter should be 1, 2, 4 or
docs: 8, otherwise, ArgumentOutOfRangeException will be thrown.
clr-sig: Vector128<long> GatherVector128(long* baseAddress, Vector128<int> index, byte scale);

    
docs: __m128i _mm256_i64gather_epi32 (int const* base_addr, __m256i vindex, const int
docs: scale) VPGATHERQD xmm, vm64y, xmm The scale parameter should be 1, 2, 4 or 8,
docs: otherwise, ArgumentOutOfRangeException will be thrown.
clr-sig: Vector128<int> GatherVector128(int* baseAddress, Vector256<long> index, byte scale);

    
docs: __m128i _mm_i64gather_epi32 (int const* base_addr, __m128i vindex, const int
docs: scale) VPGATHERQD xmm, vm64x, xmm The scale parameter should be 1, 2, 4 or 8,
docs: otherwise, ArgumentOutOfRangeException will be thrown.
clr-sig: Vector128<int> GatherVector128(int* baseAddress, Vector128<long> index, byte scale);

    
docs: __m128i _mm_i32gather_epi32 (int const* base_addr, __m128i vindex, const int
docs: scale) VPGATHERDD xmm, vm32x, xmm The scale parameter should be 1, 2, 4 or 8,
docs: otherwise, ArgumentOutOfRangeException will be thrown.
clr-sig: Vector128<int> GatherVector128(int* baseAddress, Vector128<int> index, byte scale);

    
docs: __m128d _mm_i64gather_pd (double const* base_addr, __m128i vindex, const int
docs: scale) VGATHERQPD xmm, vm64x, xmm The scale parameter should be 1, 2, 4 or 8,
docs: otherwise, ArgumentOutOfRangeException will be thrown.
clr-sig: Vector128<double> GatherVector128(double* baseAddress, Vector128<long> index, byte scale);

    
docs: __m128d _mm_i32gather_pd (double const* base_addr, __m128i vindex, const int
docs: scale) VGATHERDPD xmm, vm32x, xmm The scale parameter should be 1, 2, 4 or 8,
docs: otherwise, ArgumentOutOfRangeException will be thrown.
clr-sig: Vector128<double> GatherVector128(double* baseAddress, Vector128<int> index, byte scale);

    
docs: __m128 _mm_i32gather_ps (float const* base_addr, __m128i vindex, const int scale)
docs: VGATHERDPS xmm, vm32x, xmm The scale parameter should be 1, 2, 4 or 8, otherwise,
docs: ArgumentOutOfRangeException will be thrown.
clr-sig: Vector128<float> GatherVector128(float* baseAddress, Vector128<int> index, byte scale);

    
docs: __m256i _mm256_i64gather_epi64 (__int64 const* base_addr, __m256i vindex, const
docs: int scale) VPGATHERQQ ymm, vm64y, ymm The scale parameter should be 1, 2, 4 or
docs: 8, otherwise, ArgumentOutOfRangeException will be thrown.
clr-sig: Vector256<ulong> GatherVector256(ulong* baseAddress, Vector256<long> index, byte scale);

    
docs: __m256i _mm256_i32gather_epi64 (__int64 const* base_addr, __m128i vindex, const
docs: int scale) VPGATHERDQ ymm, vm32y, ymm The scale parameter should be 1, 2, 4 or
docs: 8, otherwise, ArgumentOutOfRangeException will be thrown.
clr-sig: Vector256<ulong> GatherVector256(ulong* baseAddress, Vector128<int> index, byte scale);

    
docs: __m256i _mm256_i32gather_epi32 (int const* base_addr, __m256i vindex, const int
docs: scale) VPGATHERDD ymm, vm32y, ymm The scale parameter should be 1, 2, 4 or 8,
docs: otherwise, ArgumentOutOfRangeException will be thrown.
clr-sig: Vector256<uint> GatherVector256(uint* baseAddress, Vector256<int> index, byte scale);

    
docs: __m256 _mm256_i32gather_ps (float const* base_addr, __m256i vindex, const int
docs: scale) VGATHERDPS ymm, vm32y, ymm The scale parameter should be 1, 2, 4 or 8,
docs: otherwise, ArgumentOutOfRangeException will be thrown.
clr-sig: Vector256<float> GatherVector256(float* baseAddress, Vector256<int> index, byte scale);

    
docs: __m256i _mm256_i32gather_epi64 (__int64 const* base_addr, __m128i vindex, const
docs: int scale) VPGATHERDQ ymm, vm32y, ymm The scale parameter should be 1, 2, 4 or
docs: 8, otherwise, ArgumentOutOfRangeException will be thrown.
clr-sig: Vector256<long> GatherVector256(long* baseAddress, Vector128<int> index, byte scale);

    
docs: __m256i _mm256_i32gather_epi32 (int const* base_addr, __m256i vindex, const int
docs: scale) VPGATHERDD ymm, vm32y, ymm The scale parameter should be 1, 2, 4 or 8,
docs: otherwise, ArgumentOutOfRangeException will be thrown.
clr-sig: Vector256<int> GatherVector256(int* baseAddress, Vector256<int> index, byte scale);

    
docs: __m256d _mm256_i64gather_pd (double const* base_addr, __m256i vindex, const int
docs: scale) VGATHERQPD ymm, vm64y, ymm The scale parameter should be 1, 2, 4 or 8,
docs: otherwise, ArgumentOutOfRangeException will be thrown.
clr-sig: Vector256<double> GatherVector256(double* baseAddress, Vector256<long> index, byte scale);

    
docs: __m256d _mm256_i32gather_pd (double const* base_addr, __m128i vindex, const int
docs: scale) VGATHERDPD ymm, vm32y, ymm The scale parameter should be 1, 2, 4 or 8,
docs: otherwise, ArgumentOutOfRangeException will be thrown.
clr-sig: Vector256<double> GatherVector256(double* baseAddress, Vector128<int> index, byte scale);

    
docs: __m256i _mm256_i64gather_epi64 (__int64 const* base_addr, __m256i vindex, const
docs: int scale) VPGATHERQQ ymm, vm64y, ymm The scale parameter should be 1, 2, 4 or
docs: 8, otherwise, ArgumentOutOfRangeException will be thrown.
clr-sig: Vector256<long> GatherVector256(long* baseAddress, Vector256<long> index, byte scale);

    
docs: __m256i _mm256_hadd_epi16 (__m256i a, __m256i b) VPHADDW ymm, ymm, ymm/m256
clr-sig: Vector256<short> HorizontalAdd(Vector256<short> left, Vector256<short> right);

    
docs: __m256i _mm256_hadd_epi32 (__m256i a, __m256i b) VPHADDD ymm, ymm, ymm/m256
clr-sig: Vector256<int> HorizontalAdd(Vector256<int> left, Vector256<int> right);

    
docs: __m256i _mm256_hadds_epi16 (__m256i a, __m256i b) VPHADDSW ymm, ymm, ymm/m256
clr-sig: Vector256<short> HorizontalAddSaturate(Vector256<short> left, Vector256<short> right);

    
docs: __m256i _mm256_hsub_epi16 (__m256i a, __m256i b) VPHSUBW ymm, ymm, ymm/m256
clr-sig: Vector256<short> HorizontalSubtract(Vector256<short> left, Vector256<short> right);

    
docs: __m256i _mm256_hsub_epi32 (__m256i a, __m256i b) VPHSUBD ymm, ymm, ymm/m256
clr-sig: Vector256<int> HorizontalSubtract(Vector256<int> left, Vector256<int> right);

    
docs: __m256i _mm256_hsubs_epi16 (__m256i a, __m256i b) VPHSUBSW ymm, ymm, ymm/m256
clr-sig: Vector256<short> HorizontalSubtractSaturate(Vector256<short> left, Vector256<short> right);

    
docs: __m256i _mm256_inserti128_si256 (__m256i a, __m128i b, const int imm8) VINSERTI128
docs: ymm, ymm, xmm, imm8
clr-sig: Vector256<ulong> InsertVector128(Vector256<ulong> value, Vector128<ulong> data, byte index);

    
docs: __m256i _mm256_inserti128_si256 (__m256i a, __m128i b, const int imm8) VINSERTI128
docs: ymm, ymm, xmm, imm8
clr-sig: Vector256<uint> InsertVector128(Vector256<uint> value, Vector128<uint> data, byte index);

    
docs: __m256i _mm256_inserti128_si256 (__m256i a, __m128i b, const int imm8) VINSERTI128
docs: ymm, ymm, xmm, imm8
clr-sig: Vector256<sbyte> InsertVector128(Vector256<sbyte> value, Vector128<sbyte> data, byte index);

    
docs: __m256i _mm256_inserti128_si256 (__m256i a, __m128i b, const int imm8) VINSERTI128
docs: ymm, ymm, xmm, imm8
clr-sig: Vector256<ushort> InsertVector128(Vector256<ushort> value, Vector128<ushort> data, byte index);

    
docs: __m256i _mm256_inserti128_si256 (__m256i a, __m128i b, const int imm8) VINSERTI128
docs: ymm, ymm, xmm, imm8
clr-sig: Vector256<int> InsertVector128(Vector256<int> value, Vector128<int> data, byte index);

    
docs: __m256i _mm256_inserti128_si256 (__m256i a, __m128i b, const int imm8) VINSERTI128
docs: ymm, ymm, xmm, imm8
clr-sig: Vector256<byte> InsertVector128(Vector256<byte> value, Vector128<byte> data, byte index);

    
docs: __m256i _mm256_inserti128_si256 (__m256i a, __m128i b, const int imm8) VINSERTI128
docs: ymm, ymm, xmm, imm8
clr-sig: Vector256<long> InsertVector128(Vector256<long> value, Vector128<long> data, byte index);

    
docs: __m256i _mm256_inserti128_si256 (__m256i a, __m128i b, const int imm8) VINSERTI128
docs: ymm, ymm, xmm, imm8
clr-sig: Vector256<short> InsertVector128(Vector256<short> value, Vector128<short> data, byte index);

    
docs: __m256i _mm256_stream_load_si256 (__m256i const* mem_addr) VMOVNTDQA ymm, m256
clr-sig: Vector256<ulong> LoadAlignedVector256NonTemporal(ulong* address);

    
docs: __m256i _mm256_stream_load_si256 (__m256i const* mem_addr) VMOVNTDQA ymm, m256
clr-sig: Vector256<uint> LoadAlignedVector256NonTemporal(uint* address);

    
docs: __m256i _mm256_stream_load_si256 (__m256i const* mem_addr) VMOVNTDQA ymm, m256
clr-sig: Vector256<ushort> LoadAlignedVector256NonTemporal(ushort* address);

    
docs: __m256i _mm256_stream_load_si256 (__m256i const* mem_addr) VMOVNTDQA ymm, m256
clr-sig: Vector256<byte> LoadAlignedVector256NonTemporal(byte* address);

    
docs: __m256i _mm256_stream_load_si256 (__m256i const* mem_addr) VMOVNTDQA ymm, m256
clr-sig: Vector256<long> LoadAlignedVector256NonTemporal(long* address);

    
docs: __m256i _mm256_stream_load_si256 (__m256i const* mem_addr) VMOVNTDQA ymm, m256
clr-sig: Vector256<int> LoadAlignedVector256NonTemporal(int* address);

    
docs: __m256i _mm256_stream_load_si256 (__m256i const* mem_addr) VMOVNTDQA ymm, m256
clr-sig: Vector256<short> LoadAlignedVector256NonTemporal(short* address);

    
docs: __m256i _mm256_stream_load_si256 (__m256i const* mem_addr) VMOVNTDQA ymm, m256
clr-sig: Vector256<sbyte> LoadAlignedVector256NonTemporal(sbyte* address);

    
docs: __m128i _mm_maskload_epi32 (int const* mem_addr, __m128i mask) VPMASKMOVD xmm,
docs: xmm, m128
clr-sig: Vector128<int> MaskLoad(int* address, Vector128<int> mask);

    
docs: __m256i _mm256_maskload_epi32 (int const* mem_addr, __m256i mask) VPMASKMOVD
docs: ymm, ymm, m256
clr-sig: Vector256<int> MaskLoad(int* address, Vector256<int> mask);

    
docs: __m128i _mm_maskload_epi64 (__int64 const* mem_addr, __m128i mask) VPMASKMOVQ
docs: xmm, xmm, m128
clr-sig: Vector128<long> MaskLoad(long* address, Vector128<long> mask);

    
docs: __m256i _mm256_maskload_epi64 (__int64 const* mem_addr, __m256i mask) VPMASKMOVQ
docs: ymm, ymm, m256
clr-sig: Vector256<long> MaskLoad(long* address, Vector256<long> mask);

    
docs: __m128i _mm_maskload_epi32 (int const* mem_addr, __m128i mask) VPMASKMOVD xmm,
docs: xmm, m128
clr-sig: Vector128<uint> MaskLoad(uint* address, Vector128<uint> mask);

    
docs: __m256i _mm256_maskload_epi32 (int const* mem_addr, __m256i mask) VPMASKMOVD
docs: ymm, ymm, m256
clr-sig: Vector256<uint> MaskLoad(uint* address, Vector256<uint> mask);

    
docs: __m128i _mm_maskload_epi64 (__int64 const* mem_addr, __m128i mask) VPMASKMOVQ
docs: xmm, xmm, m128
clr-sig: Vector128<ulong> MaskLoad(ulong* address, Vector128<ulong> mask);

    
docs: __m256i _mm256_maskload_epi64 (__int64 const* mem_addr, __m256i mask) VPMASKMOVQ
docs: ymm, ymm, m256
clr-sig: Vector256<ulong> MaskLoad(ulong* address, Vector256<ulong> mask);

    
docs: void _mm256_maskstore_epi64 (__int64* mem_addr, __m256i mask, __m256i a) VPMASKMOVQ
docs: m256, ymm, ymm
clr-sig: Void MaskStore(ulong* address, Vector256<ulong> mask, Vector256<ulong> source);

    
docs: void _mm256_maskstore_epi32 (int* mem_addr, __m256i mask, __m256i a) VPMASKMOVD
docs: m256, ymm, ymm
clr-sig: Void MaskStore(uint* address, Vector256<uint> mask, Vector256<uint> source);

    
docs: void _mm_maskstore_epi32 (int* mem_addr, __m128i mask, __m128i a) VPMASKMOVD
docs: m128, xmm, xmm
clr-sig: Void MaskStore(uint* address, Vector128<uint> mask, Vector128<uint> source);

    
docs: void _mm_maskstore_epi64 (__int64* mem_addr, __m128i mask, __m128i a) VPMASKMOVQ
docs: m128, xmm, xmm
clr-sig: Void MaskStore(ulong* address, Vector128<ulong> mask, Vector128<ulong> source);

    
docs: void _mm_maskstore_epi64 (__int64* mem_addr, __m128i mask, __m128i a) VPMASKMOVQ
docs: m128, xmm, xmm
clr-sig: Void MaskStore(long* address, Vector128<long> mask, Vector128<long> source);

    
docs: void _mm256_maskstore_epi64 (__int64* mem_addr, __m256i mask, __m256i a) VPMASKMOVQ
docs: m256, ymm, ymm
clr-sig: Void MaskStore(long* address, Vector256<long> mask, Vector256<long> source);

    
docs: void _mm256_maskstore_epi32 (int* mem_addr, __m256i mask, __m256i a) VPMASKMOVD
docs: m256, ymm, ymm
clr-sig: Void MaskStore(int* address, Vector256<int> mask, Vector256<int> source);

    
docs: void _mm_maskstore_epi32 (int* mem_addr, __m128i mask, __m128i a) VPMASKMOVD
docs: m128, xmm, xmm
clr-sig: Void MaskStore(int* address, Vector128<int> mask, Vector128<int> source);

    
docs: __m256i _mm256_max_epu32 (__m256i a, __m256i b) VPMAXUD ymm, ymm, ymm/m256
clr-sig: Vector256<uint> Max(Vector256<uint> left, Vector256<uint> right);

    
docs: __m256i _mm256_max_epu16 (__m256i a, __m256i b) VPMAXUW ymm, ymm, ymm/m256
clr-sig: Vector256<ushort> Max(Vector256<ushort> left, Vector256<ushort> right);

    
docs: __m256i _mm256_max_epi16 (__m256i a, __m256i b) VPMAXSW ymm, ymm, ymm/m256
clr-sig: Vector256<short> Max(Vector256<short> left, Vector256<short> right);

    
docs: __m256i _mm256_max_epi32 (__m256i a, __m256i b) VPMAXSD ymm, ymm, ymm/m256
clr-sig: Vector256<int> Max(Vector256<int> left, Vector256<int> right);

    
docs: __m256i _mm256_max_epu8 (__m256i a, __m256i b) VPMAXUB ymm, ymm, ymm/m256
clr-sig: Vector256<byte> Max(Vector256<byte> left, Vector256<byte> right);

    
docs: __m256i _mm256_max_epi8 (__m256i a, __m256i b) VPMAXSB ymm, ymm, ymm/m256
clr-sig: Vector256<sbyte> Max(Vector256<sbyte> left, Vector256<sbyte> right);

    
docs: __m256i _mm256_min_epu32 (__m256i a, __m256i b) VPMINUD ymm, ymm, ymm/m256
clr-sig: Vector256<uint> Min(Vector256<uint> left, Vector256<uint> right);

    
docs: __m256i _mm256_min_epu16 (__m256i a, __m256i b) VPMINUW ymm, ymm, ymm/m256
clr-sig: Vector256<ushort> Min(Vector256<ushort> left, Vector256<ushort> right);

    
docs: __m256i _mm256_min_epi8 (__m256i a, __m256i b) VPMINSB ymm, ymm, ymm/m256
clr-sig: Vector256<sbyte> Min(Vector256<sbyte> left, Vector256<sbyte> right);

    
docs: __m256i _mm256_min_epi32 (__m256i a, __m256i b) VPMINSD ymm, ymm, ymm/m256
clr-sig: Vector256<int> Min(Vector256<int> left, Vector256<int> right);

    
docs: __m256i _mm256_min_epi16 (__m256i a, __m256i b) VPMINSW ymm, ymm, ymm/m256
clr-sig: Vector256<short> Min(Vector256<short> left, Vector256<short> right);

    
docs: __m256i _mm256_min_epu8 (__m256i a, __m256i b) VPMINUB ymm, ymm, ymm/m256
clr-sig: Vector256<byte> Min(Vector256<byte> left, Vector256<byte> right);

    
docs: int _mm256_movemask_epi8 (__m256i a) VPMOVMSKB reg, ymm
    int MoveMask(Vector256<sbyte> value);

    
docs: int _mm256_movemask_epi8 (__m256i a) VPMOVMSKB reg, ymm
    int MoveMask(Vector256<byte> value);

    
docs: __m256i _mm256_mpsadbw_epu8 (__m256i a, __m256i b, const int imm8) VMPSADBW ymm,
docs: ymm, ymm/m256, imm8
clr-sig: Vector256<ushort> MultipleSumAbsoluteDifferences(Vector256<byte> left, Vector256<byte> right, byte mask);

    
docs: __m256i _mm256_mul_epi32 (__m256i a, __m256i b) VPMULDQ ymm, ymm, ymm/m256
clr-sig: Vector256<long> Multiply(Vector256<int> left, Vector256<int> right);

    
docs: __m256i _mm256_mul_epu32 (__m256i a, __m256i b) VPMULUDQ ymm, ymm, ymm/m256
clr-sig: Vector256<ulong> Multiply(Vector256<uint> left, Vector256<uint> right);

    

    
docs: __m256i _mm256_madd_epi16 (__m256i a, __m256i b) VPMADDWD ymm, ymm, ymm/m256
clr-sig: Vector256<int> MultiplyAddAdjacent(Vector256<short> left, Vector256<short> right);

    
docs: __m256i _mm256_mulhi_epi16 (__m256i a, __m256i b) VPMULHW ymm, ymm, ymm/m256
clr-sig: Vector256<short> MultiplyHigh(Vector256<short> left, Vector256<short> right);

    
docs: __m256i _mm256_mulhi_epu16 (__m256i a, __m256i b) VPMULHUW ymm, ymm, ymm/m256
clr-sig: Vector256<ushort> MultiplyHigh(Vector256<ushort> left, Vector256<ushort> right);

    
docs: __m256i _mm256_mulhrs_epi16 (__m256i a, __m256i b) VPMULHRSW ymm, ymm, ymm/m256
clr-sig: Vector256<short> MultiplyHighRoundScale(Vector256<short> left, Vector256<short> right);

    
docs: __m256i _mm256_mullo_epi32 (__m256i a, __m256i b) VPMULLD ymm, ymm, ymm/m256
clr-sig: Vector256<int> MultiplyLow(Vector256<int> left, Vector256<int> right);

    
docs: __m256i _mm256_mullo_epi16 (__m256i a, __m256i b) VPMULLW ymm, ymm, ymm/m256
clr-sig: Vector256<ushort> MultiplyLow(Vector256<ushort> left, Vector256<ushort> right);

    
docs: __m256i _mm256_mullo_epi32 (__m256i a, __m256i b) VPMULLD ymm, ymm, ymm/m256
clr-sig: Vector256<uint> MultiplyLow(Vector256<uint> left, Vector256<uint> right);

    
docs: __m256i _mm256_mullo_epi16 (__m256i a, __m256i b) VPMULLW ymm, ymm, ymm/m256
clr-sig: Vector256<short> MultiplyLow(Vector256<short> left, Vector256<short> right);

    
docs: __m256i _mm256_or_si256 (__m256i a, __m256i b) VPOR ymm, ymm, ymm/m256
clr-sig: Vector256<ulong> Or(Vector256<ulong> left, Vector256<ulong> right);

    
docs: __m256i _mm256_or_si256 (__m256i a, __m256i b) VPOR ymm, ymm, ymm/m256
clr-sig: Vector256<ushort> Or(Vector256<ushort> left, Vector256<ushort> right);

    
docs: __m256i _mm256_or_si256 (__m256i a, __m256i b) VPOR ymm, ymm, ymm/m256
clr-sig: Vector256<sbyte> Or(Vector256<sbyte> left, Vector256<sbyte> right);

    
docs: __m256i _mm256_or_si256 (__m256i a, __m256i b) VPOR ymm, ymm, ymm/m256
clr-sig: Vector256<uint> Or(Vector256<uint> left, Vector256<uint> right);

    
docs: __m256i _mm256_or_si256 (__m256i a, __m256i b) VPOR ymm, ymm, ymm/m256
clr-sig: Vector256<int> Or(Vector256<int> left, Vector256<int> right);

    
docs: __m256i _mm256_or_si256 (__m256i a, __m256i b) VPOR ymm, ymm, ymm/m256
clr-sig: Vector256<short> Or(Vector256<short> left, Vector256<short> right);

    
docs: __m256i _mm256_or_si256 (__m256i a, __m256i b) VPOR ymm, ymm, ymm/m256
clr-sig: Vector256<byte> Or(Vector256<byte> left, Vector256<byte> right);

    
docs: __m256i _mm256_or_si256 (__m256i a, __m256i b) VPOR ymm, ymm, ymm/m256
clr-sig: Vector256<long> Or(Vector256<long> left, Vector256<long> right);

    
docs: __m256i _mm256_packs_epi16 (__m256i a, __m256i b) VPACKSSWB ymm, ymm, ymm/m256
clr-sig: Vector256<sbyte> PackSignedSaturate(Vector256<short> left, Vector256<short> right);

    
docs: __m256i _mm256_packs_epi32 (__m256i a, __m256i b) VPACKSSDW ymm, ymm, ymm/m256
clr-sig: Vector256<short> PackSignedSaturate(Vector256<int> left, Vector256<int> right);

    
docs: __m256i _mm256_packus_epi16 (__m256i a, __m256i b) VPACKUSWB ymm, ymm, ymm/m256
clr-sig: Vector256<byte> PackUnsignedSaturate(Vector256<short> left, Vector256<short> right);

    
docs: __m256i _mm256_packus_epi32 (__m256i a, __m256i b) VPACKUSDW ymm, ymm, ymm/m256
clr-sig: Vector256<ushort> PackUnsignedSaturate(Vector256<int> left, Vector256<int> right);

    
docs: __m256i _mm256_permute2x128_si256 (__m256i a, __m256i b, const int imm8) VPERM2I128
docs: ymm, ymm, ymm/m256, imm8
clr-sig: Vector256<ushort> Permute2x128(Vector256<ushort> left, Vector256<ushort> right, byte control);

    
docs: __m256i _mm256_permute2x128_si256 (__m256i a, __m256i b, const int imm8) VPERM2I128
docs: ymm, ymm, ymm/m256, imm8
clr-sig: Vector256<uint> Permute2x128(Vector256<uint> left, Vector256<uint> right, byte control);

    
docs: __m256i _mm256_permute2x128_si256 (__m256i a, __m256i b, const int imm8) VPERM2I128
docs: ymm, ymm, ymm/m256, imm8
clr-sig: Vector256<sbyte> Permute2x128(Vector256<sbyte> left, Vector256<sbyte> right, byte control);

    
docs: __m256i _mm256_permute2x128_si256 (__m256i a, __m256i b, const int imm8) VPERM2I128
docs: ymm, ymm, ymm/m256, imm8
clr-sig: Vector256<ulong> Permute2x128(Vector256<ulong> left, Vector256<ulong> right, byte control);

    
docs: __m256i _mm256_permute2x128_si256 (__m256i a, __m256i b, const int imm8) VPERM2I128
docs: ymm, ymm, ymm/m256, imm8
clr-sig: Vector256<int> Permute2x128(Vector256<int> left, Vector256<int> right, byte control);

    
docs: __m256i _mm256_permute2x128_si256 (__m256i a, __m256i b, const int imm8) VPERM2I128
docs: ymm, ymm, ymm/m256, imm8
clr-sig: Vector256<short> Permute2x128(Vector256<short> left, Vector256<short> right, byte control);

    
docs: __m256i _mm256_permute2x128_si256 (__m256i a, __m256i b, const int imm8) VPERM2I128
docs: ymm, ymm, ymm/m256, imm8
clr-sig: Vector256<byte> Permute2x128(Vector256<byte> left, Vector256<byte> right, byte control);

    
docs: __m256i _mm256_permute2x128_si256 (__m256i a, __m256i b, const int imm8) VPERM2I128
docs: ymm, ymm, ymm/m256, imm8
clr-sig: Vector256<long> Permute2x128(Vector256<long> left, Vector256<long> right, byte control);

    
docs: __m256d _mm256_permute4x64_pd (__m256d a, const int imm8) VPERMPD ymm, ymm/m256,
docs: imm8
clr-sig: Vector256<double> Permute4x64(Vector256<double> value, byte control);

    
docs: __m256i _mm256_permute4x64_epi64 (__m256i a, const int imm8) VPERMQ ymm, ymm/m256,
docs: imm8
clr-sig: Vector256<long> Permute4x64(Vector256<long> value, byte control);

    
docs: __m256i _mm256_permute4x64_epi64 (__m256i a, const int imm8) VPERMQ ymm, ymm/m256,
docs: imm8
clr-sig: Vector256<ulong> Permute4x64(Vector256<ulong> value, byte control);

    
docs: __m256i _mm256_permutevar8x32_epi32 (__m256i a, __m256i idx) VPERMD ymm, ymm/m256,
docs: ymm
clr-sig: Vector256<int> PermuteVar8x32(Vector256<int> left, Vector256<int> control);

    
docs: __m256 _mm256_permutevar8x32_ps (__m256 a, __m256i idx) VPERMPS ymm, ymm/m256,
docs: ymm
clr-sig: Vector256<float> PermuteVar8x32(Vector256<float> left, Vector256<int> control);

    
docs: __m256i _mm256_permutevar8x32_epi32 (__m256i a, __m256i idx) VPERMD ymm, ymm/m256,
docs: ymm
clr-sig: Vector256<uint> PermuteVar8x32(Vector256<uint> left, Vector256<uint> control);

    
docs: __m256i _mm256_sll_epi64 (__m256i a, __m128i count) VPSLLQ ymm, ymm, xmm/m128
clr-sig: Vector256<ulong> ShiftLeftLogical(Vector256<ulong> value, Vector128<ulong> count);

    
docs: __m256i _mm256_slli_epi64 (__m256i a, int imm8) VPSLLQ ymm, ymm, imm8
clr-sig: Vector256<ulong> ShiftLeftLogical(Vector256<ulong> value, byte count);

    
docs: __m256i _mm256_sll_epi32 (__m256i a, __m128i count) VPSLLD ymm, ymm, xmm/m128
clr-sig: Vector256<uint> ShiftLeftLogical(Vector256<uint> value, Vector128<uint> count);

    
docs: __m256i _mm256_slli_epi32 (__m256i a, int imm8) VPSLLD ymm, ymm, imm8
clr-sig: Vector256<uint> ShiftLeftLogical(Vector256<uint> value, byte count);

    
docs: __m256i _mm256_sll_epi16 (__m256i a, __m128i count) VPSLLW ymm, ymm, xmm/m128
clr-sig: Vector256<ushort> ShiftLeftLogical(Vector256<ushort> value, Vector128<ushort> count);

    
docs: __m256i _mm256_sll_epi32 (__m256i a, __m128i count) VPSLLD ymm, ymm, xmm/m128
clr-sig: Vector256<int> ShiftLeftLogical(Vector256<int> value, Vector128<int> count);

    
docs: __m256i _mm256_sll_epi64 (__m256i a, __m128i count) VPSLLQ ymm, ymm, xmm/m128
clr-sig: Vector256<long> ShiftLeftLogical(Vector256<long> value, Vector128<long> count);

    
docs: __m256i _mm256_slli_epi64 (__m256i a, int imm8) VPSLLQ ymm, ymm, imm8
clr-sig: Vector256<long> ShiftLeftLogical(Vector256<long> value, byte count);

    
docs: __m256i _mm256_slli_epi32 (__m256i a, int imm8) VPSLLD ymm, ymm, imm8
clr-sig: Vector256<int> ShiftLeftLogical(Vector256<int> value, byte count);

    
docs: __m256i _mm256_sll_epi16 (__m256i a, __m128i count) VPSLLW ymm, ymm, xmm/m128
clr-sig: Vector256<short> ShiftLeftLogical(Vector256<short> value, Vector128<short> count);

    
docs: __m256i _mm256_slli_epi16 (__m256i a, int imm8) VPSLLW ymm, ymm, imm8
clr-sig: Vector256<short> ShiftLeftLogical(Vector256<short> value, byte count);

    
docs: __m256i _mm256_slli_epi16 (__m256i a, int imm8) VPSLLW ymm, ymm, imm8
clr-sig: Vector256<ushort> ShiftLeftLogical(Vector256<ushort> value, byte count);

    
docs: __m256i _mm256_bslli_epi128 (__m256i a, const int imm8) VPSLLDQ ymm, ymm, imm8
clr-sig: Vector256<ulong> ShiftLeftLogical128BitLane(Vector256<ulong> value, byte numBytes);

    
docs: __m256i _mm256_bslli_epi128 (__m256i a, const int imm8) VPSLLDQ ymm, ymm, imm8
clr-sig: Vector256<uint> ShiftLeftLogical128BitLane(Vector256<uint> value, byte numBytes);

    
docs: __m256i _mm256_bslli_epi128 (__m256i a, const int imm8) VPSLLDQ ymm, ymm, imm8
clr-sig: Vector256<sbyte> ShiftLeftLogical128BitLane(Vector256<sbyte> value, byte numBytes);

    
docs: __m256i _mm256_bslli_epi128 (__m256i a, const int imm8) VPSLLDQ ymm, ymm, imm8
clr-sig: Vector256<ushort> ShiftLeftLogical128BitLane(Vector256<ushort> value, byte numBytes);

    
docs: __m256i _mm256_bslli_epi128 (__m256i a, const int imm8) VPSLLDQ ymm, ymm, imm8
clr-sig: Vector256<int> ShiftLeftLogical128BitLane(Vector256<int> value, byte numBytes);

    
docs: __m256i _mm256_bslli_epi128 (__m256i a, const int imm8) VPSLLDQ ymm, ymm, imm8
clr-sig: Vector256<byte> ShiftLeftLogical128BitLane(Vector256<byte> value, byte numBytes);

    
docs: __m256i _mm256_bslli_epi128 (__m256i a, const int imm8) VPSLLDQ ymm, ymm, imm8
clr-sig: Vector256<long> ShiftLeftLogical128BitLane(Vector256<long> value, byte numBytes);

    
docs: __m256i _mm256_bslli_epi128 (__m256i a, const int imm8) VPSLLDQ ymm, ymm, imm8
clr-sig: Vector256<short> ShiftLeftLogical128BitLane(Vector256<short> value, byte numBytes);

    
docs: __m128i _mm_sllv_epi32 (__m128i a, __m128i count) VPSLLVD xmm, ymm, xmm/m128
clr-sig: Vector128<int> ShiftLeftLogicalVariable(Vector128<int> value, Vector128<uint> count);

    
docs: __m128i _mm_sllv_epi64 (__m128i a, __m128i count) VPSLLVQ xmm, ymm, xmm/m128
clr-sig: Vector128<long> ShiftLeftLogicalVariable(Vector128<long> value, Vector128<ulong> count);

    
docs: __m128i _mm_sllv_epi32 (__m128i a, __m128i count) VPSLLVD xmm, ymm, xmm/m128
clr-sig: Vector128<uint> ShiftLeftLogicalVariable(Vector128<uint> value, Vector128<uint> count);

    
docs: __m128i _mm_sllv_epi64 (__m128i a, __m128i count) VPSLLVQ xmm, ymm, xmm/m128
clr-sig: Vector128<ulong> ShiftLeftLogicalVariable(Vector128<ulong> value, Vector128<ulong> count);

    
docs: __m256i _mm256_sllv_epi32 (__m256i a, __m256i count) VPSLLVD ymm, ymm, ymm/m256
clr-sig: Vector256<int> ShiftLeftLogicalVariable(Vector256<int> value, Vector256<uint> count);

    
docs: __m256i _mm256_sllv_epi64 (__m256i a, __m256i count) VPSLLVQ ymm, ymm, ymm/m256
clr-sig: Vector256<long> ShiftLeftLogicalVariable(Vector256<long> value, Vector256<ulong> count);

    
docs: __m256i _mm256_sllv_epi32 (__m256i a, __m256i count) VPSLLVD ymm, ymm, ymm/m256
clr-sig: Vector256<uint> ShiftLeftLogicalVariable(Vector256<uint> value, Vector256<uint> count);

    
docs: __m256i _mm256_sllv_epi64 (__m256i a, __m256i count) VPSLLVQ ymm, ymm, ymm/m256
clr-sig: Vector256<ulong> ShiftLeftLogicalVariable(Vector256<ulong> value, Vector256<ulong> count);

    
docs: _mm256_sra_epi32 (__m256i a, __m128i count) VPSRAD ymm, ymm, xmm/m128
clr-sig: Vector256<int> ShiftRightArithmetic(Vector256<int> value, Vector128<int> count);

    
docs: _mm256_sra_epi16 (__m256i a, __m128i count) VPSRAW ymm, ymm, xmm/m128
clr-sig: Vector256<short> ShiftRightArithmetic(Vector256<short> value, Vector128<short> count);

    
docs: __m256i _mm256_srai_epi16 (__m256i a, int imm8) VPSRAW ymm, ymm, imm8
clr-sig: Vector256<short> ShiftRightArithmetic(Vector256<short> value, byte count);

    
docs: __m256i _mm256_srai_epi32 (__m256i a, int imm8) VPSRAD ymm, ymm, imm8
clr-sig: Vector256<int> ShiftRightArithmetic(Vector256<int> value, byte count);

    
docs: __m128i _mm_srav_epi32 (__m128i a, __m128i count) VPSRAVD xmm, xmm, xmm/m128
clr-sig: Vector128<int> ShiftRightArithmeticVariable(Vector128<int> value, Vector128<uint> count);

    
docs: __m256i _mm256_srav_epi32 (__m256i a, __m256i count) VPSRAVD ymm, ymm, ymm/m256
clr-sig: Vector256<int> ShiftRightArithmeticVariable(Vector256<int> value, Vector256<uint> count);

    
docs: __m256i _mm256_srl_epi64 (__m256i a, __m128i count) VPSRLQ ymm, ymm, xmm/m128
clr-sig: Vector256<ulong> ShiftRightLogical(Vector256<ulong> value, Vector128<ulong> count);

    
docs: __m256i _mm256_srli_epi64 (__m256i a, int imm8) VPSRLQ ymm, ymm, imm8
clr-sig: Vector256<ulong> ShiftRightLogical(Vector256<ulong> value, byte count);

    
docs: __m256i _mm256_srl_epi32 (__m256i a, __m128i count) VPSRLD ymm, ymm, xmm/m128
clr-sig: Vector256<uint> ShiftRightLogical(Vector256<uint> value, Vector128<uint> count);

    
docs: __m256i _mm256_srli_epi32 (__m256i a, int imm8) VPSRLD ymm, ymm, imm8
clr-sig: Vector256<uint> ShiftRightLogical(Vector256<uint> value, byte count);

    
docs: __m256i _mm256_srli_epi16 (__m256i a, int imm8) VPSRLW ymm, ymm, imm8
clr-sig: Vector256<ushort> ShiftRightLogical(Vector256<ushort> value, byte count);

    
docs: __m256i _mm256_srl_epi16 (__m256i a, __m128i count) VPSRLW ymm, ymm, xmm/m128
clr-sig: Vector256<ushort> ShiftRightLogical(Vector256<ushort> value, Vector128<ushort> count);

    
docs: __m256i _mm256_srli_epi64 (__m256i a, int imm8) VPSRLQ ymm, ymm, imm8
clr-sig: Vector256<long> ShiftRightLogical(Vector256<long> value, byte count);

    
docs: __m256i _mm256_srl_epi32 (__m256i a, __m128i count) VPSRLD ymm, ymm, xmm/m128
clr-sig: Vector256<int> ShiftRightLogical(Vector256<int> value, Vector128<int> count);

    
docs: __m256i _mm256_srli_epi32 (__m256i a, int imm8) VPSRLD ymm, ymm, imm8
clr-sig: Vector256<int> ShiftRightLogical(Vector256<int> value, byte count);

    
docs: __m256i _mm256_srl_epi16 (__m256i a, __m128i count) VPSRLW ymm, ymm, xmm/m128
clr-sig: Vector256<short> ShiftRightLogical(Vector256<short> value, Vector128<short> count);

    
docs: __m256i _mm256_srli_epi16 (__m256i a, int imm8) VPSRLW ymm, ymm, imm8
clr-sig: Vector256<short> ShiftRightLogical(Vector256<short> value, byte count);

    
docs: __m256i _mm256_srl_epi64 (__m256i a, __m128i count) VPSRLQ ymm, ymm, xmm/m128
clr-sig: Vector256<long> ShiftRightLogical(Vector256<long> value, Vector128<long> count);

    
docs: __m256i _mm256_bsrli_epi128 (__m256i a, const int imm8) VPSRLDQ ymm, ymm, imm8
clr-sig: Vector256<ulong> ShiftRightLogical128BitLane(Vector256<ulong> value, byte numBytes);

    
docs: __m256i _mm256_bsrli_epi128 (__m256i a, const int imm8) VPSRLDQ ymm, ymm, imm8
clr-sig: Vector256<ushort> ShiftRightLogical128BitLane(Vector256<ushort> value, byte numBytes);

    
docs: __m256i _mm256_bsrli_epi128 (__m256i a, const int imm8) VPSRLDQ ymm, ymm, imm8
clr-sig: Vector256<sbyte> ShiftRightLogical128BitLane(Vector256<sbyte> value, byte numBytes);

    
docs: __m256i _mm256_bsrli_epi128 (__m256i a, const int imm8) VPSRLDQ ymm, ymm, imm8
clr-sig: Vector256<uint> ShiftRightLogical128BitLane(Vector256<uint> value, byte numBytes);

    
docs: __m256i _mm256_bsrli_epi128 (__m256i a, const int imm8) VPSRLDQ ymm, ymm, imm8
clr-sig: Vector256<int> ShiftRightLogical128BitLane(Vector256<int> value, byte numBytes);

    
docs: __m256i _mm256_bsrli_epi128 (__m256i a, const int imm8) VPSRLDQ ymm, ymm, imm8
clr-sig: Vector256<short> ShiftRightLogical128BitLane(Vector256<short> value, byte numBytes);

    
docs: __m256i _mm256_bsrli_epi128 (__m256i a, const int imm8) VPSRLDQ ymm, ymm, imm8
clr-sig: Vector256<byte> ShiftRightLogical128BitLane(Vector256<byte> value, byte numBytes);

    
docs: __m256i _mm256_bsrli_epi128 (__m256i a, const int imm8) VPSRLDQ ymm, ymm, imm8
clr-sig: Vector256<long> ShiftRightLogical128BitLane(Vector256<long> value, byte numBytes);

    
docs: __m128i _mm_srlv_epi32 (__m128i a, __m128i count) VPSRLVD xmm, xmm, xmm/m128
clr-sig: Vector128<int> ShiftRightLogicalVariable(Vector128<int> value, Vector128<uint> count);

    
docs: __m128i _mm_srlv_epi64 (__m128i a, __m128i count) VPSRLVQ xmm, xmm, xmm/m128
clr-sig: Vector128<long> ShiftRightLogicalVariable(Vector128<long> value, Vector128<ulong> count);

    
docs: __m128i _mm_srlv_epi32 (__m128i a, __m128i count) VPSRLVD xmm, xmm, xmm/m128
clr-sig: Vector128<uint> ShiftRightLogicalVariable(Vector128<uint> value, Vector128<uint> count);

    
docs: __m128i _mm_srlv_epi64 (__m128i a, __m128i count) VPSRLVQ xmm, xmm, xmm/m128
clr-sig: Vector128<ulong> ShiftRightLogicalVariable(Vector128<ulong> value, Vector128<ulong> count);

    
docs: __m256i _mm256_srlv_epi32 (__m256i a, __m256i count) VPSRLVD ymm, ymm, ymm/m256
clr-sig: Vector256<int> ShiftRightLogicalVariable(Vector256<int> value, Vector256<uint> count);

    
docs: __m256i _mm256_srlv_epi64 (__m256i a, __m256i count) VPSRLVQ ymm, ymm, ymm/m256
clr-sig: Vector256<long> ShiftRightLogicalVariable(Vector256<long> value, Vector256<ulong> count);

    
docs: __m256i _mm256_srlv_epi32 (__m256i a, __m256i count) VPSRLVD ymm, ymm, ymm/m256
clr-sig: Vector256<uint> ShiftRightLogicalVariable(Vector256<uint> value, Vector256<uint> count);

    
docs: __m256i _mm256_srlv_epi64 (__m256i a, __m256i count) VPSRLVQ ymm, ymm, ymm/m256
clr-sig: Vector256<ulong> ShiftRightLogicalVariable(Vector256<ulong> value, Vector256<ulong> count);
    
    
docs: __m256i _mm256_sign_epi16 (__m256i a, __m256i b) VPSIGNW ymm, ymm, ymm/m256
clr-sig: Vector256<short> Sign(Vector256<short> left, Vector256<short> right);

    
docs: __m256i _mm256_sign_epi32 (__m256i a, __m256i b) VPSIGND ymm, ymm, ymm/m256
clr-sig: Vector256<int> Sign(Vector256<int> left, Vector256<int> right);

    
docs: __m256i _mm256_sign_epi8 (__m256i a, __m256i b) VPSIGNB ymm, ymm, ymm/m256
clr-sig: Vector256<sbyte> Sign(Vector256<sbyte> left, Vector256<sbyte> right);

    
docs: __m256i _mm256_sub_epi64 (__m256i a, __m256i b) VPSUBQ ymm, ymm, ymm/m256
clr-sig: Vector256<ulong> Subtract(Vector256<ulong> left, Vector256<ulong> right);

    
docs: __m256i _mm256_sub_epi32 (__m256i a, __m256i b) VPSUBD ymm, ymm, ymm/m256
clr-sig: Vector256<uint> Subtract(Vector256<uint> left, Vector256<uint> right);

    
docs: __m256i _mm256_sub_epi8 (__m256i a, __m256i b) VPSUBB ymm, ymm, ymm/m256
clr-sig: Vector256<sbyte> Subtract(Vector256<sbyte> left, Vector256<sbyte> right);

    
docs: __m256i _mm256_sub_epi16 (__m256i a, __m256i b) VPSUBW ymm, ymm, ymm/m256
clr-sig: Vector256<ushort> Subtract(Vector256<ushort> left, Vector256<ushort> right);

    
docs: __m256i _mm256_sub_epi32 (__m256i a, __m256i b) VPSUBD ymm, ymm, ymm/m256
clr-sig: Vector256<int> Subtract(Vector256<int> left, Vector256<int> right);

    
docs: __m256i _mm256_sub_epi16 (__m256i a, __m256i b) VPSUBW ymm, ymm, ymm/m256
clr-sig: Vector256<short> Subtract(Vector256<short> left, Vector256<short> right);

    
docs: __m256i _mm256_sub_epi8 (__m256i a, __m256i b) VPSUBB ymm, ymm, ymm/m256
clr-sig: Vector256<byte> Subtract(Vector256<byte> left, Vector256<byte> right);

    
docs: __m256i _mm256_sub_epi64 (__m256i a, __m256i b) VPSUBQ ymm, ymm, ymm/m256
clr-sig: Vector256<long> Subtract(Vector256<long> left, Vector256<long> right);

    
docs: __m256i _mm256_subs_epu8 (__m256i a, __m256i b) VPSUBUSB ymm, ymm, ymm/m256
clr-sig: Vector256<byte> SubtractSaturate(Vector256<byte> left, Vector256<byte> right);

    
docs: __m256i _mm256_subs_epi16 (__m256i a, __m256i b) VPSUBSW ymm, ymm, ymm/m256
clr-sig: Vector256<short> SubtractSaturate(Vector256<short> left, Vector256<short> right);

    
docs: __m256i _mm256_subs_epi8 (__m256i a, __m256i b) VPSUBSB ymm, ymm, ymm/m256
clr-sig: Vector256<sbyte> SubtractSaturate(Vector256<sbyte> left, Vector256<sbyte> right);

    
docs: __m256i _mm256_subs_epu16 (__m256i a, __m256i b) VPSUBUSW ymm, ymm, ymm/m256
clr-sig: Vector256<ushort> SubtractSaturate(Vector256<ushort> left, Vector256<ushort> right);

    
docs: __m256i _mm256_sad_epu8 (__m256i a, __m256i b) VPSADBW ymm, ymm, ymm/m256
clr-sig: Vector256<ushort> SumAbsoluteDifferences(Vector256<byte> left, Vector256<byte> right);

    

    
    
docs: __m256i _mm256_xor_si256 (__m256i a, __m256i b) VPXOR ymm, ymm, ymm/m256
clr-sig: Vector256<ulong> Xor(Vector256<ulong> left, Vector256<ulong> right);

    
docs: __m256i _mm256_xor_si256 (__m256i a, __m256i b) VPXOR ymm, ymm, ymm/m256
clr-sig: Vector256<short> Xor(Vector256<short> left, Vector256<short> right);

    
docs: __m256i _mm256_xor_si256 (__m256i a, __m256i b) VPXOR ymm, ymm, ymm/m256
clr-sig: Vector256<int> Xor(Vector256<int> left, Vector256<int> right);

    
docs: __m256i _mm256_xor_si256 (__m256i a, __m256i b) VPXOR ymm, ymm, ymm/m256
clr-sig: Vector256<long> Xor(Vector256<long> left, Vector256<long> right);

    
docs: __m256i _mm256_xor_si256 (__m256i a, __m256i b) VPXOR ymm, ymm, ymm/m256
clr-sig: Vector256<sbyte> Xor(Vector256<sbyte> left, Vector256<sbyte> right);

    
docs: __m256i _mm256_xor_si256 (__m256i a, __m256i b) VPXOR ymm, ymm, ymm/m256
clr-sig: Vector256<ushort> Xor(Vector256<ushort> left, Vector256<ushort> right);

    
docs: __m256i _mm256_xor_si256 (__m256i a, __m256i b) VPXOR ymm, ymm, ymm/m256
clr-sig: Vector256<uint> Xor(Vector256<uint> left, Vector256<uint> right);

    
docs: __m256i _mm256_xor_si256 (__m256i a, __m256i b) VPXOR ymm, ymm, ymm/m256
clr-sig: Vector256<byte> Xor(Vector256<byte> left, Vector256<byte> right);

Bmi1
-------------------------------------------------------------------------------

docs: unsigned int _andn_u32 (unsigned int a, unsigned int b) ANDN r32a, r32b, reg/m32
clr-sig: uint AndNot(uint left, uint right);


docs: unsigned int _bextr_u32 (unsigned int a, unsigned int start, unsigned int len)
docs: BEXTR r32a, reg/m32, r32b
clr-sig: uint BitFieldExtract(uint value, byte start, byte length);


docs: unsigned int _bextr2_u32 (unsigned int a, unsigned int control) BEXTR r32a, reg/m32,
docs: r32b
clr-sig: uint BitFieldExtract(uint value, ushort control);


docs: unsigned int _blsi_u32 (unsigned int a) BLSI reg, reg/m32
clr-sig: uint ExtractLowestSetBit(uint value);


docs: unsigned int _blsmsk_u32 (unsigned int a) BLSMSK reg, reg/m32
clr-sig: uint GetMaskUpToLowestSetBit(uint value);


docs: unsigned int _blsr_u32 (unsigned int a) BLSR reg, reg/m32
clr-sig: uint ResetLowestSetBit(uint value);


docs: int _mm_tzcnt_32 (unsigned int a) TZCNT reg, reg/m32
clr-sig: uint TrailingZeroCount(uint value);

Bmi1.X64
-------------------------------------------------------------------------------

docs: unsigned __int64 _andn_u64 (unsigned __int64 a, unsigned __int64 b) ANDN r64a,
docs: r64b, reg/m64 This intrinisc is only available on 64-bit processes
clr-sig: ulong AndNot(ulong left, ulong right);


docs: unsigned __int64 _bextr_u64 (unsigned __int64 a, unsigned int start, unsigned
docs: int len) BEXTR r64a, reg/m64, r64b This intrinisc is only available on 64-bit
docs: processes
clr-sig: ulong BitFieldExtract(ulong value, byte start, byte length);


docs: unsigned __int64 _bextr2_u64 (unsigned __int64 a, unsigned __int64 control) BEXTR
docs: r64a, reg/m64, r64b This intrinisc is only available on 64-bit processes
clr-sig: ulong BitFieldExtract(ulong value, ushort control);


docs: unsigned __int64 _blsi_u64 (unsigned __int64 a) BLSI reg, reg/m64 This intrinisc
docs: is only available on 64-bit processes
clr-sig: ulong ExtractLowestSetBit(ulong value);


docs: unsigned __int64 _blsmsk_u64 (unsigned __int64 a) BLSMSK reg, reg/m64 This intrinisc
docs: is only available on 64-bit processes
clr-sig: ulong GetMaskUpToLowestSetBit(ulong value);


docs: unsigned __int64 _blsr_u64 (unsigned __int64 a) BLSR reg, reg/m64 This intrinisc
docs: is only available on 64-bit processes
clr-sig: ulong ResetLowestSetBit(ulong value);


docs: __int64 _mm_tzcnt_64 (unsigned __int64 a) TZCNT reg, reg/m64 This intrinisc is
docs: only available on 64-bit processes
clr-sig: ulong TrailingZeroCount(ulong value);


Bmi2
-------------------------------------------------------------------------------
docs: unsigned int _mulx_u32 (unsigned int a, unsigned int b, unsigned int* hi) MULX
docs: r32a, r32b, reg/m32 The above native signature does not directly correspond to
docs: the managed signature.
clr-sig: uint MultiplyNoFlags(uint left, uint right);


docs: unsigned int _mulx_u32 (unsigned int a, unsigned int b, unsigned int* hi) MULX
docs: r32a, r32b, reg/m32 The above native signature does not directly correspond to
docs: the managed signature.
clr-sig: uint MultiplyNoFlags(uint left, uint right, uint* low);


docs: unsigned int _pdep_u32 (unsigned int a, unsigned int mask) PDEP r32a, r32b, reg/m32
clr-sig: uint ParallelBitDeposit(uint value, uint mask);


docs: unsigned int _pext_u32 (unsigned int a, unsigned int mask) PEXT r32a, r32b, reg/m32
clr-sig: uint ParallelBitExtract(uint value, uint mask);


docs: unsigned int _bzhi_u32 (unsigned int a, unsigned int index) BZHI r32a, reg/m32,
docs: r32b
clr-sig: uint ZeroHighBits(uint value, uint index);

Bmi2.X64

docs: unsigned __int64 _mulx_u64 (unsigned __int64 a, unsigned __int64 b, unsigned
docs: __int64* hi) MULX r64a, r64b, reg/m64 The above native signature does not directly
docs: correspond to the managed signature. This intrinisc is only available on 64-bit
docs: processes
clr-sig: ulong MultiplyNoFlags(ulong left, ulong right);


docs: unsigned __int64 _mulx_u64 (unsigned __int64 a, unsigned __int64 b, unsigned
docs: __int64* hi) MULX r64a, r64b, reg/m64 The above native signature does not directly
docs: correspond to the managed signature. This intrinisc is only available on 64-bit
docs: processes
clr-sig: ulong MultiplyNoFlags(ulong left, ulong right, ulong* low);


docs: unsigned __int64 _pdep_u64 (unsigned __int64 a, unsigned __int64 mask) PDEP r64a,
docs: r64b, reg/m64 This intrinisc is only available on 64-bit processes
clr-sig: ulong ParallelBitDeposit(ulong value, ulong mask);


docs: unsigned __int64 _pext_u64 (unsigned __int64 a, unsigned __int64 mask) PEXT r64a,
docs: r64b, reg/m64 This intrinisc is only available on 64-bit processes
clr-sig: ulong ParallelBitExtract(ulong value, ulong mask);


docs: unsigned __int64 _bzhi_u64 (unsigned __int64 a, unsigned int index) BZHI r64a,
docs: reg/m32, r64b This intrinisc is only available on 64-bit processes
clr-sig: ulong ZeroHighBits(ulong value, ulong index);


Aes -> Sse2 
-------------------------------------------------------------------------------
docs: __m128i _mm_aesdec_si128 (__m128i a, __m128i RoundKey) AESDEC xmm, xmm/m128
clr-sig: Vector128<byte> Decrypt(Vector128<byte> value, Vector128<byte> roundKey);

docs: __m128i _mm_aesdeclast_si128 (__m128i a, __m128i RoundKey) AESDECLAST xmm, xmm/m128
clr-sig: Vector128<byte> DecryptLast(Vector128<byte> value, Vector128<byte> roundKey);

docs: __m128i _mm_aesenc_si128 (__m128i a, __m128i RoundKey) AESENC xmm, xmm/m128
clr-sig: Vector128<byte> Encrypt(Vector128<byte> value, Vector128<byte> roundKey);

docs: __m128i _mm_aesenclast_si128 (__m128i a, __m128i RoundKey) AESENCLAST xmm, xmm/m128
clr-sig: Vector128<byte> EncryptLast(Vector128<byte> value, Vector128<byte> roundKey);

docs: __m128i _mm_aesimc_si128 (__m128i a) AESIMC xmm, xmm/m128
clr-sig: Vector128<byte> InverseMixColumns(Vector128<byte> value);

docs: __m128i _mm_aeskeygenassist_si128 (__m128i a, const int imm8) AESKEYGENASSIST
docs: xmm, xmm/m128, imm8
clr-sig: Vector128<byte> KeygenAssist(Vector128<byte> value, byte control);

Pclmulqdq -> Sse2 
-------------------------------------------------------------------------------

docs: __m128i _mm_clmulepi64_si128 (__m128i a, __m128i b, const int imm8) PCLMULQDQ
docs: xmm, xmm/m128, imm8
public static Vector128<ulong> CarrylessMultiply(Vector128<ulong> left, Vector128<ulong> right, byte control);

docs: __m128i _mm_clmulepi64_si128 (__m128i a, __m128i b, const int imm8) PCLMULQDQ
docs: xmm, xmm/m128, imm8
clr-sig: Vector128<long> CarrylessMultiply(Vector128<long> left, Vector128<long> right, byte control);
